{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI4Med","text":"<p>Welcome to AI4Med! I am excited to welcome you. This course aims to create a beginner-friendly course on statistical computing. I hope to prepare you for all what you need for the basic computational skills for data science. </p> <p>One unique feature of the course is the integration of generative AI technologies. The way of coding in the era of AI has been revolutionized.  I aim to teach you all aspects of statistical computing seamlessly integrated with AI copilot. You will not only learn various cool algorithms but also the workflow of efficient and reproducible programming. Let's dive into an exciting semester of learning and discovery!</p> <p>Dive in</p>"},{"location":"discussion_rules/","title":"Workflow for Questions and Discussions","text":""},{"location":"discussion_rules/#content-questions","title":"Content Questions","text":""},{"location":"discussion_rules/#i-have-a-question-about-the-content-taught-in-class","title":"\"I have a question about the content taught in class.\"","text":"<p>Best Option: 1. Ask AI first 2. If still confused, post your question on Github Discussions under the \"Lectures\" category, we will answer you.</p> <p>Alternatives: - Visit a TA in office hours - Visit Prof. Lu in office hours</p>"},{"location":"discussion_rules/#i-have-a-question-about-the-syllabusdeadline-and-other-logistics-of-the-class","title":"\"I have a question about the syllabus/deadline and other logistics of the class.\"","text":"<p>Best Option: - Post on Github Discussions under the \"Logistics\" category, we will answer you.</p>"},{"location":"discussion_rules/#i-want-to-share-useful-resources-latest-ai-news-my-generative-ai-results-funny-coding-memes-to-the-class","title":"\"I want to share useful resources, latest AI news, my generative AI results, funny coding memes, to the class.\"","text":"<p>Best Option: - Post on Github Discussions under the \"Misc\" category. - Do not share the content irrelevant to the class.</p>"},{"location":"discussion_rules/#homework-help","title":"Homework Help","text":""},{"location":"discussion_rules/#i-need-help-on-the-homework","title":"\"I need help on the homework.\"","text":"<p>Best Option: - Ask AI first - Discuss with your squadmates - If still unsolved, post an issue on the homework repository. - For coding or mathematics questions you don't want classmates to see (though we encourage an open-source spirit with everything we do in this class):   - Send an email to a TA on your question</p> <p>This is the workflow how you communicate with other programmers in the world. So we want you to get familiar with it.</p> <p>How to post an issue on the homework repository: - Go to the homework repository - Click on the \"Issues\" tab - Click on the \"New issue\" button - Provide a Clear Title: Summarize the issue concisely in the \"Title\" field. - Write a Detailed Description: In the comment body, include:   - Description: Explain the problem or suggestion in detail.   - Steps to Reproduce: List the steps to replicate the issue.   - Expected Behavior: Describe what you expected to happen.   - Actual Behavior: Explain what actually occurred.   - Add Visuals (if necessary): Attach screenshots or logs to illustrate the issue. - Click on the \"Submit new issue\" button</p> <p>Alternative: - Visit a TA in office hours</p>"},{"location":"discussion_rules/#homework-clarifications","title":"Homework Clarifications","text":"<p>For questions like: - \"I need clarifications on the homework\" - \"How will you grade part X of the homework?\" - \"Do I need _____ for part X of the homework?\" - \"Will you take off points if I do _____ for part X of the homework?\" - \"I feel like this part of the homework is unfair/incorrect/buggy. Can you change it?\"</p> <p>Best Option: - Post on Issue - We will answer you</p> <p>\u274c DO NOT: - Ask Instructor or a TA individually</p> <p>Important Note: We try to avoid situations where one TA provides an answer/clarification to one student that other students may not be aware of. This can lead to incorrect information being shared. Only clarifications posted publicly on Github are official.</p>"},{"location":"discussion_rules/#grade-disputes","title":"Grade Disputes","text":""},{"location":"discussion_rules/#i-feel-like-i-was-graded-incorrectlyunfairly","title":"\"I feel like I was graded incorrectly/unfairly.\"","text":"<p>Best Option: 1. Contact the TA first 2. If after talking to the TA who graded you, you still feel you deserve a different grade, contact the instructor</p>"},{"location":"chapter_ai/","title":"Index","text":""},{"location":"chapter_ai/#overview","title":"Overview","text":"<p>This chapter is about how to utilize AI tools to assist in coding tasks.</p> <p></p>"},{"location":"chapter_ai/#lectures","title":"Lectures","text":"<ul> <li>AI Copilot: Learn how to effectively use AI tools like GitHub Copilot to assist in coding tasks.</li> <li>Prompt Engineering: Learn how to write effective prompts for AI tools.</li> <li>AI Tools: Learn how to use different AI tools to assist in coding tasks.</li> </ul>"},{"location":"chapter_ai/ai_copilot/","title":"AI-Assisted Coding","text":""},{"location":"chapter_ai/ai_copilot/#github-copilot","title":"GitHub Copilot","text":"<p>We strongly recommend you to read the official documentation of GitHub Copilot and the VS Code tutorial here. There are detailed instructions and examples on how to use different functions of GitHub Copilot there. Here we just want to share some basic tips for using GitHub Copilot effectively.</p>"},{"location":"chapter_ai/ai_copilot/#what-can-ai-copilot-do","title":"What can AI Copilot do?","text":"<p>Below we list the commands you can use in the Copilot Chat. However, you do not need to remember them. You can always type <code>/</code> in the chat to see the options (or even not type <code>/explain</code> but just ask the AI to explain the code. I guess Github Copilot only optimized the correponding prompt when you use <code>/</code>).</p> <p>What really matters is that the list tells you what you can ask AI to help your coding.</p> Command Description Usage /explain Get code explanations Open file with code or highlight code you want explained and type: /explain what is the fetchPrediction method? /fix Receive a proposed fix for the problems in the selected code Highlight problematic code and type: /fix propose a fix for the problems in fetchAirports route /tests Generate unit tests for selected code Open file with code or highlight code you want tests for and type: /tests /help Get help on using Copilot Chat Type: /help what can you do? /doc Add a documentation comment Highlight code and type: /doc. You can also press CMD+I in your editor and type /doc/ inline /generate Generate code to answer your question Type: /generate code that validates a phone number /optimize Analyze and improve running time of the selected code Highlight code and type: /optimize fetchPrediction method /simplify Simplify the selected code Highlight code and type: /simplify"},{"location":"chapter_ai/ai_copilot/#other-ai-copilot-ides","title":"Other AI Copilot IDEs","text":"<p>Beside Github Copilot, there are many other AI coding tools out there, such as Cursor and Windsurf. All tools are evolving very fast and new features are added all the time. Try them out and see which one you like the most.</p>"},{"location":"chapter_ai/ai_tools/","title":"AI tools","text":""},{"location":"chapter_ai/ai_tools/#language-models","title":"Language models","text":"<ul> <li>OpenAI<ul> <li>GPT 4o: General purpose use, can search the web, can add context to the conversation.</li> <li>GPT 5: Latest model with thinking mode.</li> </ul> </li> <li>Claude: excels in coding, has a smaller ecosystem with fewer third-party integrations and plugins.</li> <li>Gemini: Longer context, thinking mode.</li> <li>LLaMA: Open source LLM developed by Meta.</li> <li>DeepSeek: The recent DeepSeek R1 + Search is comparable to GPT o1.</li> <li>Grok: xAI's LLM with thinking mode as well, can search the internet.</li> </ul> <p>The table below may be out-of-date. Please check the latest models from the official websites.</p> Need Recommended Models Key Features High-Quality, Nuanced Output GPT 5, Gemini 2.5 Pro, DeepSeek R1, Claude 4 Sonnet, Grok 4 Top-tier quality, well-structured responses, ideal for professional applications Speed-Sensitive Applications Gemini 2.5 Flash Fastest output speeds, minimal delay, great for real-time tasks Cost-Conscious Deployments Mistral 3B, Llama 4 Highly affordable per million tokens, cost-effective for large-scale use Extensive Context Processing Gemini 2.5 Pro 2 million token context window, supports deep contextual analysis Low-Latency, Fast Response Mistral NeMo Minimal response time, excellent for interactive, real-time applications <p>Explore and choose the best language model for your tasks, e.g., creative writing, coding, math, etc.</p> <p>If you are Harvard affiliated, you can get access to many language models through Harvard AI Sandbox.</p> <p>Check Chat Arena for the assessment of the language models under different tasks.</p>"},{"location":"chapter_ai/ai_tools/#assistant-code","title":"Assistant Code","text":"<ul> <li>GitHub Copilot: IDE based, copilot based, offers autocompletion, code generation, and code review.</li> <li>Cursor: IDE based, autocompletion, code generation, and agentic features for automating complex workflows.</li> <li>Windsurf: IDE based, positions itself as an \"agentic\" IDE, cascade mode for generating entire processes based on prompts.</li> <li>Tabnine: IDE based</li> <li>Codeium: Developed by the same team as Windsurf, offers autocompletion, code generation, and code review. It can be extensions to your current IDE like VS Code.</li> <li>Devin AI: Web based, Operates autonomously, capable of executing entire development tasks without human intervention, effectively acting as an independent software engineer rather than a coding assistant. Operates within a secure, sandboxed environment equipped with common developer tools. $500/month.</li> <li>Claude: Web based, provides implementation of code after code generation.</li> <li>Cline: Open sourced tool to code as IDE extension.</li> </ul>"},{"location":"chapter_ai/ai_tools/#terminal-coding","title":"Terminal Coding","text":"<p>There are more agentic coding tools which can run commands in terminal. You can design a todo list and it will generate pipelines of commands using tools to achieve your goals. Compared to the above IDE-based copilot tools, the terminal based coding tools will be more automated and efficient.</p> <ul> <li>Claude Code: One of the best terminal coding tools. Cost $200 per month. You can define ToDo list and use agents for your tasks.</li> <li>Gemini Code: Similar to Claude Code, but with free tier: 60 requests/min and 1,000 requests/day with personal Google account.</li> <li>Qwen Code: Also has free tier using Qwen 2.5 7B.</li> </ul>"},{"location":"chapter_ai/ai_tools/#search-tools","title":"Search tools","text":"<ul> <li>Perplexity: delivering real-time, well-referenced information.</li> <li>ChatGPT Search: Search within the ChatGPT interface.</li> <li>Deep research: many language models can the deep research tools and they are great for literature review or idea brainstorming. For GPT models, there is Deep Research. For Gemini, there is Deep Research. For Grok, there is DeepSearch.</li> </ul>"},{"location":"chapter_ai/ai_tools/#notebook-and-knowledge-management","title":"Notebook and Knowledge Management","text":"<ul> <li>Notion AI: An all-in-one workspace that combines note-taking, project management, and databases. Its integrated AI assistant helps with summarizing texts, generating content ideas, and improving writing quality.</li> <li>Google NotebookLLM: AI-powered research and note-taking tool that assists users in interacting with their documents. It can generate summaries, explanations, and answers based on uploaded content, enhancing understanding and organization.</li> <li>ClickUp: Primarily a project management tool, ClickUp incorporates AI to enhance note-taking and task management. It offers features like AI-generated summaries and task suggestions, streamlining workflows</li> <li>OneNote with Copilot: OneNote integrates a range of AI features through Microsoft\u2019s own 365 Copilot, enhancing the way you create and manage your notes. That includes document generation, text summarization, outlining, and even an AI chat.</li> </ul>"},{"location":"chapter_ai/ai_tools/#writing-tools","title":"Writing tools","text":"<ul> <li>Grammarly: A widely used AI-powered writing assistant that provides real-time grammar and spelling corrections, style suggestions, and clarity improvements. </li> <li>Quillbot: It offers a suite of tools tailored to assist users in various aspects of writing, including paraphrasing, grammar checking, summarizing, and more.</li> <li>GPT Zero: Detect AI generated text.</li> <li>Gamma: AI generated slides.</li> </ul>"},{"location":"chapter_ai/ai_tools/#transcription-and-meeting-assistants","title":"Transcription and meeting assistants","text":"<ul> <li>Zoom AI Companion: can take notes, summarize meetings, create action items, and analyze the predominant tones and speakers during calls</li> <li>Fireflies: helps transcribe, summarize, record, filter, and analyze virtual meetings and conversations. It integrates with platforms like Zoom, Microsoft Teams, Google Meet, and Webex.</li> <li>tl;dv: distinguishes itself with multi-meeting AI insights and extensive integrations</li> </ul>"},{"location":"chapter_ai/ai_tools/#image-generation","title":"Image generation","text":"<ul> <li>Midjourney: Renowned for producing high-quality, artistic images, Midjourney is popular among creators for its unique aesthetic outputs. It operates via a Discord-based interface, facilitating community engagement and prompt sharing.</li> <li>Dalle-E: generating detailed and realistic images from textual descriptions. It integrates seamlessly with ChatGPT, allowing users to refine prompts for enhanced outputs.</li> <li>Stable Diffusion: An open-source model by Stability AI, Stable Diffusion allows users to generate images on local machines, offering flexibility and control. Its open-source nature encourages community-driven enhancements and customizations</li> <li>HeyGen: Generate realistic videos with AI.</li> </ul>"},{"location":"chapter_ai/ai_tools/#voice-and-music-generation","title":"Voice and music generation","text":"<ul> <li>Eleven Labs: AI-assisted text-to-speech software capable of producing lifelike speech by synthesizing vocal emotion and intonation. It also provides multilingual speech generation and long-form content creation with contextually-aware voices.</li> <li>Wondercraft: generates scripts and offers a vast library of hyper-realistic AI voices to host your show. </li> <li>Descript: An all-in-one editing tool for both audio and video, Descript uses AI to automate tasks like noise reduction and audio leveling. It also offers features like overdubbing, which allows for voice cloning, and the ability to remove filler words from your audio.</li> <li>Podcastle: An all-in-one editing tool for both audio and video, Descript uses AI to automate tasks like noise reduction and audio leveling. It also offers features like overdubbing, which allows for voice cloning, and the ability to remove filler words from your audio</li> <li>Suno: Powerful AI song and music generator.</li> </ul>"},{"location":"chapter_ai/ai_tools/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>PromptPerfect: transforms user ideas into precise prompts suitable for various AI models, including GPT-4, Midjourney, and Claude. It supports both text and image models</li> <li>prompts.chat: Prompt examples for various tasks.</li> <li>Copy Coder: Upload images of full applications, UI mockups, or custom designs and it will generate prompts to build your apps faster.</li> </ul>"},{"location":"chapter_ai/ai_tools/#ai-agent-builder","title":"AI Agent Builder","text":"<ul> <li>AutoGen by Microsoft: An open-source Python framework that simplifies the creation and management of multi-agent AI systems. AutoGen allows developers to define, configure, and compose AI agents that can collaborate to solve complex tasks with minimal user input</li> <li>AutoGPT: An open-source platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows.</li> <li>CrewAI: enables AI agents to collaborate effortlessly and execute complicated tasks. </li> </ul>"},{"location":"chapter_ai/prompt_engineering/","title":"Prompt Engineering","text":"<p>Prompt engineering is the art of crafting clear and effective prompts to guide AI to generate the desired output. OpenAI has a detailed prompt engineering guide on how to design effective prompts. Actually, you do not even need to memorize all the details in the guide. You can also design a personalized meta prompt to let AI generate good prompts for you, e.g., the openAI prompt generator. </p>"},{"location":"chapter_ai/prompt_engineering/#prompt-formula","title":"Prompt Formula","text":"<p>A good prompt includes 6 components ranked by importance:</p> <ul> <li> [task] Clearly define your end goal  </li> <li> [context] Tailor your responses  </li> <li> [examples] Mimic style, structure, tone  </li> <li> [persona] Embody a specific expertise  </li> <li> [format] Bullet points, markdown, table  </li> <li> [tone] Add layer of emotional context  </li> </ul> <pre><code>You are a senior product marketing manager at Apple [persona] and you have just unveiled the lastest Apple product in collaboration with Tesla, the Apple Car, and received 12,000 pre-orders, which is 200% higher than target [context]\n\nWrite [task] an email [format] to your boss , Tim Cook, sharing this positve news \n\nThe email should include a ti; dr (too long, didn't read) section, project background (why this product came into existence), business results section (quantifiable, business metrics), and end with a section thanking the product and engineering teams.\n[exemplar]\n\nUse clear and concise language and write in a confident yet friendly tone [tone]\n</code></pre>"},{"location":"chapter_ai/prompt_engineering/#prompt-tips","title":"Prompt Tips","text":"<ol> <li> <p>Always add context</p> <ul> <li>Include all the related documentations </li> <li>Add comments in the beginning of the file to describe what the file is for</li> <li>Include external website links (API docs, etc. if needed)</li> </ul> </li> <li> <p>Establish a Clear Objective</p> <ul> <li>Use a markdown file to write your goals for AI to understand</li> <li>Set clear, high-level goals when starting with a blank file/codebase</li> </ul> </li> </ol> <p>Example: <pre><code>Project: REST API Development\nObjective: Create a FastAPI backend service that will:\n1. Handle user authentication\n2. Provide CRUD operations for blog posts\n3. Implement rate limiting\n4. Include API documentation\n5. Follow REST best practices\n\nTechnical Requirements:\n- Python 3.9+\n- FastAPI framework\n- PostgreSQL database\n- JWT authentication\n</code></pre></p> <ol> <li> <p>Decompose Tasks into Steps</p> <ul> <li>Break down your task into simple, specific steps after communicating the main goal</li> <li>Ask the AI to use \"Chain of Thought\" to think step by step</li> </ul> </li> </ol> <p>Example: <pre><code>Task: Implement a data pipeline for processing sensor data\n\nSteps:\n1. Data Ingestion\n   - Read raw JSON files from S3 bucket\n   - Validate data schema\n   - Handle missing values\n\n2. Data Processing\n   - Convert timestamps to UTC\n   - Calculate rolling averages\n   - Remove outliers\n\n3. Data Aggregation\n   - Group by sensor ID\n   - Calculate hourly statistics\n   - Generate summary metrics\n\n4. Data Export\n   - Save processed data to PostgreSQL\n   - Generate CSV reports\n   - Log processing metrics\n</code></pre></p> <ol> <li> <p>Provide Examples</p> <ul> <li>This is called in-context learning or few-shot learning. AI performs better when you provide examples.</li> <li>You should be specific about the format of the output.</li> </ul> </li> <li> <p>Check the Code</p> <ul> <li>Always review and validate the code generated by AI</li> <li>You can also ask the AI to critique itself and improve the code</li> </ul> </li> </ol> <p>Example: <pre><code>After AI generates code, ask:\n\n1. Code Review Checklist:\n   - Are all edge cases handled?\n   - Is there proper error handling?\n   - Are there unit tests?\n   - Does it follow project's coding style?\n   - Are there any performance concerns?\n\n2. Improvement Request:\n   \"Please review the code for:\n   - Potential memory leaks\n   - SQL injection vulnerabilities\n   - API rate limiting issues\n   - Proper logging implementation\"\n</code></pre></p> <ol> <li> <p>Iterative Refinement</p> <ul> <li>Use the AI's output as a starting point</li> <li>Refine the implementation through multiple iterations</li> <li>Ask AI to provide feedback on what needs improvement</li> </ul> </li> </ol> <p>Example: <pre><code>Initial prompt:\n\"Create a data visualization function for our sales data\"\n\nCritique:\n\"Provide a concise paragraph on how to improve the code. Be very critical in your response.\"\n\nRefinement 1:\n\"The plot looks good, but please:\n1. Add proper axis labels\n2. Include a title\n3. Use a colorblind-friendly palette\n4. Add a legend\"\n\nRefinement 2:\n\"Now enhance it with:\n1. Interactive tooltips\n2. Ability to zoom\n3. Export to PNG option\"\n</code></pre></p>"},{"location":"chapter_ai/prompt_engineering/#prompt-generation-tools","title":"Prompt Generation Tools","text":"<p>You can use AI to generate prompts for you. Here is an example of meta-prompt for prompt generation:</p> <pre><code>I want you to become my Expert Prompt Creator. Your goal is to help me craft the best possible prompt for my needs. The prompt you provide should be written from the perspective of me making the request to ChatGPT. Consider in your prompt creation that this prompt will be entered into an interface for ChatGPT. The process is as follows:\n1. You will generate the following sections:\nPrompt:\n{provide the best possible prompt according to my request}\nCritique:\n{provide a concise paragraph on how to improve the prompt. Be very critical in your response.\nQuestions:\n{ask any questions pertaining to what additional information is needed from me to improve the prompt (max of 3). If the prompt needs more clarification or details in certain areas, ask questions to get more information to include in the prompt}\n2. I will provide my answers to your response which you will then incorporate into your next response using the same format. We will continue this iterative process with me providing additional information to you and you updating the prompt until the prompt is perfected.\nRemember, the prompt we are creating should be written from the perspective of me making a request to ChatGPT. Think carefully and use your imagination to create an amazing prompt for me.\nYou're first response should only be a greeting to the user and to ask what the prompt should be about.\n</code></pre> <p>Here are some tools that can help you generate prompts:</p> <ul> <li>PromptPerfect: transforms user ideas into precise prompts suitable for various AI models, including GPT-4, Midjourney, and Claude. It supports both text and image models</li> <li>prompts.chat: Prompt examples for various tasks.</li> <li>Copy Coder: Upload images of full applications, UI mockups, or custom designs and it will generate prompts to build your apps faster.</li> </ul>"},{"location":"chapter_appendix/","title":"Appendix","text":""},{"location":"chapter_appendix/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation Guide for Course</li> <li>Github Classroom</li> <li>Class Cluster</li> </ul> <p>If you are new to Python, you can refer to the Python 101 for the basic Python we expect you to know.</p> <p>You are welcome to contribute to this course following the contribution guide.</p>"},{"location":"chapter_appendix/cluster/","title":"Class Cluster","text":"<p>The cluster computing of the class is supported by Harvard Academic Technologies Group similar to the FAS-RC cluster. The cluster can be accessed by logging in with your Harvard credentials here. Please read over the HUIT user guide.</p>"},{"location":"chapter_appendix/cluster/#terminal-access","title":"Terminal Access","text":"<p>To get started with the cluster, click \"Clusters\" and then \"Academic shell access\". This will open a terminal very similar to the one on your personal computer. </p> <p></p> <p>Upon opening the terminal, you will be in your personal directory where you can store your files. Type <code>pwd</code> to get the path to this directory.  Your personal directory should be <code>~/shared/home/YOUR_USERNAME</code>.</p>"},{"location":"chapter_appendix/cluster/#vs-code-access","title":"VS Code Access","text":"<p>You can also use virtual VS Code on cluster by following the steps below.</p> &lt;1&gt;&lt;2&gt;&lt;3&gt;&lt;4&gt; <p></p> <p></p> <p></p> <p></p>"},{"location":"chapter_appendix/cluster/#getting-started-with-slurm","title":"Getting started with SLURM","text":"<p>When you want to run code, please use Slurm to submit jobs to a compute node. Avoid running computationally intensive code on the login node. The following is a simple demonstration on how to use Slurm to run a python script. </p> <p>If you are using the Academic shell</p> <ul> <li>Create a python file called <code>hello.py</code><ul> <li>Run <code>nano hello.py</code> to open the text editor</li> <li>Type <code>print(\"Hello world\")</code>. </li> <li>Type <code>^X</code> to exit the text editor. </li> </ul> </li> <li>Typing <code>ls</code> prints a list of files in your current directory. <code>hello.py</code> should be included in the list. </li> <li>Run <code>nano slurm_test.sh</code> and paste the following</li> </ul> <pre><code>#!/bin/bash\n#SBATCH --job-name=example_job   # Name of the job\n#SBATCH --output=slurm_%j.log    # Log file \n#SBATCH --error=slurm_%j.err     # Error file \n#SBATCH --time=24:00:00          # Time limit (24 hours)\n#SBATCH --ntasks=1               # Number of tasks (default is 1)\n#SBATCH --cpus-per-task=1        # Number of CPU cores per task\n#SBATCH --mem=4G                 # Memory per node \n\n\necho \"Job started\"\n# Run your command or script here\npython3 hello.py\n# Print the end time\necho \"Job ended\"\n</code></pre> <ul> <li> <p>It is important not to overspecify amount of computation required (time limit, memory CPUs) because the cluster is shared with other users. For this class, please do not use a time limit greater than 24 hours. </p> </li> <li> <p>To submit the job, run <code>sbatch slurm_test.sh</code>. You should see <code>Submitted batch job X</code> where <code>X</code> is the job number. </p> </li> <li> <p>To check the status of your job, type <code>sacct</code>. You will see a list of all your submitted jobs and their status. </p> </li> <li> <p>Once the job is <code>COMPLETED</code>, you can see the output by typing <code>slurm_X.log</code>, where <code>X</code> is the job number. </p> </li> <li> <p>This workflow can be used as a template when submitting more complicated jobs, but feel free to modify it as needed. </p> </li> </ul> <p>If you are using VS Code</p> <ul> <li>Create the <code>slurm_test.sh</code> file like above in VS Code.</li> <li>Press <code>Ctrl + ~</code> to open the terminal in VS Code.</li> <li>Type <code>sbatch slurm_test.sh</code> to submit the job.</li> </ul> <p>Tips</p> <p>VS Code on the cluster cannot directly use Github Copilot. We suggest you code locally and use Github to synchronize your local and cluster code. </p>"},{"location":"chapter_appendix/contribution/","title":"Contributing","text":"<p>We welcome any contributions to improve the quality of the course. If you discover any typos, broken links, missing content, textual ambiguities, unclear explanations, or unreasonable text structures, please assist us in making corrections to provide readers with better quality learning resources.</p>"},{"location":"chapter_appendix/contribution/#content-fine-tuning","title":"Content fine-tuning","text":"<p>You can follow these steps to modify text or code.</p> <ol> <li>Open the our course github repository. Find the Markdown source file you want to modify.</li> <li>Modify the Markdown source file content, check the accuracy of the content, and try to keep the formatting consistent.</li> <li>Fill in the modification description at the bottom of the page, then click the \"Propose file change\" button. After the page redirects, click the \"Create pull request\" button to initiate the pull request.</li> </ol> <p>Figures cannot be directly modified and require the creation of a new Issue or a comment to describe the problem. We will redraw and replace the figures as soon as possible.</p>"},{"location":"chapter_appendix/contribution/#content-creation","title":"Content creation","text":"<p>If you are interested in participating in this open-source project, including translating code into other programming languages or expanding article content, then the following Pull Request workflow needs to be implemented.</p> <ol> <li>Log in to GitHub and Fork the code repository of this book to your personal account.</li> <li>Go to your Forked repository web page and use the <code>git clone</code> command to clone the repository to your local machine.</li> <li>Create content locally and perform complete tests to verify the correctness of the code.</li> <li>Commit the changes made locally, then push them to the remote repository.</li> <li>Refresh the repository webpage and click the \"Create pull request\" button to initiate the pull request.</li> </ol>"},{"location":"chapter_appendix/contribution/#docker-deployment","title":"Docker deployment","text":"<p>In the <code>ai4med</code> root directory, execute the following Docker script to access the project at <code>http://localhost:8000</code>:</p> <pre><code>docker-compose up -d\n</code></pre> <p>Use the following command to remove the deployment:</p> <pre><code>docker-compose down\n</code></pre>"},{"location":"chapter_appendix/contribution/#resources","title":"Resources","text":"<ul> <li>This course website is built on the infrastructure of a fascinating open-source project AI4Med. Many materials of this course are adapted from their book. We thank the authors for their selfless contributions to the open-source community.</li> <li>This course website is build on the package of MkDocs. MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. You can learn more about it here.</li> </ul>"},{"location":"chapter_appendix/github/","title":"Github Classroom","text":""},{"location":"chapter_appendix/github/#github","title":"Github","text":"<p>Each student should create Github account if you don't already have one. Be sure to include your Harvard email as one of the emails associated with the account, as this will give us access to additional features such as GitHub Copilot</p> <p>One option to avoid inputting your username and password when pushing to a private repository is to use SSH keys. If you've previously set up SSH keys for Github, you can reuse them by copying your existing key pair to the <code>~/.ssh</code> directory. Otherwise, you'll need to generate new SSH keys by following these steps:</p> <ol> <li>Launch your terminal</li> <li>Run <code>ssh-keygen -t rsa</code> and follow the instructions</li> <li>You have now created your ssh keys in the directory <code>~/.ssh</code></li> <li>Run <code>cat .ssh/id_rsa.pub</code> to display your public key</li> <li>Copy your public key and enter it into Github under Settings &gt; SSH Keys</li> </ol>"},{"location":"chapter_appendix/github/#getting-started-with-github-classroom","title":"Getting Started with Github Classroom","text":"<p>Please fill out this form so that we can add your username to this GitHub classroom. </p> <p>Please read the Homework Submission Instructions for the required format of the homework submission.</p>"},{"location":"chapter_appendix/github/#setting-up-your-repository","title":"Setting Up Your Repository","text":"<p>When a GitHub classroom assignment is created, we will share a link with you. Click the link and select \"Accept this assignment\". This will create a forked repository accessible by your personal GitHub account. Note that each team should create only one repository for each homework. You should discuss with your team members to decide who will host the repository and how to collaborate with each other.</p> <p>Follow these steps to get started:</p> <ol> <li>Look for an email with the Github Classroom assignment invitation</li> <li>Open the link and select \"Accept this assignment\" </li> <li>Wait for your repository to be created - you'll see a confirmation page</li> <li>Find the repository URL by clicking the green \"Code\" button</li> <li>In your terminal:    <pre><code>git clone REPO_URL\n</code></pre></li> <li>Follow the Git tutorial to learn how to use Git and GitHub for your homework projects.</li> </ol>"},{"location":"chapter_appendix/github/#submitting-assignments","title":"Submitting Assignments","text":"<p>When submitting assignments:</p> <pre><code>git tag -a hw-ready-for-grade -m \"Homework Submission\"\ngit push origin main --tags\n</code></pre> <p>Make sure to commit all changes before tagging and pushing. We will only grade the commit with the tag <code>hw-ready-for-grade</code>. The late days will be counted based on the date of the commit with this tag. Do not make other commits after this tag.</p>"},{"location":"chapter_appendix/installation/","title":"Installation Guide for the Course","text":""},{"location":"chapter_appendix/installation/#operating-system","title":"Operating System","text":"<p>Most of the materials in this course are platform-independent. The instructions below include steps for all major operating systems: Linux, macOS, and Windows.</p>"},{"location":"chapter_appendix/installation/#install-ide","title":"Install IDE","text":"<p>For your local Integrated Development Environment (IDE), we suggest using Visual Studio Code (VS Code) - a lightweight, open-source editor. Head to the VS Code official website to download and install the version that matches your operating system.</p>"},{"location":"chapter_appendix/installation/#create-a-github-account","title":"Create a GitHub account","text":""},{"location":"chapter_appendix/installation/#github-copilot","title":"Github Copilot","text":"<p>We recommend using the AI assistant Github Copilot or other similar tools. As Harvard students, you may apply for a free license of GitHub Copilot by following the steps here. Note that you will need to upload proof of enrollment: you can use a picture of your student ID your GSAS enrollment documentation (which can be found in my.harvard). </p> <p>To install Github Copilot in VS Code, you can search for <code>Github Copilot</code> in the VS Code extension marketplace and install it.</p> <p>We also strongly recommend you to read the VS Code tutorial for Github Copilot. It introduces many useful features of Github Copilot.</p>"},{"location":"chapter_appendix/installation/#other-ai-copilot-resources","title":"Other AI-Copilot resources","text":"<p>We also recommend (though not required) to install the following AI-Copilot tools: - Cursor - Windsurf</p>"},{"location":"chapter_appendix/installation/#install-language-environments","title":"Install language environments","text":""},{"location":"chapter_appendix/installation/#python","title":"Python","text":"<p>We suggest using pyenv to install python for better version management. The installation process varies by operating system:</p>"},{"location":"chapter_appendix/installation/#for-macos","title":"For macOS:","text":"<ol> <li> <p>Install pyenv:     <pre><code>brew install pyenv\n</code></pre></p> </li> <li> <p>Add pyenv to your shell configuration:</p> <ul> <li>For bash, add to <code>~/.bashrc</code>:</li> <li>For zsh, add to <code>~/.zshrc</code>: <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n</code></pre></li> </ul> </li> </ol>"},{"location":"chapter_appendix/installation/#for-linux","title":"For Linux:","text":"<ol> <li> <p>Install pyenv:     <pre><code>curl https://pyenv.run | bash\n</code></pre></p> </li> <li> <p>Add the same configuration as macOS to your <code>~/.bashrc</code> or <code>~/.zshrc</code></p> </li> </ol>"},{"location":"chapter_appendix/installation/#for-windows","title":"For Windows:","text":"<ol> <li> <p>Install pyenv-win using PowerShell (run as Administrator):     <pre><code>Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &amp;\"./install-pyenv-win.ps1\"\n</code></pre></p> </li> <li> <p>Add System Environment Variables:</p> <ul> <li>Open System Properties &gt; Advanced &gt; Environment Variables</li> <li>Add to System Variables:<ul> <li>PYENV: <code>%USERPROFILE%\\.pyenv\\pyenv-win</code></li> <li>PYENV_HOME: <code>%USERPROFILE%\\.pyenv\\pyenv-win</code></li> </ul> </li> <li>Add to Path:<ul> <li><code>%USERPROFILE%\\.pyenv\\pyenv-win\\bin</code></li> <li><code>%USERPROFILE%\\.pyenv\\pyenv-win\\shims</code></li> </ul> </li> </ul> </li> </ol> <p>For all operating systems, after installation:</p> <ol> <li> <p>Install Python:    <pre><code>pyenv install 3.10.0\npyenv global 3.10.0\n</code></pre></p> </li> <li> <p>Verify the installation:    <pre><code>python --version  # Should show Python 3.10.0\n</code></pre></p> </li> </ol>"},{"location":"chapter_appendix/installation/#check-pip-installation","title":"Check pip installation","text":"<p>Pip is Python's package installer. It usually comes with Python, but it's good to verify the installation:</p> <ol> <li>Check if pip is installed: <pre><code>pip --version\n</code></pre></li> </ol> <p>If pip is not installed or you need to upgrade it:</p>"},{"location":"chapter_appendix/installation/#for-macoslinux","title":"For macOS/Linux:","text":"<pre><code>curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython get-pip.py\n</code></pre>"},{"location":"chapter_appendix/installation/#for-windows_1","title":"For Windows:","text":"<pre><code>python -m ensurepip --upgrade\n</code></pre> <p>After installation, verify pip is working: <pre><code>pip --version\n</code></pre></p>"},{"location":"chapter_appendix/installation/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>The installation process is the same for all operating systems:</p> <ol> <li> <p>Install Jupyter using pip:    <pre><code>pip install notebook\n</code></pre></p> </li> <li> <p>Verify the installation:    <pre><code>jupyter notebook --version\n</code></pre></p> </li> <li> <p>Launch Jupyter Notebook:    <pre><code>jupyter notebook\n</code></pre></p> </li> </ol>"},{"location":"chapter_appendix/installation/#vs-code-python","title":"VS Code Python","text":"<p>Install the following extensions from VS Code marketplace (same for all operating systems): - Python - Jupyter - Pylance - Pylint</p>"},{"location":"chapter_appendix/installation/#r","title":"R","text":""},{"location":"chapter_appendix/installation/#for-macos_1","title":"For macOS:","text":"<ul> <li>Download and install R from CRAN</li> </ul>"},{"location":"chapter_appendix/installation/#for-linux_1","title":"For Linux:","text":"<pre><code>sudo apt-get update\nsudo apt-get install r-base\n</code></pre>"},{"location":"chapter_appendix/installation/#for-windows_2","title":"For Windows:","text":"<ul> <li>Download and install R from CRAN</li> <li>Download and install Rtools from CRAN</li> </ul> <p>For all operating systems:</p> <ol> <li> <p>Install radian: <pre><code>pip install -U radian\n</code></pre></p> </li> <li> <p>Install required R packages from R console: <pre><code>install.packages(\"languageserver\")\ninstall.packages(\"httpgd\")\n</code></pre></p> </li> <li> <p>Configure VS Code. Please refer to the VS Code R Tutorial for all the steps. We just list the key steps here:</p> <ul> <li>Install the R extension</li> <li>Set radian path in VS Code settings:          - For Windows: Set <code>r.rterm.windows</code> to the path of radian (typically <code>%USERPROFILE%\\AppData\\Local\\Programs\\Python\\Python3x\\Scripts\\radian.exe</code>)          - For macOS/Linux: Set <code>r.rterm.mac</code> or <code>r.rterm.linux</code> to the output of <code>which radian</code></li> </ul> </li> </ol>"},{"location":"chapter_appendix/installation/#optional-software","title":"Optional software","text":""},{"location":"chapter_appendix/installation/#cursor","title":"Cursor","text":"<p>Cursor is available for all operating systems. Download from the official website.</p>"},{"location":"chapter_appendix/installation/#conda","title":"Conda","text":""},{"location":"chapter_appendix/installation/#for-macos_2","title":"For macOS:","text":"<ul> <li>Download the appropriate installer (Apple Silicon or Intel) from Miniconda download page</li> <li>Install using the .pkg installer</li> </ul>"},{"location":"chapter_appendix/installation/#for-linux_2","title":"For Linux:","text":"<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>"},{"location":"chapter_appendix/installation/#for-windows_3","title":"For Windows:","text":"<ul> <li>Download the Windows installer from Miniconda download page</li> <li>Run the .exe installer</li> <li>During installation, check \"Add Miniconda3 to my PATH environment variable\"</li> </ul> <p>For all operating systems, after installation:</p> <ol> <li> <p>Initialize conda: <pre><code>conda init\n</code></pre></p> </li> <li> <p>Verify installation: <pre><code>conda --version\n</code></pre></p> </li> <li> <p>Run <code>conda deactivate</code> to leave the environment.</p> </li> </ol> <p>For more detailed information, refer to the official Miniconda installation documentation.</p>"},{"location":"chapter_appendix/python_intro/","title":"Python 101","text":"<p>This is a basic tutorial for Python. We expect you to know the following materials for understanding the materials in the class. It covers the fundamentals of Python programming. For more topics, you can refer to the official Python tutorial. We also recommend you to read  VS Code Python Tutorial for how to use VS Code for Python programming.</p>"},{"location":"chapter_appendix/python_intro/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Python 101</li> <li>Table of Contents</li> <li>Getting Started </li> <li>Basic Commands </li> <li>Data Types </li> <li>Data Structures <ul> <li>Lists</li> <li>Sets</li> <li>Dictionaries</li> </ul> </li> <li>Control Flow <ul> <li>Conditional Statements</li> <li>For Loops</li> <li>While Loops</li> </ul> </li> <li>Functions <ul> <li>Lambda Functions</li> </ul> </li> <li>Advanced Topics <ul> <li>List Comprehensions</li> <li>Object-Oriented Programming</li> </ul> </li> <li>Data Analysis with Pandas and Scikit-learn </li> </ul>"},{"location":"chapter_appendix/python_intro/#getting-started","title":"Getting Started","text":"<p>Before diving into Python programming, it's important to know which version you're using. Python has two major versions (2 and 3) with significant differences. Here's how to check your Python version:</p> <p><pre><code>import sys\nprint(\"Python version\")\nprint(sys.version)\nprint(\"Version info.\")\nprint(sys.version_info)\n</code></pre> We suggest using Python 3 and following the installation guide.</p>"},{"location":"chapter_appendix/python_intro/#basic-commands","title":"Basic Commands","text":"<p>Python's syntax is clean and readable. Here are some basic commands:</p> <pre><code># Print text\nprint('Hello World!')\n\n# Perform calculations\nprint(3 + 3)\n\n'''\nYou can write multi-line\ncomments using triple quotes\n'''\n</code></pre>"},{"location":"chapter_appendix/python_intro/#data-types","title":"Data Types","text":"<p>Python has several built-in data types. You can check the type of any value using the <code>type()</code> function:</p> <pre><code># Integer\nprint(type(3))  # &lt;class 'int'&gt;\n\n# Float\nprint(type(3.0))  # &lt;class 'float'&gt;\n\n# String\nprint(type('hello'))  # &lt;class 'str'&gt;\n\n# Boolean\nprint(type(True))  # &lt;class 'bool'&gt;\n\n# Boolean expression\nprint(type(3 == 3))  # &lt;class 'bool'&gt;\n</code></pre>"},{"location":"chapter_appendix/python_intro/#data-structures","title":"Data Structures","text":""},{"location":"chapter_appendix/python_intro/#lists","title":"Lists","text":"<p>Lists are one of Python's most versatile data structures. Key features: - Mutable (can be modified) - Can contain multiple data types - Support indexing and slicing - Zero-based indexing</p> <pre><code># Creating lists\nfoo = [1, 2, 3]\nbar = ['a', 'b', 'c']\nbaz = [16, 'apple', 17.4, True]\n\n# Accessing elements\nprint(foo[0])  # First element\nprint(bar[1:3])  # Slice from index 1 to 2\n\n# Modifying lists\nfoo[1] = 'a'\n</code></pre>"},{"location":"chapter_appendix/python_intro/#sets","title":"Sets","text":"<p>Sets are unique collections of elements. Key features: - Unordered - Cannot be indexed or sliced - Elements must be unique - Useful for removing duplicates from lists</p> <pre><code># Creating sets\nnew_set = {1, 2, 1, 2, 3, 4, 1}  # Duplicates are automatically removed\nprint(new_set)  # {1, 2, 3, 4}\n\n# Converting list to set and back (removes duplicates)\nl = [1, 1, 2, 3]\nprint(list(set(l)))  # [1, 2, 3]\n</code></pre>"},{"location":"chapter_appendix/python_intro/#dictionaries","title":"Dictionaries","text":"<p>Dictionaries store key-value pairs. Key features: - Fast lookup by key - Keys must be unique - Values can be any data type - Support nested structures</p> <pre><code>state_dict = {\n    'Pennsylvania': 20,\n    'Arizona': 11,\n    'Georgia': 16,\n    'Nevada': 6\n}\n\nprint('Georgia has', state_dict['Georgia'], 'Electoral Votes')\n</code></pre>"},{"location":"chapter_appendix/python_intro/#control-flow","title":"Control Flow","text":""},{"location":"chapter_appendix/python_intro/#conditional-statements","title":"Conditional Statements","text":"<p>Python uses indentation to define code blocks:</p> <pre><code>num1 = 5\nnum2 = 100\n\nif num1 &gt; num2:\n    print(num1, 'is greater than', num2)\nelif num1 &lt; num2:\n    print(num1, 'is less than', num2)\nelse:\n    print(num1, 'is equal to', num2)\n</code></pre>"},{"location":"chapter_appendix/python_intro/#for-loops","title":"For Loops","text":"<p>Python's for loops are versatile and can iterate over many types of sequences:</p> <pre><code># Range-based loop\nfor i in range(1, 5):\n    print(i)\n\n# Iterating over a list\nfoo = ['a', 'b', 'c', 'd']\nfor letter in foo:\n    print(letter)\n\n# Iterating over dictionary\nfor key, val in state_dict.items():\n    print(key, val)\n</code></pre>"},{"location":"chapter_appendix/python_intro/#while-loops","title":"While Loops","text":"<p>While loops continue until a condition is met:</p> <pre><code># Calculate factorial\nnum = 5\nfac = 1\nwhile num &gt; 1:\n    fac *= num\n    num -= 1\n</code></pre>"},{"location":"chapter_appendix/python_intro/#functions","title":"Functions","text":"<p>Functions in Python are defined using the <code>def</code> keyword:</p> <pre><code>def sum_fun(a, b):\n    '''\n    This function takes two input numbers\n    and returns their sum\n\n    Input: a (int/float), b(int/float)\n    Output: my_sum(int/float)\n    '''\n    my_sum = a + b\n    return my_sum\n\nprint(sum_fun(5, 7))\n</code></pre>"},{"location":"chapter_appendix/python_intro/#lambda-functions","title":"Lambda Functions","text":"<p>Lambda functions are anonymous, inline functions:</p> <pre><code># Filter even numbers using lambda\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nevens = list(filter(lambda x: x % 2 == 0, my_list))\n</code></pre>"},{"location":"chapter_appendix/python_intro/#advanced-topics","title":"Advanced Topics","text":""},{"location":"chapter_appendix/python_intro/#list-comprehensions","title":"List Comprehensions","text":"<p>List comprehensions provide a concise way to create lists:</p> <pre><code># Traditional way\nfoo = []\nbar = [1, 17, 8, 83, 26, 11, 14, 92, 37]\nfor num in bar:\n    if num % 2 == 0:\n        foo.append(num)\n\n# List comprehension\nfoo = [num for num in bar if num % 2 == 0]\n</code></pre>"},{"location":"chapter_appendix/python_intro/#object-oriented-programming","title":"Object-Oriented Programming","text":"<p>Python is an object-oriented language. Here's a simple class example:</p> <pre><code>class Dog:\n    def __init__(self, name):\n        self.name = name\n        self.breed = ''\n        self.age = 0\n        self.fed_status = 'Not fed'\n\n    def have_birthday(self):\n        self.age += 1\n\n    def feed(self, food):\n        self.fed_status = f'{self.name} has been fed with {food}'\n\n# Create and use an instance\nmy_dog = Dog('Spot')\nmy_dog.feed('pizza')\nmy_dog.have_birthday()\n</code></pre>"},{"location":"chapter_appendix/python_intro/#data-analysis-with-pandas-and-scikit-learn","title":"Data Analysis with Pandas and Scikit-learn","text":"<p>Python is powerful for data analysis using libraries like Pandas and Scikit-learn:</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\n\n# Read data\ndata = pd.read_csv('data.csv')\n\n# Data preprocessing\nX = data.drop(columns=['target'])\ny = data['target']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n# Create and train model\nmodel = svm.SVC()\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"chapter_generative_model/","title":"Overview","text":"<p>This chapter introduces the concept of generative models. We will discuss the basic concepts of generative models, including the state, action, and reward. We will also discuss the different types of generative models, mainly diffusion models.</p> <p></p> <p>Generative video of Will Smith eating spaghetti: method in 2023 vs 2025</p> <p>Lectures:</p> <ul> <li>Langevin Dynamics</li> <li>Diffusion Models</li> <li>Flow Matching</li> </ul>"},{"location":"chapter_generative_model/ddpm/","title":"Diffusion Model","text":"<p>The Langevin Dynamics can be used to sample from a distribution \\(p \\propto \\exp(-f)\\) when \\(f\\) is known. However, for most cases, we need to learn the distribution from finite samples, e.g., images, where the distribution is unknown. This lecture introduces a diffusion generative model based on the diffusion process, which is a powerful tool for learning unknown distributions.</p> <p>From the previous lecture, we know that the diffusion process</p> \\[ X_t =  X_0 + \\sqrt{2t} \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, I) \\] <p>will converge to a Gaussian distribution. Imagining you add noise to an image, it will be blurred gradually to a white noise image as follows:</p> <p></p> <p>The key idea of diffusion generative model is to learn the reverse process of the diffusion process. We can generate images of cats by removing the noise from the image.</p> <p></p>"},{"location":"chapter_generative_model/ddpm/#from-noise-to-data","title":"From Noise to Data","text":"<p>Michelangelo:</p> <p>Every block of stone has a statue inside it. I just chip away the stone that doesn\u2019t look like David.</p> <p>Let\u2019s say we want to build a generative model as a process that transforms a random noise vector \\(z\\) into a data sample \\(x\\). Imagine you are Michelangelo, and you want to build a David statue from a block of stone.</p> <p></p> <p>Prompt: \"a hyper realistic twitter post by Michelangelo . include a selfie of him chopping the stones on a halfway done David statue.\"</p> <p>We can imagine this process as \u201csculpting,\u201d where the random noise \\(z\\) is raw materials like stones, and the data sample \\(x\\) is the sculpture. So, a generative model is like Michelangelo who removes the stone to reveal the sculpture.</p> \\[ \\begin{array}{ccc} \\text{Random noise $z$} &amp; \\xrightarrow{\\text{reverse}} &amp; \\text{Sample data $x$} \\\\ \\downarrow&amp; &amp;   \\downarrow \\\\ \\text{Rock} &amp; \\xrightarrow{\\text{chip}} &amp; \\text{Sculture} \\end{array} \\] <p>This process is hard, which is why there\u2019s so much research on generative models. But as the saying goes, \u201cdestruction is easier than construction.\u201d Maybe you can\u2019t build a skyscraper, but you can definitely tear one down. So let\u2019s think about the reverse process of dismantling a skyscraper into bricks and cement.</p> <p>Let \\(x_0\\) be the finished sculpture (data sample), and \\(x_T\\) be the pile of stones (random noise). Assume it takes \\(T\\) steps to dismantle it. The entire process is:</p> \\[ x = x_0 \\rightarrow x_1 \\rightarrow x_2 \\rightarrow \\cdots \\rightarrow x_{T-1} \\rightarrow x_T = z \\] <p></p> <p>The challenge of building a David statue is that going from raw materials \\(x_T\\) to the final structure \\(x_0\\) is too big a leap. But if we have the intermediate states \\(x_1, x_2, \\dots, x_T\\), we can understand how to go from one step to the next. Even master like Michelangelo need to chip one block at a time.</p> <p>So, if we know the transformation \\(x_{t-1} \\rightarrow x_t\\) (dismantling), then reversing it \\(x_t \\rightarrow x_{t-1}\\) is like chipping. If we can learn the reverse function \\(\\mu(x_t)\\), then starting from \\(x_T\\), we can repeatedly apply \\(\\mu\\) to reconstruct the David statue:</p> \\[ x_{T-1} = \\mu(x_T),\\quad x_{T-2} = \\mu(x_{T-1}), \\dots \\]"},{"location":"chapter_generative_model/ddpm/#denoising-diffusion-probabilistic-models","title":"Denoising Diffusion Probabilistic Models","text":"<p>We now are ready to introduce the Denoising Diffusion Probabilistic Models (DDPM) to implement the reverse process.</p> <p>As the saying goes, \u201cone bite at a time.\u201d As another saying goes, \"Rome wasn't built in a day.\" DDPM follows this principle by defining a gradual transformation from data samples to noise (dismantling), then learns the reverse (chipping). So it\u2019s more accurate to call DDPM a \u201cgradual model\u201d rather than a \u201cdiffusion model.\u201d</p>"},{"location":"chapter_generative_model/ddpm/#forward-process","title":"Forward Process","text":"<p>Specifically, DDPM defines the dismantling process as:</p> \\[ x_t = \\alpha_t x_{t-1} + \\beta_t \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, I) \\] <p>Here, \\(\\alpha_t, \\beta_t &gt; 0\\) and \\(\\alpha_t^2 + \\beta_t^2 = 1\\). Typically, \\(\\beta_t\\) is close to 0, representing small degradation at each step. The noise \\(\\varepsilon_t\\) adds randomness\u2014think of it as raw material injected at each step.</p> <p>Repeating this step, we get:</p> \\[ \\begin{align*} x_t &amp;= \\alpha_t x_{t-1} + \\beta_t \\varepsilon_t\\\\  &amp;= \\alpha_t (\\alpha_{t-1} x_{t-2} + \\beta_{t-1} \\varepsilon_{t-1}) + \\beta_t \\varepsilon_t\\\\  &amp;= (\\alpha_t \\alpha_{t-1}) x_{t-2} + (\\alpha_t \\beta_{t-1} + \\beta_t) \\varepsilon_{t-1} + \\beta_t \\varepsilon_t\\\\  &amp;= \\cdots\\\\  &amp;= (\\alpha_t \\cdots \\alpha_1)x_0 + \\underbrace{(\\alpha_t \\cdots \\alpha_2) \\varepsilon_1 + (\\alpha_t \\cdots \\alpha_3) \\varepsilon_2 + \\cdots + \\beta_t \\varepsilon_{t}}_{\\text{a weighted sum of Gaussian noises}} \\end{align*} \\] <p>Why do we require \\(\\alpha_t^2 + \\beta_t^2 = 1\\)? Because the Gaussian noise sum then becomes a single Gaussian with mean 0 and total variance 1, i.e.,</p> \\[ x_t = \\underbrace{(\\alpha_t \\cdots \\alpha_1)}_{\\bar{\\alpha}_t}x_0 + \\underbrace{\\sqrt{1 - (\\alpha_t \\cdots \\alpha_1)^2}}_{\\bar{\\beta}_t}\\bar{\\varepsilon}_t, \\quad \\bar{\\varepsilon}_t \\sim \\mathcal{N}(0, I) \\tag{1} \\] <p>This makes computing \\(x_t\\) very convenient. Furthermore, \\(\\bar{\\alpha}_T \\approx 0\\), meaning that after \\(T\\) steps, only noise remains.</p> <p></p>"},{"location":"chapter_generative_model/ddpm/#reverse-process","title":"Reverse Process","text":"<p>Now that we have data pairs \\((x_{t-1}, x_t)\\) from dismantling, we can learn the reverse \\(x_t \\rightarrow x_{t-1}\\) via a model \\(\\mu(x_t)\\). The loss is:</p> \\[ \\|x_{t-1} - \\mu(x_t)\\|^2 \\tag{2} \\] <p>The dismantling equation:</p> \\[ x_{t-1} = \\frac{1}{\\alpha_t}(x_t - \\beta_t \\varepsilon_t) \\] <p>motivates us to design the reverse model as:</p> \\[ \\mu(x_t) = \\frac{1}{\\alpha_t}(x_t - \\beta_t \\varepsilon_\\theta(x_t, t)) \\] <p>We aim to model the stone-to-be-chipped \\(\\varepsilon_t\\) as \\(\\varepsilon_\\theta(x_t, t)\\).</p> <p>The loss in (2) becomes:</p> \\[ \\|\\varepsilon_t - \\varepsilon_\\theta(x_t, t)\\|^2  \\] <p>Now we get the training loss:</p> \\[ \\mathbb{E}_{x_0 \\sim \\text{Data}, t \\sim \\text{Uniform}\\{1 ,\\ldots, T\\}, \\varepsilon_t \\sim \\mathcal{N}(0, I)} \\|\\varepsilon_t - \\varepsilon_\\theta(x_t, t)\\|^2 \\tag{3} \\]"},{"location":"chapter_generative_model/ddpm/#variance-reduction-trick","title":"Variance-Reduction Trick","text":"<p>From the loss in (3), we can see that the variance is high because of many things to sample:</p> <ul> <li>\\(x_0\\) from the data distribution</li> <li>\\(t\\) from \\(1, 2, \\ldots, T\\)</li> <li>\\(\\varepsilon_t, t=1, 2, \\ldots, T\\) from the normal distribution</li> </ul> <p>The more to sample, the higher the variance of loos will be, and harder to train.</p> <p>To reduce the variance, we can use the variance-reduction trick proposed in the DDPM paper. You will see it is basic change of variables and Gaussian addition tricks.</p> <p>Using the earlier expression for \\(x_t\\) in (1):</p> \\[ \\begin{align*} x_t  &amp;= \\alpha_t x_{t-1} + \\beta_t \\varepsilon_t \\\\      &amp;= \\alpha_t (\\bar{\\alpha}_{t-1} x_0 +  \\bar{\\beta}_{t-1} \\bar{\\varepsilon}_{t-1} )+ \\beta_t \\varepsilon_t \\\\      &amp;= \\bar{\\alpha}_t x_0 + \\alpha_t \\bar{\\beta}_{t-1} \\bar{\\varepsilon}_{t-1} + \\beta_t \\varepsilon_t  \\end{align*} \\] <p>We get the training loss:</p> \\[ \\|\\varepsilon_t - \\varepsilon_\\theta(\\bar{\\alpha}_t x_0 + \\alpha_t \\bar{\\beta}_{t-1} \\bar{\\varepsilon}_{t-1} + \\beta_t \\varepsilon_t , t)\\|^2 \\] <p>and it is easy to check that \\(\\omega, \\varepsilon \\sim \\mathcal{N}(0, I)\\) and \\(\\mathbb{E}[\\varepsilon \\omega^T] = 0\\). So \\(\\omega\\) is independent of \\(\\varepsilon\\).</p> <p>We construct two new Gaussian noises:</p> \\[ \\begin{align*} \\bar{\\beta}_t \\varepsilon &amp;= \\alpha_t \\bar{\\beta}_{t-1} \\bar{\\varepsilon}_{t-1} + \\beta_t \\varepsilon_t \\\\ \\bar{\\beta}_t \\omega &amp;= \\beta_t \\bar{\\varepsilon}_{t-1} - \\alpha_t \\bar{\\beta}_{t-1} \\varepsilon_t \\end{align*} \\] <p>We can also represent \\(\\varepsilon_t\\) by \\(\\omega\\) and \\(\\varepsilon\\) as:</p> \\[ \\varepsilon_t = \\frac{\\beta_t \\varepsilon - \\alpha_t \\bar{\\beta}_{t-1} \\omega}{\\bar{\\beta}_t} \\] <p>Substitute back to the loss in (3) and simplify to get the final DDPM loss:</p> \\[ \\begin{align*} &amp;\\mathbb{E}_{\\varepsilon_t \\sim \\mathcal{N}(0, I)} \\|\\varepsilon_t - \\varepsilon_{\\theta}  (x_t, t)\\|^2\\\\ &amp;= \\mathbb{E}_{\\varepsilon, \\omega \\sim \\mathcal{N}(0, I)} \\left\\|\\frac{\\beta_t \\varepsilon - \\alpha_t \\bar{\\beta}_{t-1} \\omega}{\\bar{\\beta}_t} - \\frac{\\bar{\\beta}_t}{\\beta_t} \\varepsilon_\\theta(\\bar{\\alpha}_t x_0 + \\bar{\\beta}_t \\varepsilon, t)\\right\\|^2 \\tag{4}\\\\ &amp;= \\frac{\\bar{\\beta}^2_t}{\\beta^2_t}\\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)} \\left\\| \\varepsilon - \\varepsilon_\\theta(\\bar{\\alpha}_t x_0 + \\bar{\\beta}_t \\varepsilon, t)\\right\\|^2 + \\text{const}, \\end{align*} \\] <p>where the last equality holds by taking the expectation out of \\(\\omega\\) by noticing the loss in (4) is quadratic in \\(\\omega\\).</p> <p>This gives us the final DDPM loss:</p> \\[ \\mathbb{E}_{x_0 \\sim \\text{Data}, t \\sim \\text{Uniform}\\{1 ,\\ldots, T\\}, \\varepsilon \\sim \\mathcal{N}(0, I)} \\| \\varepsilon - \\varepsilon_\\theta(\\bar{\\alpha}_t x_0 + \\bar{\\beta}_t \\varepsilon, t)\\|^2  \\] <p>We can see that we need to train the noise predictor \\(\\varepsilon_\\theta\\) with the time step \\(t\\) as input. In the original DDPM paper, the time step \\(t\\) is specified by adding the Transformer sinusoidal position embedding into each residual block. The paper also suggests to choose \\(T=1000\\) and \\(\\alpha_t = \\sqrt{1 - 0.02t/T}\\) to choose smaller steps when closer to the original data distribution.</p>"},{"location":"chapter_generative_model/ddpm/#sampling","title":"Sampling","text":"<p>Once \\(\\varepsilon_\\theta\\) is trained, DDPM generates samples by starting from \\(x_T \\sim \\mathcal{N}(0, I)\\) and running:</p> \\[ x_{t-1} = \\frac{1}{\\alpha_t}(x_t - \\beta_t \\varepsilon_\\theta(x_t, t)) \\] <p>You can do the random sampling as follows:</p> \\[ x_{t-1} = \\frac{1}{\\alpha_t}(x_t - \\beta_t \\varepsilon_\\theta(x_t, t)) + \\beta_t z, z \\sim \\mathcal{N}(0, I). \\] <p></p> <p></p>"},{"location":"chapter_generative_model/ddpm/#code-implementation","title":"Code Implementation","text":"<p>We can implement the DDPM model in PyTorch by defining a class <code>DDPM</code> with both the noise predictor and sampling.</p> <pre><code>import torch \nfrom torch import nn, Tensor\n\nclass DDPM(nn.Module):\n    def __init__(self, dim: int = 2, h: int = 64, n_steps: int = 100):\n        super().__init__()\n        self.n_steps = n_steps\n        # Define beta schedule from small to large values\n        self.betas = torch.linspace(1e-4, 0.02, n_steps)\n        # Calculate alphas: \u03b1_t = 1 - \u03b2_t\n        self.alphas = 1.0 - self.betas\n        # Calculate cumulative product of alphas: \u1fb1_t = \u220f_{i=1}^t \u03b1_i\n        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n\n        # Simple MLP network for noise prediction \u03b5_\u03b8\n        self.net = nn.Sequential(\n            nn.Linear(dim + 1, h), nn.ELU(),\n            nn.Linear(h, h), nn.ELU(),\n            nn.Linear(h, h), nn.ELU(),\n            nn.Linear(h, dim)\n        )\n\n    def forward(self, t: Tensor, x_t: Tensor) -&gt; Tensor:\n        # Reshape time step and concatenate with noisy input\n        # This implements \u03b5_\u03b8(x_t, t)\n        t = t.view(-1, 1)\n        return self.net(torch.cat((t, x_t), dim=-1))\n\n    def sample_step(self, x_t: Tensor, t: int) -&gt; Tensor:\n        # Sample Gaussian noise for the stochastic part of sampling\n        noise = torch.randn_like(x_t)\n        # Get \u03b1_t and \u1fb1_t for current timestep\n        alpha_t = self.alphas[t]\n        alpha_bar_t = self.alpha_bars[t]\n        # Calculate coefficient for the noise prediction\n        coeff = (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)\n        # Normalize time step to [0,1] range for the model\n        t_tensor = torch.full((x_t.shape[0],), t / self.n_steps, device=x_t.device)\n        # Predict noise using the model: \u03b5_\u03b8(x_t, t)\n        predicted_noise = self(t_tensor, x_t)\n        # Implement the sampling formula: x_{t-1} = (x_t - coeff * \u03b5_\u03b8(x_t, t)) / sqrt(\u03b1_t) + noise term\n        x_t = (x_t - coeff * predicted_noise) / torch.sqrt(alpha_t)\n        # Add noise term if not the final step, implementing the stochastic sampling\n        return x_t + torch.sqrt(1 - alpha_t) * noise if t &gt; 0 else x_t\n</code></pre> <p>We can train the DDPM model by compute the loss in (3).</p> <pre><code>from sklearn.datasets import make_moons\n\nddpm = DDPM()\noptimizer = torch.optim.Adam(ddpm.parameters(), lr=1e-3)\nloss_fn = nn.MSELoss()\n\nfor _ in range(10000):\n    x_0 = Tensor(make_moons(256, noise=0.05)[0])\n    t = torch.randint(0, ddpm.n_steps, (x_0.shape[0],))\n    noise = torch.randn_like(x_0)\n\n    alpha_bar_t = ddpm.alpha_bars[t].view(-1, 1)\n    x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * noise\n\n    optimizer.zero_grad()\n    t_normalized = t / ddpm.n_steps\n    predicted_noise = ddpm(t_normalized, x_t)\n    loss = loss_fn(predicted_noise, noise)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"chapter_generative_model/ddpm/#contextual-ddpm","title":"Contextual DDPM","text":"<p>We can generalize the DDPM model to generate the conditional distribution \\(p(x |c)\\) where \\(c\\) could be even be a text prompt.</p> <p>We can just simple add the context \\(c\\) to the input of the noise predictor \\(\\varepsilon_\\theta(x_t, t, c)\\) and train the loss function as:</p> \\[ \\mathbb{E}_{(x_0, c) \\sim \\text{Data}, t \\sim \\text{Uniform}\\{1 ,\\ldots, T\\}, \\varepsilon \\sim \\mathcal{N}(0, I)} \\| \\varepsilon - \\varepsilon_\\theta(\\bar{\\alpha}_t x_0 + \\bar{\\beta}_t \\varepsilon, t, c)\\|^2  \\] <p>More technologies like the CLIP can be used to improve the quality of the generated images. Please refer to the OpenAI DALL-E paper for more details.</p> <p></p>"},{"location":"chapter_generative_model/ddpm/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>For conditional probabiltiy generation, there is a trade-off between the fidelity and mode-coverage (diversity) of the generated images. In order to tune the trade-off, we can use the classifier-free guidance to sample using a linear combination of conditional and unconditional samples:</p> \\[ \\tilde{\\varepsilon}_\\theta(x_t, t, c) = (1+w) \\varepsilon_\\theta(x_t, t, c) - w \\varepsilon_\\theta(x_t, t), \\] <p>where \\(\\varepsilon_\\theta(x_t, t)\\) is an unconditional noise predictor. Usually, we will use the same network for both conditional and unconditional cases. For unconditional case, we will use a null token \\(\\varnothing\\) as the context \\(c\\) and fit \\(\\varepsilon_\\theta(x_t, t) = \\varepsilon_\\theta(x_t, t, \\varnothing)\\).</p> <p>The training process can be summarized as follows.</p> <p>Input: \\(p_{uncond}\\): probability of unconditional training</p> <p>Repeat:</p> <ol> <li>Sample data with conditioning from the dataset: \\((x, c) \\sim p(x, c)\\)</li> <li>Randomly discard conditioning to train unconditionally: \\(c \\leftarrow \\emptyset\\) with probability \\(p_{uncond}\\)</li> <li>Sample log SNR value: \\(\\lambda \\sim p(\\lambda)\\)</li> <li>Sample Gaussian noise: \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)</li> <li>Corrupt data to the sampled log SNR value: \\(z_\\lambda = \\alpha_\\lambda x + \\sigma_\\lambda \\epsilon\\)</li> <li>Take gradient step on \\(\\nabla_\\theta \\|\\epsilon_\\theta(z_\\lambda, c) - \\epsilon\\|^2\\)</li> </ol> <p>Until converged</p> <p>We will then use \\(\\tilde{\\varepsilon}_\\theta(x_t, t, c)\\) to sample from the model.</p> <p>When \\(w\\) increases from 0 to \\(\\infty\\), the generated images will become less fidelity and more diversity.</p>"},{"location":"chapter_generative_model/flow_match/","title":"Flow Matching","text":"<p>In the diffusion model, the forward process adding noise to the data distribution as</p> \\[ x_t = \\sqrt{\\alpha_t} x_0 + \\sqrt{1 - \\alpha_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>where \\(\\alpha_t\\) is a function of \\(t\\).</p> <p>This process connects the data distribution \\(p_0(x)\\) to Gaussian distribution \\(p_{\\infty}(x)\\) as \\(x_t\\) for \\(t \\to \\infty\\). And then DDPM aims to reverse this process to start from Gaussian distribution \\(p_{\\infty}(x)\\) and recover the data distribution \\(p_0(x)\\) by training a denoising diffusion model.</p> <p>However, there are two problems for the DDPM:</p> <ol> <li>We add noise in each step in the forward process, which adds variance to the process.</li> <li>\\(p_T(x)\\) is close but not exactly a Gaussian distribution, which introduces bias to the reverse process.</li> </ol> <p>To overcome these two problems, Flow matching's idea is to directly interpolate between the data density \\(p_1(x)\\) and the Gaussian density (or other simple density) \\(p_0(x)\\).</p> <p></p> <p>So how do we interpolate between \\(p_1(x)\\) and \\(p_0(x)\\)? From the previous lecture, we know that a density \\(p_t(x)\\) can be evolved following a vector field \\(u_t(x)\\):</p> \\[ \\frac{\\partial p_t}{\\partial t} = - \\nabla \\cdot (p_t u_t) \\] <p></p> <p>The interpolation between \\(X_0 \\sim p_0(x)\\) and \\(X_1 \\sim p_1(x)\\) can be achieved by many ways but the simplest way is to use a linear interpolation:</p> \\[ X_t = (1-t)X_0 + t X_1 \\sim p_t. \\] <p>We can learn the vector field \\(u_t(x)\\) by a neural network model \\(u^{\\theta}_t(x)\\) or other methods and minimize the following Flow Matching loss:</p> \\[ \\mathcal{L}_{\\text{FM}} = \\mathbb{E}_{t \\sim U[0,1], X_t\\sim p_t(x)} \\left\\| u^{\\theta}_t(X_t) - u_t(X_t) \\right\\|^2  \\] <p> However, the vector field \\(u_t(x)\\) is usually too complicated to make the above loss possible to solve.</p>"},{"location":"chapter_generative_model/flow_match/#conditional-flow-matching","title":"Conditional Flow Matching","text":"<p>To overcome this problem, we now consider a simpler case for the target density \\(p_1(x)\\) being a singleton point \\(x_1\\). The interpolation between \\(X_0 \\sim p_0(x) = N(0, I)\\) and \\(X_1 = x_1\\) becomes:</p> \\[ X_{t|1} = (1-t)X_0 + t x_1 \\sim p_{t|1} = N(t x_1, (1-t)^2 I). \\] <p></p> <p>As we have</p> \\[ \\frac{d}{dt} X_{t|1} = x_1 - X_0 = \\frac{x_1-X_{t|1}}{1-t} = u_{t}(X_{t|1}| x_1), \\] <p>so we have the conditional vector field </p> \\[ u_{t}(x | x_1) = \\frac{x_1-x}{1-t}. \\] <p>It can be shown that if \\(p_0(x)\\) evolves over the vector field \\(u_{t}(x | x_1)\\), it will converge to \\(x_1\\) at \\(t=1\\).</p> <p>We can then take the above example as a conditional case for the general target density \\(X_1 \\sim p_1(x)\\). Each \\(X_{t|1}\\) is a conditional path and we mix all paths \\(p_t(x|x_1)\\) to get the final density \\(p_t(x)\\).</p> <p></p> <p>We consider the conditional flow matching loss:</p> \\[ \\mathcal{L}_{\\text{CFM}} = \\mathbb{E}_{t \\sim U[0,1], X_t\\sim p_t(x), X_1 \\sim p_1(x)} \\left\\| u^{\\theta}_t(X_t) - u_t(X_t|X_1) \\right\\|^2  \\] <p>It can be shown that \\(\\mathcal{L}_{\\text{CFM}}\\) is same as the marginal flow matching loss \\(\\mathcal{L}_{\\text{FM}}\\) up to a constant:</p> \\[ \\mathcal{L}_{\\text{CFM}}(\\theta) = \\mathcal{L}_{\\text{FM}}(\\theta) + \\text{const}. \\] <p>Plugging the conditional vector field \\(u_t(X_t|X_1) = X_1 - X_0\\) into the CFM loss, we get:</p> \\[ \\mathcal{L}_{\\text{CFM}}(\\theta) = \\mathbb{E}_{t \\sim U[0,1], X_t\\sim p_t(x), X_1 \\sim p_1(x)} \\left\\| u^{\\theta}_t(X_t) - (X_1 - X_0) \\right\\|^2  \\]"},{"location":"chapter_generative_model/flow_match/#code-implementation","title":"Code Implementation","text":"<p>We can implement the CFM in PyTorch by defining a class with both the noise predictor and sampling.</p> <pre><code>import torch \nfrom torch import nn, Tensor\n\nclass Flow(nn.Module):\n    def __init__(self, dim: int = 2, h: int = 64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim + 1, h), nn.ELU(),\n            nn.Linear(h, h), nn.ELU(),\n            nn.Linear(h, h), nn.ELU(),\n            nn.Linear(h, dim))\n\n    def forward(self, t: Tensor, x_t: Tensor) -&gt; Tensor:\n        return self.net(torch.cat((t, x_t), -1))\n\n    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor) -&gt; Tensor:\n        t_start = t_start.view(1, 1).expand(x_t.shape[0], 1)\n\n        return x_t + (t_end - t_start) * self(t=t_start, x_t= x_t)\n</code></pre> <p>The we can train the model by minimizing the CFM loss.</p> <pre><code>flow = Flow()\n\noptimizer = torch.optim.Adam(flow.parameters(), 1e-2)\nloss_fn = nn.MSELoss()\n\nfor _ in range(10000):\n    x_1 = Tensor(make_moons(256, noise=0.05)[0])\n    x_0 = torch.randn_like(x_1)\n    t = torch.rand(len(x_1), 1)\n\n    x_t = (1 - t) * x_0 + t * x_1\n    dx_t = x_1 - x_0\n\n    optimizer.zero_grad()\n    loss_fn(flow(t=t, x_t=x_t), dx_t).backward()\n    optimizer.step()\n</code></pre> <p>The animation below compares the flow matching and the DDPM on the moons dataset. You can see that the flow matching is more stable and efficient.</p> Flow Matching DDPM"},{"location":"chapter_generative_model/flow_match/#general-case","title":"General Case","text":"<p>The linear interpolation of course is the simplest case. Summarizing the above, we have the following general strategy: we first construct an interpolation between \\(p_0(x)\\) and a singleton point \\(x_1\\) and then we find the corresponding conditional vector field \\(u_t(x|x_1)\\). </p> <p>Following this, we have the general form of the density \\(p_t\\) interpolated between \\(N(0, I)\\) and \\(x_1\\). Suppose we want conditional vector field which generates a path of Gaussians, i.e.,</p> \\[ p_t(x|x_1) = N(\\mu_t(x_1), \\sigma_t^2(x_1) I) \\] <p>where \\(\\mu_0(x_1) = 0\\), \\(\\mu_1(x_1) = x_1\\) and \\(\\sigma_0(x_1) = 1\\), \\(\\sigma_1(x_1) = \\sigma_{\\min}\\). Here we choose a sufficiently small \\(\\sigma_{\\min}\\) and use \\(N(x_1, \\sigma_{\\min}^2 I)\\) to approximate the singleton point \\(x_1\\). </p> <p>Namely, we have</p> \\[ X_{t|1} = \\sigma_t(x_1) X_0 +  \\mu_t(x_1) \\text{ has } X_{t|1} \\sim p_t(x|x_1). \\] <p>So we has the ordinary differential equation:</p> \\[ \\begin{align*} \\frac{d}{dt} X_{t|1} &amp;= X_{0} \\sigma'_t(x_1) + \\mu'_t(x_1) \\\\ &amp;=  \\frac{\\sigma'_t(x_1)}{\\sigma_t(x_1)} (X_{t|1} - \\mu_t(x_1)) + \\mu'_t(x_1) \\\\ &amp;= u_{t|1}(X_{t|1}|x_1). \\end{align*} \\] <p>This implies that the conditional vector field is:`</p> \\[ u_{t|1}(x|x_1) = \\frac{\\sigma'_t(x_1)}{\\sigma_t(x_1)} (x - \\mu_t(x_1)) + \\mu'_t(x_1). \\] <p>We then have the general conditional flow matching loss:</p> \\[ \\mathcal{L}_{\\text{CFM}}(\\theta) = \\mathbb{E}_{t, X_1, X_0} \\left\\| u^{\\theta}_t(X_t) - X_{0} \\sigma'_t(X_1) - \\mu'_t(X_1) \\right\\|^2 , \\] <p>where \\(X_t = \\sigma_t(X_1) X_0 + \\mu_t(X_1)\\).</p>"},{"location":"chapter_generative_model/flow_match/#example-optimal-transport-conditional-vector-field","title":"Example:  Optimal Transport conditional vector field","text":"<p>The aforementioned linear interpolation considers:</p> \\[ \\mu_t(x_1) = t x_1, \\quad \\sigma_t(x_1) = 1 - (1-\\sigma_{\\min})t. \\] <p>Thus the conditional vector field is:</p> \\[ u_{t|1}(x|x_1) = \\frac{x_1 - (1-\\sigma_{\\min})x}{1-(1-\\sigma_{\\min})t}. \\] <p>The CFM loss becomes:</p> \\[ \\mathcal{L}_{\\text{CFM}}(\\theta) = \\mathbb{E}_{t, X_1, X_0} \\left\\| u^{\\theta}_t(X_t) - (X_1 - (1-\\sigma_{\\min})X_0) \\right\\|^2 . \\]"},{"location":"chapter_generative_model/flow_match/#example-diffusion-conditional-vector-field","title":"Example: Diffusion Conditional Vector Field","text":"<p>Recalling the forward process of the diffusion model, we have</p> \\[ X_t = \\alpha_{1-t} X_1 + \\sqrt{1 - \\alpha^2_{1-t}} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I). \\] <p>Note that we flipped the notation compared to the previous lecture: \\(X_1\\) now is the simple Gaussian distribution \\(N(0, I)\\) and \\(X_0\\) is the data distribution. So we need to reverse the time of weight as \\(\\alpha_{1-t}\\).</p> <p>Then we have the conditional vector field:</p> \\[ u_{t|1}(x|x_1) = \\frac{\\alpha'_{1-t}}{1-\\alpha^2_{1-t}} (\\alpha_{1-t}x - x_1). \\] <p>We usually choose \\(\\alpha_t = \\exp(-\\frac{1}{2}T(t))\\), where \\(T(t) = \\int_0^t \\beta(s) ds\\). Such choice corresponds to the process (see the paper for more details):</p> \\[ X_t = \\sqrt{1-\\beta_t} X_{t-1} + \\sqrt{\\beta_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I). \\] <p>Therefore, we have the conditional vector field:</p> \\[ u_{t|1}(x|x_1) = -\\frac{T'(1-t)}{2} \\left[ \\frac{e^{-T(1-t)}x - e^{-\\frac{1}{2}T(1-t)}x_1}{1 - e^{-T(1-t)}} \\right]. \\] <p>Mathematically, this is equivalent to the DDPM. However, it has been shown that the flow matching approach is more stable and efficient.</p>"},{"location":"chapter_generative_model/langevin_dynamics/","title":"Langevin Dynamics","text":"<p>Our goal is to sample from a target distribution </p> \\[ p(x) \\propto e^{-f(x)} \\leftrightarrow  \\log p(x) = -f(x) + \\text{const} \\] <p>When \\(f\\) is complicated, we cannot sample from \\(p\\) directly. It might even be intractable to compute the partition function \\(Z = \\int e^{-f(x)} \\, dx\\). We are going to heuristically show you that sampling from \\(p\\) is essentially an optimization problem. There will be many gaps in the following derivation. We suggest the reader to read the paper for more details.</p>"},{"location":"chapter_generative_model/langevin_dynamics/#from-sampling-to-optimization","title":"From Sampling to Optimization","text":"<p>For simplicity, we now consider \\(p = e^{-f(x)}\\).  In order to sample from \\(p\\), our idea is to start with an optimization problem with the minimizer as \\(p\\). One natural choice of the optimization problem is:</p> \\[ p = \\min_{\\rho} \\text{KL}(\\rho \\| p) = \\int \\rho(x) \\log \\frac{\\rho(x)}{p(x)} \\, dx \\] <p>where \\(\\text{KL}(\\rho \\| p)\\) quantifies the discrepancy between distributions \\(\\rho\\) and \\(p\\), and it is called the Kullback-Leibler (KL) divergence.</p> <p>As \\(p = e^{-f(x)}\\), the loss function</p> \\[ \\begin{aligned}  F(\\rho):=\\text{KL}(\\rho \\| p) &amp; = \\mathbb{E}_{\\rho}[ \\log(\\rho/q)]  \\\\  &amp;= \\int \\rho(x) \\log \\frac{\\rho(x)}{p(x)} \\, dx \\\\  &amp;= \\int \\rho(x) \\left( \\log \\rho(x) - \\log p(x) \\right) \\, dx \\\\  &amp;= \\mathbb{E}_{\\rho}[f] - H(\\rho) \\end{aligned} \\] <p>where \\(H(\\rho) = - \\int \\rho(x) \\log \\rho(x) \\, dx\\) is the entropy of \\(\\rho\\).</p> <p>So now our problem is how to minimize \\(F(\\rho)\\) over the space of distributions which is an infinite-dimensional optimization problem.</p> <p>To compare to the finite-dimensional optimization, we need to find out the analog of the following concepts:</p> <ul> <li>Compared to \\(x_{t+1} = x_t - \\epsilon \\nabla f(x_t)\\), how to update \\(\\rho\\)</li> <li>What is the gradient of \\(F(\\rho)\\)?</li> </ul> <p>In the following sections, we will heuristically build the language for the infinite-dimensional optimization for density.</p>"},{"location":"chapter_generative_model/langevin_dynamics/#vector-field-of-gradient-flow","title":"Vector Field of Gradient Flow","text":"<p>We will answer the first question: how to update \\(\\rho\\)? Recall the gradient descent algorithm for finite-dimensional optimization \\(\\min_{x} f(x)\\) is given by:</p> \\[ x_{t+1} = x_t - \\epsilon g(x_t), \\] <p>where \\(g(x) = \\nabla f(x)\\).</p> <p>To make an analogy, our idea is to first consider the infinitesimal case with \\(\\epsilon \\to 0\\), working in the continuous-time domain, and then discretize the continuous-time dynamics to get the discrete-time update.</p> <p>As \\(\\epsilon \\to 0\\), the discrete update can be rewritten as a differential equation:</p> \\[ \\frac{x_{t+\\epsilon} - x_t}{\\epsilon} = -g(x_t) \\] <p>Taking the limit as \\(\\epsilon \\to 0\\), we get the continuous-time differential equation:</p> \\[ \\frac{dx(t)}{dt} = -g(x(t)) \\] <p>This ordinary differential equation (ODE) describes how a finite-dimensional point \\(x(t)\\) moves along the negative gradient direction of \\(f(x)\\). Thus we also call this gradient flow. In general, if we have a vector-valued function \\(v(x)\\) defined on any point \\(x\\) in the space (e.g., \\(v(x) = -g(x)\\) for the gradient flow), we call it a vector field, namely any point \\(x\\) in the space has a corresponding vector \\(v(x)\\). In general, the vector field can be written as:</p> \\[ \\frac{dx(t)}{dt} = v(x(t)) \\] <p></p> <p>Now we want to consider the infinite-dimensional case. As \\(\\rho\\) is a density of \\(x\\), imagine that each \\(x\\) is a particle moving along the vector field \\(v(x)\\), following the differential equation above. We aim to answer the key question: How the density \\(\\rho(x)\\) evolves as \\(\\rho(x,t)\\) when every of its sample \\(x(t)\\) is moving along the vector field \\(v(x)\\)?</p> <p></p>"},{"location":"chapter_generative_model/langevin_dynamics/#continuity-equation","title":"Continuity Equation","text":"<p>Our encourage to build the picture of the evolution of \\(\\rho(x,t)\\) as the fluid flow. We are going to use the law of mass conservation to derive the evolution of \\(\\rho(x,t)\\).</p> <p></p> <p>We consider an infinitesimally small box with volume \\(\\Delta x \\Delta y\\) in the space. The mass of fluid in the box is \\(\\rho(x,y,t) \\Delta x \\Delta y\\). </p> <p>When time evolves from \\(t\\) to \\(t+\\Delta t\\), we have the law of mass conservation:</p> <p>Mass changed in the box = Net mass flowed in the boundary</p> <p>We are going to compute both sides of the above equation.</p> \\[ \\text{Mass changed in the box} =(\\rho(x,y,t+\\Delta t) - \\rho(x,y,t)) \\Delta x \\Delta y \\] <p>For the right hand side, recalling that the vector field \\(v(x,y)\\) is a two-dimensional vector \\((v_x, v_y)\\), without loss of generality, we assume \\(v\\) points to the upper right corner of the box. Namely: the mass flowed in on the left and bottom boundaries, and flowed out on the right and top boundaries, i.e., Net mass flowed in the boundary = Mass flowed in - Mass flowed out.</p> \\[ \\begin{align*} \\text{Mass flowed in} = &amp;v_x(x,y) \\rho(x,y,t) \\Delta y \\Delta t\\\\ &amp; + v_y(x,y) \\rho(x,y,t) \\Delta x \\Delta t \\\\ \\text{Mass flowed out} = &amp;v_x(x+\\Delta x,y) \\rho(x+\\Delta x,y,t) \\Delta y \\Delta t \\\\ &amp; + v_y(x,y+\\Delta y) \\rho(x,y+\\Delta y,t) \\Delta x \\Delta t \\end{align*} \\] <p>The net increase of mass flowed in the boundary is:</p> \\[ \\begin{align*}  &amp;- \\left[\\left( v_x(x+\\Delta x,y)\\rho(x+\\Delta x,y,t) -v_x(x,y)\\rho(x,y,t)  \\right)  \\Delta y  \\right. \\\\  &amp; \\qquad \\left. + \\left(v_y(x,y+\\Delta y)\\rho(x,y+\\Delta y,t) - v_y(x,y)\\rho(x,y,t) \\right) \\Delta x \\right]\\Delta t \\\\ = &amp;- \\left[\\frac{\\left( v_x(x+\\Delta x,y)\\rho(x+\\Delta x,y,t) -v_x(x,y)\\rho(x,y,t)  \\right)}{\\Delta x} \\right. \\\\ &amp; \\qquad \\left. + \\frac{\\left(v_y(x,y+\\Delta y)\\rho(x,y+\\Delta y,t) - v_y(x,y)\\rho(x,y,t) \\right)}{\\Delta y}\\right]\\Delta t \\Delta x \\Delta y \\end{align*} \\] <p>Now we combine the two sides of the mass conservation equation together:</p> \\[ \\begin{align*} \\frac{\\rho(x,y,t+\\Delta t) - \\rho(x,y,t)}{\\Delta t} = &amp;- \\left[\\frac{\\left( v_x(x+\\Delta x,y) \\rho(x+\\Delta x,y,t) -v_x(x,y) \\rho(x,y,t)  \\right)}{\\Delta x} \\right. \\\\ &amp; \\qquad \\left. + \\frac{\\left(v_y(x,y+\\Delta y) \\rho(x,y+\\Delta y,t) - v_y(x,y) \\rho(x,y,t) \\right)}{\\Delta y}\\right] \\end{align*} \\] <p>Let all the infinitesimal quantities \\(\\Delta x, \\Delta y, \\Delta t\\) go to zero, we get the continuity equation:</p> \\[ \\frac{\\partial \\rho(x,y,t)}{\\partial t} = - \\frac{\\partial}{\\partial x} \\left( v_x(x,y) \\rho(x,y,t) \\right) - \\frac{\\partial}{\\partial y} \\left( v_y(x,y) \\rho(x,y,t) \\right) \\] <p>Now we want to simplify the notation above to make math more elegant. We will treat the gradient operator as a vector  \\(\\nabla = (\\partial_x, \\partial_y)\\), then we define the divergence of a vector field \\(v = (v_x, v_y)\\) as</p> \\[ \\nabla \\cdot v= \\partial_x v_x(x,y) + \\partial_y v_y(x,y), \\] <p>where we abuse the inner product notation \\(a \\cdot b = a_1 b_1 + a_2 b_2\\) for vectors \\(a, b \\in \\mathbb{R}^2\\).</p> <p>Then the above equation can be rewritten as:</p> \\[ \\frac{\\partial \\rho}{\\partial t} = - \\nabla \\cdot (v \\rho) \\] <p>This equation is true for all dimensions.</p> <p>Continuity Equation for Vector Field</p> <p>Given a density \\(\\rho\\) and a velocity field \\(v\\), we suppose each particle \\(x\\) moves along the velocity field \\(v(x)\\). Let \\(\\rho(x,t)\\) be the density of \\(x(t)\\). Then \\(\\rho(x,t)\\) evolves following the continuity equation:</p> \\[ \\frac{\\partial \\rho}{\\partial t} = - \\nabla \\cdot (v \\rho) \\] <p>This answer the first question on how to generalize the gradient descent to the infinite-dimensional case: If we have a descent direction \\(v\\), then the density \\(\\rho\\) evolves following the continuity equation.</p>"},{"location":"chapter_generative_model/langevin_dynamics/#gradient-descent-on-the-space-of-densities","title":"Gradient Descent on the Space of Densities","text":"<p>Now we are ready to answer the second question: what is the gradient of \\(F(\\rho)\\)?</p> <p>First, we need to specify what do we mean  by the gradient of \\(F(\\rho)\\) denoted as \\(\\text{grad}_{\\rho}F\\). By the continuity equation, we aim to find the right vector field \\(v\\) to minimize \\(F(\\rho)\\). Thus \\(\\text{grad}_{\\rho}F\\) should be a vector field.</p> <p>Let us first recall the finite-dimensional case. Given a function \\(f(x)\\), how can we \"rediscover\" the concept of gradient? One way is to consider the time derivative of \\(f(x(t))\\) along the trajectory \\(x(t)\\):</p> \\[ \\frac{d}{dt} f(x(t)) = \\left\\langle \\nabla f, \\frac{dx}{dt} \\right\\rangle \\] <p>This means if a vector \\(g\\) satisfies </p> \\[ \\left\\langle g, \\frac{dx}{dt} \\right\\rangle = \\frac{d}{dt} f(x(t)) \\] <p>then \\(g\\) is the gradient of \\(f\\).</p> <p>In the infinite-dimensional case, the inner product is replaced by the integral:</p> \\[ \\langle f, g \\rangle_{\\rho} = \\mathbb{E}_{\\rho}[fg] = \\int f(x) g(x)  \\rho(x) \\, dx \\] <p>Therefore, heuristically, the gradient of \\(F(\\rho)\\) should satisfy:</p> \\[ \\frac{d}{dt} F(\\rho) = \\left\\langle \\text{grad}_{\\rho}F, \\frac{d\\rho}{dt} \\right\\rangle_{\\rho} \\tag{1} \\] <p>We now start from the left hand side. Recalling the chain rule for finite-dimensional functions, if we have a function \\(f(x(t))\\) for a vector \\(x(t)\\), then the time derivative of \\(f\\) is given by:</p> \\[ \\frac{d}{dt} f(x(t)) = \\left\\langle \\nabla f(x), \\frac{dx}{dt} \\right\\rangle \\] <p>The chain rule for the infinite-dimensional  \\(\\rho\\) changes the inner product to integration:</p> \\[ \\frac{d}{dt} F(\\rho) = \\int \\frac{\\delta F}{\\delta \\rho} \\frac{d\\rho}{dt} \\, dx \\] <p>where \\(\\frac{\\delta F}{\\delta \\rho}\\) is the functional derivative of \\(F\\) with respect to \\(\\rho\\).  Plugging in the continuity equation \\(\\frac{d\\rho}{dt} = - \\nabla \\cdot (v \\rho)\\) to the above equation, we get:</p> \\[ \\begin{align*} \\frac{d}{dt} F(\\rho) &amp;= \\int \\frac{\\delta F}{\\delta \\rho} \\frac{d\\rho}{dt} \\, dx\\\\ &amp;= \\int \\frac{\\delta F}{\\delta \\rho} \\left( - \\nabla \\cdot (v \\rho) \\right) \\, dx \\\\ &amp;=  \\int \\left( \\nabla  \\frac{\\delta F}{\\delta \\rho} \\right) v \\rho  \\, dx\\\\ &amp;= \\left\\langle \\nabla \\frac{\\delta F}{\\delta \\rho}, v \\right\\rangle_{\\rho}, \\tag{2} \\end{align*}  \\] <p>where we apply the integration by parts in the third line:</p> \\[ \\int f \\nabla g \\, dx = - \\int \\nabla f \\cdot g \\, dx + f g |_{-\\infty}^{\\infty} = - \\int \\nabla f \\cdot g \\, dx \\] <p>if \\(f, g\\) vanish at the infinity. (Note that we abuse a lot of notations here. Please see the paper for rigorous derivation.)</p> <p>Therefore, by comparing (1) and (2), we have:</p> \\[ \\text{grad}_{\\rho}F = \\nabla \\frac{\\delta F}{\\delta \\rho} \\] <p>We are going to practice our infinite-dimensional calculus now. To compute \\(\\frac{\\delta F}{\\delta \\rho}\\), you can pretend that \\(\\rho\\) is a \"variable\" and \\(F\\) is a \"function\" of \\(\\rho\\). When taking the functional derivative, just taking the derivative of the integrand with respect to \\(\\rho\\). Let us start with the simplest case of linear functional, i.e., \\(L(\\rho) = \\int f(x) \\rho(x) \\, dx\\). Then </p> \\[ \\frac{\\delta L }{\\delta \\rho} = f \\text{ and }\\text{grad}_{\\rho} L = \\nabla \\frac{\\delta L}{\\delta \\rho} = \\nabla f \\] <p>Now let us try a more complicated case of  entropy functional, \\(H(\\rho) = - \\int \\rho \\log \\rho \\, dx\\). </p> \\[ \\frac{\\delta H }{\\delta \\rho} = -\\log \\rho - 1 \\text{ and }\\text{grad}_{\\rho} H = \\nabla \\frac{\\delta H}{\\delta \\rho} = \\nabla (-\\log \\rho - 1) = - \\frac{\\nabla \\rho}{\\rho} \\] <p>Therefore, the gradient of our functional loss \\(F(\\rho) = \\int f(x) \\rho(x) \\, dx - H(\\rho)\\) is given by:</p> \\[ \\text{grad}_{\\rho}F = \\nabla \\frac{\\delta F}{\\delta \\rho} = \\nabla f + \\frac{\\nabla \\rho}{\\rho} \\] <p>Thus we have answered the second question on the analog of the gradient  for the infinite-dimensional  loss function \\(F(\\rho)\\).</p> <p>Plugging in the negative gradient \\(-\\text{grad}_{\\rho}F\\) as the vector field \\(v\\) into the continuity equation, we get the Fokker-Planck equation:</p> \\[ \\begin{align*} \\frac{\\partial \\rho}{\\partial t} &amp;= - \\nabla \\cdot (\\rho \\cdot -\\text{grad}_{\\rho}F  ) \\\\ &amp;= - \\nabla \\cdot \\left( \\rho \\cdot \\left( \\nabla f - \\frac{\\nabla \\rho}{\\rho} \\right) \\right)\\\\ &amp; = - \\nabla \\cdot( \\rho \\nabla f) + \\Delta \\rho  \\end{align*} \\] <p>where the last equation is due to \\(\\nabla \\cdot \\nabla = \\Delta\\), where \\(\\Delta\\) is the Laplacian operator \\(\\Delta f =  \\sum_{i=1}^d \\frac{\\partial^2 f}{\\partial x_i^2}\\).</p> <p>Therefore, we have the following \"gradient algorithm\" for our loss function \\(F(\\rho)\\):</p> <p>Fokker-Planck Equation for Gradient Flow</p> <p>Let \\(p = e^{-f}\\) be the target distribution. The gradient algorithm for \\(\\min_\\rho \\text{KL}(\\rho \\| p)\\) is equivalent to the Fokker-Planck equation:</p> \\[ \\frac{\\partial \\rho}{\\partial t} = - \\nabla \\cdot( \\rho \\nabla f) + \\Delta \\rho \\] <p>Mathematicians will be angry about our derivation above. Our derivation above is totally heuristic though we get the correct answer. However, we will defend ourself by the following quote:</p> <p>An anonymous bad statistician:</p> <p>A bad statistician makes odd number of mistakes. A good statistician makes even number of mistakes.</p>"},{"location":"chapter_generative_model/langevin_dynamics/#sampling-from-the-fokker-planck-equation","title":"Sampling from the Fokker-Planck Equation","text":"<p>You might have forgotten why we are here. Let me remind you: we want to sample from the target distribution \\(p = e^{-f}\\) by solving the optimization problem \\(\\min_\\rho \\text{KL}(\\rho \\| p)\\). Our verbose derivation above is to show that the gradient descent analogy solving this problem is equivalent to the evolution of the density \\(\\rho\\) following the Fokker-Planck equation.</p> <p>But how could we use the Fokker-Planck equation to sample from \\(p\\)? Recalling the continuity equation, the evolution of \\(\\rho\\) following the continuity equation \\(\\frac{\\partial \\rho}{\\partial t} = - \\nabla \\cdot( \\rho v)\\) is equivalent to the movement of particles following the vector field \\(v\\).</p> <p>That means if the density \\(\\rho\\) evolves following</p> \\[ \\frac{\\partial \\rho}{\\partial t} = - \\nabla \\cdot( \\rho \\nabla f), \\] <p>then the movement of particles \\(x(t)\\) will follow the vector field  \\(- \\nabla f(x)\\), namely </p> \\[ x_{t+1} = x_t - \\epsilon \\nabla f(x_t). \\] <p>This is exactly the gradient descent algorithm.</p> <p>However, the Fokker-Planck equation has two terms on the right hand side:  \\(- \\nabla \\cdot( \\rho \\nabla f)\\) which gives the gradient descent, and \\(\\Delta \\rho\\) which we will discuss now.</p> <p>As the continuity equation considers the first term, we now consider the equation with only the Laplacian term on the right hand side:</p> \\[ \\frac{\\partial \\rho}{\\partial t} =  \\Delta \\rho \\] <p>This is the famous heat equation which you can check that the Gaussian distribution below is the solution:</p> \\[ \\rho(x,t) = \\frac{1}{(4\\pi t)^{d/2}} e^{-\\frac{|x|^2}{4t}}, \\text{ i.e., } x \\sim \\mathcal{N}(0,2t) \\] <p>In fact, the solution of the heat equation cannot be unique without the  initial condition at time \\(t=0\\), i.e., \\(\\rho(x,0)\\). We have the following heat equation solution:</p> <p>Heat Equation Solution</p> <p>The solution of the heat equation</p> \\[ \\frac{\\partial \\rho}{\\partial t} =  \\Delta \\rho, \\quad \\rho(x,0) = \\rho_0(x) \\] <p>is given by the density of adding \\(X_0 \\sim \\rho_0\\) with Gaussian noise:</p> \\[ X_t = X_0 + \\sqrt{2t} \\xi, \\quad \\xi \\sim \\mathcal{N}(0,I). \\] <p>Namely, the solution \\(\\rho(x,t)\\) as the density of \\(X_t\\) is the convolution of the initial density \\(\\rho_0\\) with the Gaussian distribution:</p> \\[ \\rho(x,t) = \\int \\rho_0(y) \\frac{1}{(4\\pi t)^{d/2}} e^{-\\frac{|x-y|^2}{4t}} \\, dy. \\] <p> You can check that the solution of the heat equation by plugging in the solution into the heat equation. We can see how the density changes over time under the heat equation in the animation above. You can see that the heat equation is called the diffusion equation as it smooths out the density over time.</p> <p>We can discretize \\(X_t = X_0 + \\sqrt{2t} \\xi\\) as an iterative process:</p> \\[ X_{t+1} = X_t + \\sqrt{2\\epsilon} \\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0,I). \\] <p>Now we have the good intuition on the two terms in the Fokker-Planck equation (3). The first term \\(- \\nabla \\cdot( \\rho \\nabla f)\\) gives the gradient descent, and the second term \\(\\Delta \\rho\\) add the Gaussian noise. </p> <p>Let's summarize the relationship between the Fokker-Planck equation and Langevin dynamics with the following diagram:</p> \\[ \\begin{array}{ccccl} \\text{Density Evolution} &amp; \\xrightarrow{\\text{Fokker-Planck}} &amp; \\frac{\\partial \\rho}{\\partial t} =&amp; -\\nabla \\cdot(\\rho \\nabla f) &amp; + \\Delta \\rho \\\\ \\updownarrow \\text{vector field}&amp; &amp;  &amp; \\updownarrow \\text{drift}&amp; \\updownarrow \\text{diffusion}\\\\ \\text{Particle Movement} &amp; \\xrightarrow{\\text{Langevin}} &amp; dX_t =&amp; -\\nabla f(X_t) dt &amp; + \\sqrt{2} dW_t \\end{array} \\] <p>Here \\(dW_t\\) is the Wiener process which you can think it as \\(\\epsilon \\xi_t\\) with \\(\\epsilon \\to 0\\).</p> <p>The top row represents how the probability density \\(\\rho\\) evolves over time according to the Fokker-Planck equation, while the bottom row shows how individual particles move according to Langevin dynamics. These two perspectives are equivalent: the Fokker-Planck equation describes the collective behavior of particles following Langevin dynamics.</p> <p>This duality gives us a powerful insight: we can sample from a complex distribution by simulating the movement of particles, rather than directly manipulating the density function.</p> <p>Therefore, we can sample from the target distribution \\(p\\) by translating the Fokker-Planck equation to the particle movement:</p> <p>Langevin Dynamics</p> <p>The Fokker-Planck equation </p> \\[ \\frac{\\partial \\rho}{\\partial t} =  - \\nabla \\cdot( \\rho \\nabla f) + \\Delta \\rho \\] <p>is equivalent to the Langevin dynamics:</p> \\[ \\frac{dX_t}{dt} = - \\nabla f(X_t) + \\sqrt{2} \\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0,I) \\] <p>which can be discretized as:</p> \\[ X_{t+1} = X_t - \\epsilon \\nabla f(X_t) + \\sqrt{2\\epsilon} \\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0,I), \\] <p>where \\(\\epsilon\\) is the step size.</p> <p>The above theory shows the connection between the sampling and the optimization:</p> <p>Langevin Dynamics Sampling</p> <p>We can sample the target distribution \\(p = e^{-f}\\) by running the Langevin dynamics:</p> \\[ X_{t+1} = X_t - \\epsilon \\nabla f(X_t) + \\sqrt{2\\epsilon} \\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0,I), \\] <p>The distribution of \\(X_t\\) converges to the target distribution \\(p\\) as \\(\\epsilon \\to 0\\) and \\(t \\to \\infty\\).</p> <p>You can see the convergence of the Langevin dynamics to the target distribution in the animation below:</p> <p></p> <p>Selecting an appropriate step size \\(\\epsilon\\) is crucial and presents similar challenges to determining the optimal learning rate in gradient descent. When \\(\\epsilon\\) is excessively large, we introduce too much noise per iteration, causing the chain to rapidly converge to a stationary distribution that significantly deviates from our target distribution. Conversely, if \\(\\epsilon\\) is too small, the samples will require an impractically long time to evolve from their starting positions, making the sampling process inefficient.</p> <p></p> <p>One way to have a robust way to sample using Langevin dynamics insensitive to the choice of step size \\(\\epsilon\\) is to use the Metropolis-adjusted Langevin algorithm (MALA). We will not go into the details of MALA here. You can refer to the paper for more details.</p>"},{"location":"chapter_language_model/","title":"Index","text":""},{"location":"chapter_language_model/#overview","title":"Overview","text":"<p>This chapter covers language models with utilzing the transformer architecture. We will also introduce the Hugging Face Transformers library and how to use it to fine-tune and generate text.</p> <p></p>"},{"location":"chapter_language_model/#lectures","title":"Lectures","text":"<ul> <li>Word Vectors</li> <li>Attention</li> <li>Transformer</li> <li>Hugging Face</li> </ul>"},{"location":"chapter_language_model/attention/","title":"Attention","text":""},{"location":"chapter_language_model/attention/#kernel-regression","title":"Kernel Regression","text":"<p>The Nadaraya-Watson estimator is a fundamental nonparametric regression technique that uses kernel functions to weight observations based on their proximity to the query point. This approach can be viewed as an early form of attention mechanism, where the model \"attends\" to training examples based on their relevance to the current query.</p> <p>Given a dataset of input-output pairs \\(\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n\\), the Nadaraya-Watson estimator predicts the output at a new query point \\(\\mathbf{q}\\) as:</p> \\[\\hat{f}(\\mathbf{q}) = \\frac{\\sum_{i=1}^n K(\\mathbf{q}, \\mathbf{x}_i) y_i}{\\sum_{i=1}^n K(\\mathbf{q}, \\mathbf{x}_i)}\\] <p>where \\(K(\\mathbf{q}, \\mathbf{x}_i)\\) is a kernel function that measures the similarity between the query point \\(\\mathbf{q}\\) and the training point \\(\\mathbf{x}_i\\).</p> <p></p> <p>Some common kernels are</p> \\[\\begin{aligned} \\alpha(\\mathbf{q}, \\mathbf{k}) &amp; = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{k}\\|^2 \\right) &amp;&amp; \\textrm{Gaussian;} \\\\ \\alpha(\\mathbf{q}, \\mathbf{k}) &amp; = 1 \\textrm{ if } \\|\\mathbf{q} - \\mathbf{k}\\| \\leq 1 &amp;&amp; \\textrm{Boxcar;} \\\\ \\alpha(\\mathbf{q}, \\mathbf{k}) &amp; = \\mathop{\\mathrm{max}}\\left(0, 1 - \\|\\mathbf{q} - \\mathbf{k}\\|\\right) &amp;&amp; \\textrm{Epanechikov.} \\end{aligned} \\] <p>This can be rewritten in a form that highlights its connection to attention mechanisms:</p> \\[\\hat{f}(\\mathbf{q}) = \\sum_{i=1}^n \\alpha(\\mathbf{q}, \\mathbf{x}_i) y_i\\] <p>where the attention weights \\(\\alpha(\\mathbf{q}, \\mathbf{x}_i)\\) are defined as:</p> \\[\\alpha(\\mathbf{q}, \\mathbf{x}_i) = \\frac{K(\\mathbf{q}, \\mathbf{x}_i)}{\\sum_{j=1}^n K(\\mathbf{q}, \\mathbf{x}_j)}\\] <p>These attention weights satisfy \\(\\sum_{i=1}^n \\alpha(\\mathbf{q}, \\mathbf{x}_i) = 1\\), forming a probability distribution over the training points.</p> <p>Let us consider the Gaussian kernel:</p> \\[\\alpha(\\mathbf{q}, \\mathbf{x}_i) = \\frac{\\exp\\left(-\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{x}_i\\|^2 \\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{x}_j\\|^2 \\right)}\\] <p>Then we have the kernel weighted estimator is weighted softmax:</p> \\[\\hat{f}(\\mathbf{q}) = \\sum_{i=1}^n \\text{softmax}(-\\|\\mathbf{q} - \\mathbf{x}_i\\|^2/2) y_i\\] <p>This means that if a key \\(\\mathbf{x}_i\\) is close to the query \\(\\mathbf{q}\\), then we will assign more weight \\(\\alpha(\\mathbf{q}, \\mathbf{x}_i)\\) to \\(y_i\\), i.e., the output \\(y_i\\) will have more attention on the prediction.</p> <p>We can generalize the above Gaussian kernel to a more general case:</p> \\[ a(\\mathbf{q}, \\mathbf{k}_i) = \\text{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^n \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\] <p>where \\(a(\\mathbf{q}, \\mathbf{k}_i)\\) is a similarity function between the query \\(\\mathbf{q}\\) and the key \\(\\mathbf{k}_i\\). One of the most common choices is the dot product:</p> \\[ a(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q}^\\top \\mathbf{k} \\] <p>This leads to the attention mechanism.</p>"},{"location":"chapter_language_model/attention/#attention-mechanism","title":"Attention Mechanism","text":"<p>Given an input \\(X\\) matrix, there are three weight matrices \\(W_q, W_k, W_v\\) to learn such that </p> \\[ \\begin{align*} Q = X W_q = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_n], \\\\ K = X W_k = [\\mathbf{k}_1, \\mathbf{k}_2, \\ldots, \\mathbf{k}_n], \\\\ V = X W_v = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n] \\end{align*} \\] <p>So the attention mechanism is:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d}}\\right) V = \\sum_{i=1}^n \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i, \\] <p>where \\(\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\exp(\\mathbf{q}^\\top \\mathbf{k}_i)}{\\sum_{j=1}^n \\exp(\\mathbf{q}^\\top \\mathbf{k}_j)}\\) is the attention weight.</p> <p>In attention, the query matches all keys softly, to a weight between 0 and 1. The keys\u2019 values are multiplied by the weights and summed.</p> <p></p> <p>So to summarize the self-attention mechanism with the input \\(X\\), we have the following steps:</p> <ol> <li>Project the input \\(X\\) to three matrices \\(Q, K, V\\) with weight matrices \\(W_q, W_k, W_v\\).</li> <li>Compute the attention weights \\(\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\exp(\\mathbf{q}^\\top \\mathbf{k}_i)}{\\sum_{j=1}^n \\exp(\\mathbf{q}^\\top \\mathbf{k}_j)}\\) for each query \\(\\mathbf{q}\\) and each key \\(\\mathbf{k}_i\\).</li> <li>Multiply the values \\(\\mathbf{v}_i\\) by the attention weights and sum them up to get the output.</li> </ol> <p>In summary, we have</p> \\[ f(X)  = \\text{Attention}(W_qX, W_kX, W_vX) = \\text{softmax}\\left(\\frac{X W_q (X W_k)^\\top}{\\sqrt{d}}\\right) X W_v  \\] <p></p>"},{"location":"chapter_language_model/attention/#pytorch-implementation-of-self-attention","title":"PyTorch Implementation of Self-Attention","text":"<p>We can implement the self-attention mechanism in PyTorch as follows:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_dim):\n        \"\"\"    \n        Args:\n            embed_dim (int): Dimensionality of the input embeddings (and output dimensions of the linear transforms).\n        \"\"\"\n        super(SelfAttention, self).__init__()\n        self.embed_dim = embed_dim \n        # Linear projection for queries, keys, and values.\n        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n\n    def forward(self, X):\n        # Compute linear projections (queries, keys, values)\n        Q = self.W_q(X)  # Shape: (batch_size, sequence_length, embed_dim)\n        K = self.W_k(X)  # Shape: (batch_size, sequence_length, embed_dim)\n        V = self.W_v(X)  # Shape: (batch_size, sequence_length, embed_dim)\n\n        # Compute attention scores \n        scores = torch.matmul(Q, K.transpose(-2, -1))  # Shape: (batch_size, sequence_length, sequence_length)\n        scores = scores / math.sqrt(self.embed_dim)\n\n        attention_weights = F.softmax(scores, dim=-1)  # Shape: (batch_size, sequence_length, sequence_length)\n        # Multiply the attention weights with the values to get the final output.\n        output = torch.matmul(attention_weights, V)  # Shape: (batch_size, sequence_length, embed_dim)\n        return output\n</code></pre>"},{"location":"chapter_language_model/attention/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Attention treats each word\u2019s representation as a query to access and incorporate information from a set of values. Attention is parallelizable, and solves bottleneck issues.</p> <p>Multi-head attention extends the basic attention mechanism by allowing the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function with \\(d\\)-dimensional keys, values, and queries, multi-head attention performs the attention function in parallel \\(h\\) times, with different, learned linear projections to \\(d_k\\), \\(d_k\\), and \\(d_v\\) dimensions. These parallel attention outputs, or \"heads,\" are then concatenated and linearly transformed to produce the final output. This approach enables the model to capture different aspects of the input sequence simultaneously.</p> <p>In specific, for each head \\(i\\), we have the linear weights \\(W_q^{(i)}, W_k^{(i)}, W_v^{(i)}\\) to map the input \\(X\\) to the head</p> \\[ h_i = \\text{Attention}(W_q^{(i)}X, W_k^{(i)}X, W_v^{(i)}X) \\] <p>Then we have the multi-head attention by concatenating all the heads together and project them to the output space:</p> \\[ \\text{MultiHead}(X) = \\text{Concat}(h_1, h_2, \\ldots, h_h) W^O \\] <p></p> <p>PyTorch has a built-in function for the multi-head attention mechanism <code>torch.nn.MultiheadAttention(embed_dim, num_heads)</code> where <code>embed_dim</code> is the dimension of the input embeddings and <code>num_heads</code> is the number of heads.</p>"},{"location":"chapter_language_model/hg_transformers/","title":"Hugging Face and Transformers Package","text":"<p>In this lecture, we'll explore Hugging Face's Transformers library\u2014a powerful Python package for working with state-of-the-art NLP (Natural Language Processing) models. You can refer to the hugging face course for more details.</p>"},{"location":"chapter_language_model/hg_transformers/#hugging-face-website","title":"Hugging Face \ud83e\udd17 Website","text":"<p>Hugging Face \ud83e\udd17 is often referred to as the \"GitHub for AI models\" - a central hub where researchers and developers can share, discover, and collaborate on machine learning models, datasets, and applications. The platform hosts thousands of pre-trained models that anyone can download and use.</p> <ul> <li> <p>Model Repository: Access thousands of pre-trained models for various tasks including language modeling, computer vision, audio, and more. Each model has detailed documentation, including their capabilities, limitations, and intended uses.</p> </li> <li> <p>Datasets: A collection of public datasets for training and evaluating models.</p> </li> <li> <p>Spaces: Interactive web applications to demonstrate AI capabilities without requiring any setup.</p> </li> </ul> <p>The Hugging Face \ud83e\udd17 ecosystem significantly lowers the barrier to entry for working with advanced AI models, making cutting-edge NLP accessible to developers of all skill levels.</p>"},{"location":"chapter_language_model/hg_transformers/#transformers-package","title":"Transformers Package","text":"<p>Hugging Face \ud83e\udd17 provides a powerful Python package called Transformers that simplifies working with many different NLP models. You can refer to the official documentation for more details of API usage.</p> <p>To use the Transformers package, you need to install the following packages:</p> <pre><code>pip install torch transformers datasets accelerate\n</code></pre> <p>In order to use the pre-trained models, you need to check the model hub to find the model you want to use. The model string is in the format of <code>{model_owner}/{model_name}</code>.  We will use the <code>AutoTokenizer</code> and <code>AutoModel</code> classes to load the tokenizer and model.</p> <p>Below is an example of how to load the tokenizer and model for DistilBERT, a lightweight and efficient transformer model:</p> <pre><code>from transformers import AutoTokenizer, AutoModel\n\nmodel_name = \"distilbert/distilbert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name) # Load the tokenizer\nmodel = AutoModel.from_pretrained(model_name) # Load the model\nprint(tokenizer)\nprint(model)\n</code></pre>"},{"location":"chapter_language_model/hg_transformers/#tokenization","title":"Tokenization","text":"<p>Once you have loaded the tokenizer, you can use the <code>tokenize</code> method to convert raw text into tokens.</p> <pre><code>text = \"Hello, how are you?\"\ntokens = tokenizer.tokenize(text)\n\nprint(\"Tokens:\", tokens)\n# Example output: ['Hello', ',', 'how', 'are', 'you', '?']\n</code></pre> <p>Models don't directly understand text\u2014they use numerical representations called token IDs.</p> <pre><code># Convert tokens to token IDs\ntoken_ids = tokenizer.encode(text)\nprint(\"Token IDs:\", token_ids)\n\n# Convert token IDs back to tokens (includes special tokens such as [CLS], [SEP])\ntokens_with_special = tokenizer.convert_ids_to_tokens(token_ids)\nprint(\"Tokens with special characters:\", tokens_with_special)\n# Example output: ['[CLS]', 'Hello', ',', 'how', 'are', 'you', '?', '[SEP]']\n</code></pre> <p>Models require input tensors. Here, we tokenize and convert our input text to PyTorch tensors:</p> <pre><code>inputs = tokenizer(text, return_tensors='pt')  # 'pt' stands for PyTorch tensors\nprint(inputs)\n# {'input_ids': \n#    tensor([[ 101, 8667,  117, 1293, 1132, 1128,  136,  102]]), \n#  'attention_mask': \n#    tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n# }\n</code></pre> <p>The <code>inputs</code> dictionary contains keys:</p> <ul> <li><code>input_ids</code>: The token IDs of the input text with dimensions <code>(batch_size, sequence_length)</code>.</li> <li><code>attention_mask</code>: A binary mask indicating which tokens are real (1) and which are padding (0) with dimensions <code>(batch_size, sequence_length)</code>.</li> </ul> <p>You can then pass the <code>inputs</code> dictionary to the model using two different methods:</p> <pre><code># Option 1: Pass the entire inputs dictionary\noutputs = model(**inputs)\n# Option 2: Pass the input IDs and attention mask separately\noutputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n</code></pre> <p>Batch of sentences</p> <p>You can also pass batch of sentences to the model. Tokenizing multiple sentences at once can require padding to ensure consistent input lengths:</p> <pre><code>texts = [\n    \"Hello, how are you?\",\n    \"I'm fine, thank you! And you?\",\n    \"I'm not fine.\"\n]\n\n# Pad sentences to match the length of the longest sentence\nmodel_inputs = tokenizer(texts, padding=True, return_tensors='pt')\n\nprint(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n# Print input ids\nprint(model_inputs['input_ids'])\n# Print attention mask\nprint(model_inputs['attention_mask'])\n# Pad token: [PAD] | Pad token id: 0\n# tensor([[101,8667,117,1293,1132,1128,136,102,0,0,0,0,0],\n#         [101,146,112,182,2503,117,6243,1128,106,1262,1128,136,102],\n#         [101,146,112,182,1136,2503,119,102,0,0,0,0,0]])\n# tensor([[1,1,1,1,1,1,1,1,0,0,0,0,0],\n#         [1,1,1,1,1,1,1,1,1,1,1,1,1],\n#         [1,1,1,1,1,1,1,1,0,0,0,0,0]])\n</code></pre>"},{"location":"chapter_language_model/hg_transformers/#model-parameters","title":"Model Parameters","text":"<p>Obtain embeddings for multiple sentences and explore model configuration:</p> <pre><code>model_outputs = model(**model_inputs)\ntoken_embeddings = model_outputs.last_hidden_state\n\nprint(\"Token embeddings shape (multiple sentences):\", token_embeddings.shape)\n\n# Inspect model configuration (details like number of layers, hidden sizes)\nprint(\"Model configuration:\", model.config)\n</code></pre> <p>To get the middle layer of the model, you can set <code>output_hidden_states=True</code> when initializing the model. Then the model output will have <code>hidden_states</code> and <code>attentions</code> attributes.</p> <pre><code>model = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\nmodel.eval()\ninput_str = \"Hello, how are you?\"\nmodel_inputs = tokenizer(input_str, return_tensors=\"pt\")\nwith torch.no_grad():\n    model_output = model(**model_inputs)\n\nprint(f\"Hidden state size (per layer):  {model_output.hidden_states[0].shape}\")\nprint(f\"Attention head size (per layer): {model_output.attentions[0].shape}\")     # (layer, head_number, query_word_idx, key_word_idxs)\n# Attention is softmax(K^T * Q / sqrt(d_k))\n</code></pre> <p>You can visualize the attention scores from different layers of different heads using the following code:</p> <pre><code>import matplotlib.pyplot as plt\nn_layers = len(model_output.attentions)\nn_heads = len(model_output.attentions[0][0])\nfig, axes = plt.subplots(6, 12)\nfig.set_size_inches(18.5*2, 10.5*2)\nfor layer in range(n_layers):\n    for i in range(n_heads):\n        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n        axes[layer][i].set_xticks(list(range(8)))\n        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n        axes[layer][i].set_yticks(list(range(8)))\n        axes[layer][i].set_yticklabels(labels=tokens)\n\n        if layer == 5:\n            axes[layer, i].set(xlabel=f\"head={i}\")\n        if i == 0:\n            axes[layer, i].set(ylabel=f\"layer={layer}\")\n\nplt.subplots_adjust(wspace=0.3)\nplt.show()\n</code></pre> <p></p>"},{"location":"chapter_language_model/hg_transformers/#loading-datasets","title":"Loading Datasets","text":"<p>Similar to the torchvision, Hugging Face \ud83e\udd17 provides a <code>datasets</code> package that allows you to load and preprocess datasets. You can refer to the datasets documentation for more details.</p> <p>You can find many datasets in the datasets hub. Here we will use the imdb dataset for sentiment analysis with the reviews as the text and the binary labels {0, 1} as the negative and positive targets.</p> <pre><code># load dataset\nfrom datasets import load_dataset, DatasetDict\n# DataLoader(zip(list1, list2))\ndataset_name = \"stanfordnlp/imdb\"\nimdb_dataset = load_dataset(dataset_name)\n</code></pre> <p>We take a subset of the dataset for demonstration.</p> <p><pre><code># Just take the first 50 tokens for speed/running on cpu\ndef truncate(example):\n    return {\n        'text': \" \".join(example['text'].split()[:50]),\n        'label': example['label']\n    }\n# Take 128 random examples for train and 32 validation\nsmall_imdb_dataset = DatasetDict(\n    train=imdb_dataset['train'].shuffle(seed=1111).select(range(128)).map(truncate),\n    val=imdb_dataset['train'].shuffle(seed=1111).select(range(128, 160)).map(truncate),\n)    \n</code></pre> We then use the tokenizer to tokenize the text and convert the text to tokens.</p> <pre><code>small_tokenized_dataset = small_imdb_dataset.map(\n    lambda example: tokenizer(example['text'], padding=True, truncation=True), # It truncates any input text that exceeds the model\u2019s maximum token length\n    batched=True,\n    batch_size=16\n)\n\nsmall_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\nsmall_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\nsmall_tokenized_dataset.set_format(\"torch\")\n</code></pre> <p>Now we can use the <code>DataLoader</code> to load the dataset.</p> <pre><code># %% Now we can create a DataLoader as usual\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\neval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)\n</code></pre> <p>The remaining part is regular training loop as the previous chapter. For models in hugging face, we can directly get the loss by <code>model(**input).loss</code>.</p> <pre><code>from tqdm import tqdm\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n\nnum_epochs = 1\nnum_training_steps = len(train_dataloader)\noptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\nlr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\nbest_val_loss = float(\"inf\")\nprogress_bar = tqdm(range(num_training_steps))\nfor epoch in range(num_epochs):\n    # training\n    model.train()\n    for batch_i, batch in enumerate(train_dataloader):\n        # batch = ([text1, text2], [0, 1])\n        output = model(**batch)\n        optimizer.zero_grad()\n        output.loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        progress_bar.update(1)\n\n    # validation\n    model.eval()\n    for batch_i, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            output = model(**batch)\n        loss += output.loss\n\n    avg_val_loss = loss / len(eval_dataloader)\n    print(f\"Validation loss: {avg_val_loss}\")\n    if avg_val_loss &lt; best_val_loss:\n        print(\"Saving checkpoint!\")\n        best_val_loss = avg_val_loss\n        # torch.save({\n        #     'epoch': epoch,\n        #     'model_state_dict': model.state_dict(),\n        #     'optimizer_state_dict': optimizer.state_dict(),\n        #     'val_loss': best_val_loss,\n        #     },\n        #     f\"checkpoints/epoch_{epoch}.pt\"\n        # )\n</code></pre>"},{"location":"chapter_language_model/hg_transformers/#hugging-face-trainer","title":"Hugging Face Trainer","text":"<p>Hugging Face \ud83e\udd17 provides a <code>Trainer</code> class that simplifies the training loop. You can refer to the Trainer documentation for more details. With the <code>Trainer</code>, we can write the above training loop as follows:</p> <pre><code>from transformers import TrainingArguments, Trainer\n\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n\narguments = TrainingArguments(\n    output_dir=\"sample_hf_trainer\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    load_best_model_at_end=True,\n    seed=224\n)\n\n\ndef compute_metrics(eval_pred):\n    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    # calculates the accuracy\n    return {\"accuracy\": np.mean(predictions == labels)}\n\n\ntrainer = Trainer(\n    model=model,\n    args=arguments,\n    train_dataset=small_tokenized_dataset['train'],\n    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n</code></pre>"},{"location":"chapter_language_model/hg_transformers/#fine-tuning-a-model","title":"Fine-tuning a model","text":"<p>Similar to the previous chapter, we can fine-tune a model for a specific task. Here we will fine-tune the model from hugging face by freezing the base encoder.</p> <pre><code>model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\nprint(model)\nfor param in model.distilbert.parameters():\n    param.requires_grad = False  # freeze base encoder\n# Continue training the model\n</code></pre> <p>Fine-tuning a language model has become a complex task due to the large scale of the models nowadays. The code above is just a simple example to only fine-tune the head of the model.</p> <p>For more complicated fine-tuning, you can read the hugging face parameter-efficient fine-tuning (PEFT) tutorial for more sophisticated fine-tuning strategies. There are also more wrapped packages like Axolotl for easier LLM fine-tuning.</p> <p>We also encourage you to read the blog for a guide on how to fine-tune LLMs in 2025.</p>"},{"location":"chapter_language_model/hg_transformers/#model-inference","title":"Model Inference","text":"<p>Although pretrained transformers generally share similar architectures, they require task-specific \"heads\" - additional layers of weights that need training for particular tasks like sequence classification or question answering. Hugging Face simplifies this by providing specialized model classes that automatically configure the appropriate architecture. For example, DistilBERT has the following models for different tasks:</p> <ul> <li><code>DistilBertModel</code>: The base model for all the tasks.</li> <li><code>DistilBertForMaskedLM</code>: For masked language modeling.</li> <li><code>DistilBertForSequenceClassification</code>: For sequence classification.</li> <li><code>DistilBertForMultipleChoice</code>: For multiple choice tasks.</li> <li><code>DistilBertForTokenClassification</code>: For token classification tasks.</li> <li><code>DistilBertForQuestionAnswering</code>: For question answering tasks.</li> </ul> <p>You can also load different tasks models by <code>AutoModelFor*</code>:</p> <ul> <li><code>AutoModelForSequenceClassification</code>: For sequence classification.</li> <li><code>AutoModelForMaskedLM</code>: For masked language modeling.</li> <li><code>AutoModelForMultipleChoice</code>: For multiple choice tasks.</li> <li><code>AutoModelForTokenClassification</code>: For token classification tasks.</li> <li><code>AutoModelForQuestionAnswering</code>: For question answering tasks.</li> </ul> <p>Below is an example of how to load the model for sequence classification using two different methods.</p> <pre><code>from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertModel\nprint('Loading base model')\nbase_model = DistilBertModel.from_pretrained('distilbert-base-cased')\n# Method 1\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n# Method 2\nmodel = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n</code></pre> <p>As we mentioned in the previous lecture, BERT is a encoder model which is proper for classification tasks.</p>"},{"location":"chapter_language_model/hg_transformers/#language-generation","title":"Language Generation","text":"<p>Hugging Face also has the encoder models like GPT-2.</p> <p>We can use the <code>AutoModelForCausalLM</code> to load the GPT-2 model and generate text.</p> <pre><code>from transformers import AutoModelForCausalLM\ngpt2 = AutoModelForCausalLM.from_pretrained('distilgpt2')\n\nprompt = \"Once upon a time\"\ntokenized_prompt = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n\nfor i in range(10):\n    output = gpt2.generate(**tokenized_prompt,\n                  max_length=50,\n                  do_sample=True,\n                  top_p=0.9)\n\n    print(f\"{i + 1}) {gpt2_tokenizer.batch_decode(output)[0]}\")\n</code></pre>"},{"location":"chapter_language_model/hg_transformers/#pipeline","title":"Pipeline","text":"<p>Hugging Face \ud83e\udd17 also provides a <code>pipeline</code> function that simplifies the inference of the model. You can refer to the pipeline documentation for more details.</p> <pre><code>from transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\")\ninput_text = \"I've been waiting for a Hugging Face course my whole life.\"\nresult = classifier(input_text)\nprint(result)\n# [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n\n# You can also specify the model and tokenizer to use\nclassifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\nresult = classifier(input_text)\nprint(result)\n# [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n</code></pre> <p>You can also use the pipeline for language generation.</p> <pre><code>generator = pipeline(\"text-generation\")\nresult = generator(\"Hello, I've been\")\n# [{'label': 'POSITIVE', 'score': 0.9982948899269104}]\n\n# You can specify the model and tokenizer to use\ngenerator = pipeline(\"text-generation\", model=\"distilgpt2\")\nresult = generator(\"Hello, I've been\", max_length=50, do_sample=True, top_p=0.9)\n</code></pre>"},{"location":"chapter_language_model/hg_transformers/#accelerate-package","title":"Accelerate Package","text":"<p>Hugging Face \ud83e\udd17 also provides a package called Accelerate that simplifies the distributed training especially for multi-GPU and mixed precision training. So you do not need to worry the details we discussed in the previous chapter. You can refer to the Accelerate documentation for more details.</p> <p>To begin with, you can set up your environment using the accelerate config command. This interactive setup detects your hardware and allows you to specify configurations like distributed setups and mixed precision training.</p> <pre><code>accelerate config\n</code></pre> <p>To integrate Accelerate into your existing PyTorch training script, follow these modifications:</p> <ul> <li>Import and Initialize Accelerator:</li> </ul> <pre><code>from accelerate import Accelerator\n\naccelerator = Accelerator()\n</code></pre> <ul> <li>Handling Batch Sizes: Be mindful of how batch sizes are handled in distributed training. By default, the effective batch size is the per-device batch size multiplied by the number of devices. To maintain the same batch size regardless of the number of devices, initialize the Accelerator with <code>split_batches=True</code>:</li> </ul> <pre><code>accelerator = Accelerator(split_batches=True)\n</code></pre> <ul> <li>Prepare Your Objects: Pass your model, optimizer, dataloaders, and any other relevant components to the prepare method. This ensures they are placed on the appropriate devices and are ready for distributed training:</li> </ul> <pre><code>model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n</code></pre> <ul> <li> <p>Adjust Device Placement: Remove any manual <code>.to(device)</code> or <code>.cuda()</code> calls, as Accelerate manages device placement automatically. If you need to reference the device, use <code>accelerator.device</code>.</p> </li> <li> <p>Backward Pass: Replace <code>loss.backward()</code> with <code>accelerator.backward(loss)</code> to ensure compatibility with different distributed setups:</p> </li> </ul> <pre><code>loss = model(input_ids, attention_mask, labels).loss\naccelerator.backward(loss)\n</code></pre> <ul> <li>Launching Your Script: After modifying your script, run it using the accelerate launch command:</li> </ul> <pre><code>accelerate launch --config_file accelerate_config.yaml your_script.py\n</code></pre> <p>Here is an example of how to use the accelerate package to train the distilbert model for sequence classification.</p> <pre><code># train.py\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom tqdm import tqdm\n\ndef preprocess_function(examples, tokenizer):\n    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n\ndef main():\n    accelerator = Accelerator()\n    # Load the IMDb dataset\n    raw_datasets = load_dataset(\"imdb\")\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n    # Preprocess the datasets\n    tokenized_datasets = raw_datasets.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n    # Split datasets\n    train_dataset = tokenized_datasets[\"train\"]\n    eval_dataset = tokenized_datasets[\"test\"]\n    # Create data loaders\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n    eval_dataloader = DataLoader(eval_dataset, batch_size=8)\n    # Load the model\n    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n    # Define the optimizer\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    # Prepare everything with `accelerator`\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n    # Learning rate scheduler\n    num_training_steps = 3 * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n    )\n    # Training loop\n    model.train()\n    for epoch in range(3):\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n    # Save the fine-tuned model\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\"distilbert-imdb-accelerate\", save_function=accelerator.save)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>After saving the script as train.py, execute it using the accelerate launch command:</p> <pre><code>accelerate launch train.py\n</code></pre>"},{"location":"chapter_language_model/transformer/","title":"Transformer","text":"<p>The Transformer is a neural network architecture that is used for natural language processing tasks. It was introduced in the paper Attention is All You Need based on the attention mechanism. We will first introduce the architecture of the Transformer and then the training process.</p>"},{"location":"chapter_language_model/transformer/#transformer-block","title":"Transformer block","text":"<p>The Transformer block is the main building block of the Transformer.</p> <p>Let \\(X \\in \\mathbb{R}^{n \\times d}\\) be the input matrix, where \\(n\\) is the sequence length and \\(d\\) is the embedding dimension. The Transformer block consists of the following ope`rations:</p> <ul> <li>Multi-Head Attention:</li> </ul> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>For multi-head attention with \\(h\\) heads:</p> \\[ \\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O \\] <p>where each head is:</p> \\[ \\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V) \\] <ul> <li>Residual Connection and Layer Normalization (Add &amp; Norm):</li> </ul> \\[ X' = \\text{LayerNorm}(X + \\text{MultiHead}(X)) \\] <p>where the layer normalization is similar to the batch normalization but instead of computing the mean and variance over the batch, we compute them over the embedding dimension. In PyTorch, you can use <code>torch.nn.LayerNorm</code> to implement the layer normalization.</p> <ul> <li>Feed-Forward Network (FFN):</li> </ul> \\[ \\text{FFN}(X') = \\text{ReLU}(X'W_1 + b_1)W_2 + b_2 \\] <ul> <li>Second Residual Connection and Layer Normalization (Add &amp; Norm):</li> </ul> \\[ \\text{Output} = \\text{LayerNorm}(X' + \\text{FFN}(X')) \\] <p></p> <p>Using PyTorch <code>torch.nn.MultiheadAttention</code>, we can implement the Transformer block as follows:</p> <pre><code>import torch.nn as nn\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n        \"\"\"\n        Args:\n            embed_dim (int): Dimensionality of the input embeddings.\n            num_heads (int): Number of attention heads.\n            ff_hidden_dim (int): Hidden layer dimensionality in the feed-forward network.\n        \"\"\"\n        super(TransformerBlock, self).__init__()\n\n        # Multi-head attention layer. We use batch_first=True so that input shape is (batch_size, sequence_length, embed_dim).\n        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, \n                                         num_heads=num_heads,\n                                         batch_first=True)\n\n        # First layer normalization applied after the multi-head attention residual addition.\n        self.attention_norm = nn.LayerNorm(embed_dim)\n\n        # Feed-forward network: two linear layers with ReLU activation.\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, ff_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(ff_hidden_dim, embed_dim)\n        )\n\n        # Second layer normalization after the feed-forward residual addition.\n        self.ffn_norm = nn.LayerNorm(embed_dim)\n\n\n    def forward(self, x, attn_mask=None, key_padding_mask=None):\n        # Apply Multi-Head Attention (self-attention) where Q = K = V = x.\n        # attn_output shape: (batch_size, sequence_length, embed_dim)\n        attn_output = self.mha(x, x, x, need_weights=False) # need_weights=False to avoid computing the attention weights\n\n        # First residual connection and layer normalization.\n        # X' = LayerNorm(x + attn_output)\n        x = self.attention_norm(x + attn_output)\n        # Feed-Forward Network (FFN)\n        ffn_output = self.ffn(x)\n        # Second residual connection and layer normalization.\n        # Output = LayerNorm(x + ffn_output)\n        output = self.ffn_norm(x + ffn_output)\n        return output\n</code></pre>"},{"location":"chapter_language_model/transformer/#transformer-encoder","title":"Transformer encoder","text":"<p>The Transformer encoder is a stack of multiple Transformer blocks and connect to a final fully connected layer for classification output.</p> <p>Using the <code>TransformerBlock</code> we defined above, we can build the encoder as follows:</p> <pre><code>class TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim, num_layers):\n        super(TransformerEncoder, self).__init__()\n        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, ff_hidden_dim) for _ in range(num_layers)])\n\n    def forward(self, x, attn_mask=None, key_padding_mask=None):\n        for block in self.blocks:\n            x = block(x, attn_mask, key_padding_mask)\n        return x\n</code></pre> Model Layers Hidden Size Attention Heads Feedforward Size Parameters BERT-Base 12 768 12 3072 110M BERT-Large 24 1024 16 4096 340M DistilBERT 6 768 12 3072 66M <p></p>"},{"location":"chapter_language_model/transformer/#transformer-decoder","title":"Transformer decoder","text":"<p>The Transformer decoder is similar to the encoder but with a key difference: it uses masked self-attention in its first sublayer. This masking prevents the decoder from attending to future positions during training, which is essential for autoregressive generation.</p>"},{"location":"chapter_language_model/transformer/#masked-self-attention","title":"Masked Self-Attention","text":"<p>In the decoder's masked self-attention, we modify the attention mechanism to ensure that the prediction for position \\(i\\) can only depend on known outputs at positions less than \\(i\\). This is achieved by masking future positions in the attention weights:</p> \\[ \\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V \\] <p>where \\(M\\) is a mask matrix with:</p> \\[ M_{ij} =  \\begin{cases}  0 &amp; \\text{if } i \\geq j \\\\ -\\infty &amp; \\text{if } i &lt; j  \\end{cases} \\] <p></p> <p>When we apply softmax to a row containing \\(-\\infty\\) values, those positions effectively receive \\(0\\) attention weight, preventing information flow from future tokens.</p> <p></p> <p>The Transformer decoder stacks multiple masked self-attention layers. Modern generative language models like GPT-2 and GPT-3 use the decoder-only architecture with a stack of masked self-attention layers followed by a feed-forward network.</p> <p></p> <p>GPT-2 is so far the last open-sourced model from OpenAI. It has 124M, 355M, and 774M parameters for small, medium, and large models, respectively.</p> Model Layers Hidden Size Attention Heads Feedforward Size Parameters GPT-2 Small 12 768 12 3072 124M GPT-2 Medium 24 1024 16 4096 355M GPT-2 Large 36 1280 20 5120 774M"},{"location":"chapter_language_model/transformer/#encoder-decoder-transformer","title":"Encoder-Decoder Transformer","text":"<p>The encoder-decoder transformer is a variant of the Transformer that uses both encoder and decoder blocks. It is used for sequence-to-sequence tasks such as translation and summarization.</p> <p></p>"},{"location":"chapter_language_model/transformer/#choosing-transformer-architecture","title":"Choosing Transformer Architecture","text":"<p>We list below the best use cases for each type of transformer architecture.</p> <p>Encoder-Only Models</p> <ul> <li>Best for: Understanding and analyzing input text (classification, entity recognition, sentiment analysis)</li> <li>Examples: BERT, RoBERTa, DistilBERT</li> <li>Characteristics: Bidirectional attention (can see full context in both directions)</li> <li>Use when: Your task requires deep understanding of input text without generating new text</li> </ul> <p>Decoder-Only Models</p> <ul> <li>Best for: Text generation tasks (completion, creative writing, chat)</li> <li>Examples: GPT-2</li> <li>Characteristics: Autoregressive generation with masked self-attention</li> <li>Use when: Your primary goal is to generate coherent, contextually relevant text</li> </ul> <p>Encoder-Decoder Models</p> <ul> <li>Best for: Sequence-to-sequence tasks (translation, summarization)</li> <li>Examples: T5, BART, Pegasus</li> <li>Characteristics: Encoder processes input, decoder generates output based on encoder representations</li> <li>Use when: Your task involves transforming one sequence into another related sequence</li> </ul>"},{"location":"chapter_language_model/transformer/#transformer-in-pytorch","title":"Transformer in PyTorch","text":"<p>PyTorch provides a <code>torch.nn.Transformer</code> module that implements the Transformer architecture. We will use this module to implement the encoder and decoder of the Transformer.</p> <pre><code>import torch.nn as nn\nimport torch\n# Transformer encoder\nencoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\nencoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\nsrc = torch.rand(10, 32, 512)\nout = encoder(src)\n\n# Transformer decoder\ndecoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\ndecoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n\ntgt = torch.rand(10, 32, 512)\nmemory = torch.rand(10, 32, 512) # the output of the last layer of the encoder\nout = decoder(tgt, memory)\n\n# Transformer encoder-decoder\nencoder_decoder = nn.Transformer(encoder, decoder)\nsrc = torch.rand(10, 32, 512)\ntgt = torch.rand(10, 32, 512)\nout = encoder_decoder(src, tgt)\n</code></pre>"},{"location":"chapter_language_model/wordvec/","title":"Word Vectors","text":"<p>Word vectors, also known as word embeddings, are dense vector representations of words. As computer programs can only understand numbers, we need to convert the words into numbers. </p> <p>The simplest way to represent words as vectors is to use one-hot vectors (means one 1, the rest 0s):</p> \\[ \\begin{align*} \\text{motel} = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] \\\\ \\text{hotel} = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] \\end{align*} \\] <p>If there are \\(N\\) words in the vocabulary, the one-hot vector has \\(N\\) dimensions, and each word is represented by a one-hot vector with a single 1.</p> <p>However, the one-hot vector does not capture the semantic meaning of the words. </p> <p>We expect word embeddings capture the semantic relationships between words by encoding their meanings in a lower-dimensional space. This approach allows mathematical operations on words that reflect their semantic relationships - for example, \"king\" - \"man\" + \"woman\" \u2248 \"queen\". </p> <p></p> <p>There are many methods to learn the word embeddings, such as GloVe and Word2Vec. In this course, we will not discuss in detail  how these methods to learn the word embeddings. The high level idea of these methods is to use the co-occurrence of words to learn the word embeddings. The intuition is that the meaning of a word is given by the words that frequently appear close-by.</p> <p></p> <p>In modern language models, the word embeddings are learned along with the model parameters. We will show a simple example of how to learn the word embeddings using PyTorch below and discuss how embeddings are trained in the language models in the next lecture.</p> <p>PyTorch Implementation</p> <p>In PyTorch, we can use the <code>nn.Embedding</code> layer to learn the word embeddings.</p> <pre><code>import torch\nimport torch.nn as nn\n\n# The vocabulary size is 10 and the embedding dimension is 3.\nembedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n</code></pre> <p>The trainable parameters are the word embeddings, which have the shape of <code>[num_embeddings, embedding_dim]</code>.</p> <p>The input to the <code>nn.Embedding</code> layer is the index of the word in the vocabulary. The output is the word embedding of the word.</p> <pre><code>input = torch.tensor([1, 2, 3, 4, 5])\noutput = embedding(input)\nprint(output) # output shape: [5, 3]\n</code></pre> <p>You can then train your embeddings by connecting the <code>nn.Embedding</code> layer to the model. For example, we train a linear model of the word embeddings to predict the next word in the sentence.</p> <pre><code>model = nn.Sequential(\n    nn.Embedding(num_embeddings=10, embedding_dim=3),\n    nn.Linear(3, 10)\n)\n</code></pre>"},{"location":"chapter_language_model/wordvec/#tokenization","title":"Tokenization","text":"<p>In natural language processing, the most basic unit is the token instead of the word. A token is the smallest unit of text that has a meaning.  These tokens can be words, characters, or subwords.</p> <p>Different tokenization approaches serve different purposes:</p> <ul> <li> <p>Word-level tokenization: Splits text by spaces and punctuation   <pre><code>\"I love NLP!\" \u2192 [\"I\", \"love\", \"NLP\", \"!\"]\n</code></pre></p> </li> <li> <p>Character-level tokenization: Splits text into individual characters   <pre><code>\"Hello\" \u2192 [\"H\", \"e\", \"l\", \"l\", \"o\"]\n</code></pre></p> </li> <li> <p>Subword tokenization: In modern NLP, we often break words into meaningful subunits   <pre><code>\"unhappiness\" \u2192 [\"un\", \"happy\", \"ness\"]\n</code></pre></p> </li> <li> <p>Special tokens: We sometimes need to add some special tokens to the vocabulary to help the model understand the text.</p> </li> <li><code>[UNK]</code>: Unknown token</li> <li><code>[PAD]</code>: Padding token</li> <li><code>[CLS]</code>: Classification token</li> <li><code>[SEP]</code>: Separator token</li> <li><code>[MASK]</code>: Mask token</li> </ul> <p>Sometimes, for non-text data, we also need to tokenize the data. </p> <ul> <li>Audio: Tokenize the audio into frames</li> <li>Image: Tokenize the image into patches</li> <li>DNA: Tokenize the DNA sequence into nucleotides 'A', 'T', 'C', 'G'</li> <li>Protein: Tokenize the protein sequence into amino acids </li> </ul> <p>We can represent each token by a vector in \\(\\mathbb{R}^d\\). Then, a text can be represented as a sequence of token vectors, i.e., the matrix \\(X = [x_1, x_2, \\cdots, x_n] \\in \\mathbb{R}^{n \\times d}\\).</p>"},{"location":"chapter_language_model/wordvec/#tokenization-in-hugging-face-transformers","title":"Tokenization in Hugging Face Transformers","text":"<p>Hugging Face Transformers provides a unified interface for tokenization, model loading, and training. We will start with how to convert text into tokens using Transformers. Please refer to the lecture on Hugging Face for more details.</p> <p>Transformers provides a <code>AutoTokenizer</code> class that can be used to convert text into tokens ids using the <code>tokenize</code> method. For different models, the tokenizer is different. You can choose the tokenizer method from the <code>AutoTokenizer.from_pretrained(name)</code> class, where <code>name</code> is the name of the model choosing from the model hub.</p> <pre><code>from transformers import AutoTokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntext = \"Hello, how are you?\"\n# Get tokens as strings\ntokens = tokenizer.tokenize(text)\n# ['Hello', ',', 'how', 'are', 'you', '?']\n\ninputs = tokenizer(text, return_tensors='pt') # return_tensors='pt' returns a PyTorch tensor\n</code></pre> <p>The <code>inputs</code> is a dictionary with the keys <code>input_ids</code> and <code>attention_mask</code>.</p> <ul> <li><code>input_ids</code>: The token ids of the input text.</li> <li><code>attention_mask</code>: The attention mask of the input text.</li> </ul> <p>After tokenizing text, we often need to convert tokens into embeddings that capture semantic meaning. We show the code for getting token embeddings from BERT model using the <code>AutoModel</code> class. The tokenization embedding input has the shape of <code>[batch_size, seq_length, embedding_dim]</code>.</p> <pre><code>import torch\nfrom transformers import BertTokenizer, BertModel\n\n# Initialize the pretrained BERT model and tokenizer.\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n# Example input sentence.\ninput_text = \"Hello, how are you?\"\n# Convert text to input IDs.\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n# Get input_ids tensor (shape: [batch_size, seq_length])\ninput_ids = inputs[\"input_ids\"]\nword_embeddings = model.get_input_embeddings()(input_ids) # [batch_size, seq_length, embedding_dim]\n</code></pre>"},{"location":"chapter_language_model/wordvec/#position-embeddings","title":"Position Embeddings","text":"<p>In language models, position embeddings are crucial because the self-attention mechanism is inherently permutation-invariant. Without position information, the model would treat tokens the same regardless of their position in the sequence.</p> <p>In order to inject position information, we can use position embeddings. The original transformer paper introduced sinusoidal position embeddings:</p> \\[ P_{(j, 2i)} = \\sin\\left(\\frac{j}{10000^{2i/d}}\\right), P_{(j, 2i+1)} = \\cos\\left(\\frac{j}{10000^{2i/d}}\\right) \\] <p>where - \\(j\\) is the position of the token in the sequence - \\(i\\) is the dimension index - \\(d\\) is the embedding dimension</p> <p></p> <p></p> <p>From the plot above, we can see that the position embeddings use the binary period to encode the absolute position information and the sinusoidal function to encode the relative position information. We skip the rationale why we design the position embeddings in this way. You can refer to the detailed tutorial here. Also you can use other types of position embeddings, such as Rotary Position Embedding (RoPE) and ALiBi (Attention with Linear Biases).</p> <p>In the language models, we usually use the input by adding the token embeddings and the position embeddings </p> \\[ \\text{Model Input} = X+P \\] <p>PyTorch Implementation</p> <p>Here's how to implement both sinusoidal and learned position embeddings:</p> <pre><code>class PositionalEncoding(nn.Module):\n    def __init__(self, num_hiddens, dropout, max_len=1000):\n        super(PositionalEncoding, self).__init__()\n        self.P = torch.zeros((1, max_len, num_hiddens))\n        X = torch.arange(max_len, dtype=torch.float32).reshape(\n            -1, 1) / torch.pow(10000, torch.arange(\n            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n        self.P[:, :, 0::2] = torch.sin(X)\n        self.P[:, :, 1::2] = torch.cos(X)\n\n    def forward(self, X):\n        X = X + self.P[:, :X.shape[1], :].to(X.device)\n        return X\n</code></pre> <p>In practice, we sometimes even directly take the positional embeddings as learnable parameters and train them together with the model parameters. See the example below.</p> <pre><code>class InputEmbeddings(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, d_model))\n\n    def forward(self, x):\n        tok_emb = self.tok_emb(x)\n        pos_emb = self.pos_emb(x)\n        return tok_emb + pos_emb\n</code></pre> <p>Many language models in Hugging Face use the approach above. We can directly get the position embeddings from the <code>AutoModel</code> class.</p> <pre><code>from transformers import AutoTokenizer, AutoModel\nname = \"distilbert/distilbert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(name)\nmodel = AutoModel.from_pretrained(name)\n\ntext = \"Hello, how are you?\"\n# Get input IDs from your tokenized input\ninputs = tokenizer(text, return_tensors='pt')\ninput_ids = inputs[\"input_ids\"]\nposition_ids = torch.arange(0, input_ids.size(1)).unsqueeze(0)\nposition_embeddings = model.distilbert.embeddings.position_embeddings\nposition_embeddings_output = position_embeddings(position_ids)\nprint(position_embeddings_output.shape) # [batch_size, seq_length, embedding_dim]\nword_embeddings = model.get_input_embeddings()(input_ids) \ninput_embeddings = word_embeddings + position_embeddings_output # Model_Input = X + P\n</code></pre>"},{"location":"chapter_neural_networks/","title":"Neural Networks","text":""},{"location":"chapter_neural_networks/#overview","title":"Overview","text":"<p>Neural networks are a type of machine learning model that are inspired by the structure of the human brain. They are a type of deep learning model that are used to model complex relationships between input and output variables.</p> <p></p>"},{"location":"chapter_neural_networks/#lectures","title":"Lectures","text":"<ul> <li>Neural Networks</li> <li>Regularization</li> <li>Convolutional Neural Networks</li> <li>Residual Networks</li> <li>Fine-tuning</li> <li>Computer Vision</li> <li>Training Workflow</li> <li>GPU Training</li> <li>Tips for Training Neural Networks</li> <li>PyTorch Pitfalls</li> </ul>"},{"location":"chapter_neural_networks/cnn/","title":"Convolutional Neural Networks","text":"<p>Convolutional Neural Networks (CNNs) are a specialized type of neural network designed for processing structured grid data such as images. Unlike fully connected networks where each neuron is connected to all neurons in the previous layer, CNNs use a mathematical operation called convolution that applies localized filters to the input data.</p> <p>The key intuition behind CNNs is to exploit spatial locality - nearby pixels in images tend to be related. By using convolutional filters that operate on local regions of the input, CNNs can efficiently identify local patterns like edges, textures, and shapes that are important for tasks like image recognition.</p>"},{"location":"chapter_neural_networks/cnn/#the-convolution-operation","title":"The Convolution Operation","text":"<p>Given an input image \\(\\mathbf{I}\\) and a kernel (filter) \\(\\mathbf{K}\\), the 2D convolution operation is defined as:</p> \\[ (\\mathbf{I} * \\mathbf{K})(i,j) = \\sum_{m} \\sum_{n} \\mathbf{I}(i+m, j+n) \\mathbf{K}(m,n) \\] <p>This operation slides the kernel \\(\\mathbf{K}\\) over the input image \\(\\mathbf{I}\\), performing element-wise multiplication at each location and summing the results to produce a single output value at each position.</p>"},{"location":"chapter_neural_networks/cnn/#padding-and-stride","title":"Padding and Stride","text":"<p>The convolution operation has two important hyperparameters:</p> <ul> <li>Padding: Adding zeros around the input to control the spatial dimensions of the output. If we have a \\(n \\times n\\) image and a \\(k \\times k\\) kernel:</li> <li>Without padding: Output size is \\((n - k + 1) \\times (n - k + 1)\\)</li> <li> <p>With padding \\(p\\): Output size is \\((n + 2p - k + 1) \\times (n + 2p - k + 1)\\)</p> </li> <li> <p>Stride: The step size for moving the kernel. With a stride of \\(s\\), the output size becomes:</p> </li> <li>\\(\\lfloor (n + 2p - k) / s + 1 \\rfloor \\times \\lfloor (n + 2p - k) / s + 1 \\rfloor\\)</li> </ul> <p>You can compute the output size with the following formula:</p> \\[ \\text{Output Size} =  \\frac{\\text{Input Size} - \\text{Kernel Size} + 2 \\times \\text{padding}}{\\text{stride}} + 1  \\] <p>Below is an example of a 2D convolution operation with <code>kernel_size=3</code>, <code>stride=1</code>, and <code>padding=1</code>. </p> <p>In PyTorch, we can use the <code>nn.Conv2d</code> layer to perform the 2D convolution operation.</p> <p><pre><code>conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n</code></pre> </p> <p>Here are the tips for choosing the hyperparameters of the convolution operation:</p> <ul> <li>Kernel size: Choose odd numbers like 3, 5, 7, ... because they have a center pixel and we can pad the same amount on both sides.</li> <li>Stride: Set stride to 1 to preserve the spatial dimensions of the input. If you want to reduce the spatial dimensions of the input by \\(1/N\\) times, you can set stride to \\(N\\).</li> <li>Padding: Choose \\((\\text{kernel size} - 1) / 2\\) as the padding amount with <code>stride=1</code> will keep the spatial dimensions of the input unchanged.</li> </ul>"},{"location":"chapter_neural_networks/cnn/#pooling-operations","title":"Pooling Operations","text":"<p>Pooling layers reduce the spatial dimensions of the feature maps, providing: 1. Computational efficiency 2. Some degree of translation invariance 3. Control over overfitting</p> <p>There are two types of pooling operations:</p> <ul> <li>Max pooling: takes the maximum value within a local region</li> <li>Average pooling: takes the average value within a local region</li> </ul> <p>In PyTorch, we can use the <code>nn.MaxPool2d</code> layer to perform the max pooling operation. Below is an example of a max pooling operation with <code>kernel_size=2</code>, <code>stride=2</code>.</p> <pre><code>pool1 = nn.MaxPool2d(kernel_size, stride)\npool2 = nn.AvgPool2d(kernel_size, stride)\n</code></pre> <p></p> <p>Similarly, we can use the <code>nn.AvgPool2d</code> layer to perform the average pooling operation.</p>"},{"location":"chapter_neural_networks/cnn/#cnn-architectures","title":"CNN Architectures","text":"<p>The typical CNN architecture consists of:</p> <ol> <li>First Convolutional layer: Extract local patterns and reduce spatial dimensions to the proper size if the input image is too large</li> <li>Activation functions (typically ReLU): Add non-linearity</li> <li>A Convolutional Block: May have multiple convolutional layers keeping the spatial dimensions unchanged connected by activation functions</li> <li>Pooling layers: Reduce spatial dimensions typically by half</li> <li>Repeat the above steps: Have multiple convolutional blocks to make the model deeper</li> <li>Flatten layer: Flatten the output of the last convolutional block to a 1D vector</li> <li>Fully connected layers: Final classification/regression</li> </ol> <p>We illustrate the VGG16 architecture with the following diagram:</p> <p></p>"},{"location":"chapter_neural_networks/cnn/#pytorch-for-cnns","title":"PyTorch for CNNs","text":"<p>PyTorch provides a convenient API for building CNNs using the <code>torch.nn</code> module. The basic building blocks include:</p> <ul> <li><code>nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)</code>: 2D convolutional layer</li> <li><code>nn.MaxPool2d(kernel_size, stride)</code>: Max pooling layer</li> <li><code>nn.AvgPool2d(kernel_size, stride)</code>: Average pooling layer</li> <li><code>nn.ReLU()</code>: ReLU activation function</li> <li><code>nn.Flatten()</code>: Flatten layer to convert the output of the last convolutional block to a 1D vector</li> <li><code>nn.Linear(in_features, out_features)</code>: Linear layer to perform the final classification/regression</li> </ul> <p>Simple CNN with Sequential API</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Define a simple CNN for MNIST (28x28 grayscale images)\ninput_channels, output_size = 1, 10\nmodel = nn.Sequential(\n    # First convolutional block\n    nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x14x14\n\n    # Second convolutional block\n    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 32x7x7\n\n    # Flatten and fully connected layers\n    nn.Flatten(),  # Output: 32*7*7 = 1568\n    nn.Linear(32*7*7, 128),\n    nn.ReLU(),\n    nn.Linear(128, output_size)\n)\n</code></pre> <p>Custom CNN using nn.Module</p> <p>We will define a TinyVGG model with the following architecture:</p> <p></p> <p>Here is an interactive website of the TinyVGG model and training.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TinyVGG(nn.Module):\n    def __init__(self, input_channels=3, hidden_units=10, num_classes=10):\n        super().__init__()\n\n        # First convolutional block\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_channels, \n                      out_channels=hidden_units, \n                      kernel_size=3, \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2)\n        )\n\n        # Second convolutional block\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units*2, \n                      kernel_size=3, \n                      padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units*2, \n                      out_channels=hidden_units*2,\n                      kernel_size=3,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2)\n        )\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Note: The actual input size will depend on the input image dimensions\n            # For a 32x32 input image, after two 2x2 max pooling layers, the feature map size is 8x8\n            nn.Linear(in_features=hidden_units*2*8*8, \n                      out_features=num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_block_1(x)\n        x = self.conv_block_2(x)\n        x = self.classifier(x)\n        return x\n\n# Example usage\nmodel = TinyVGG(input_channels=3, hidden_units=64, num_classes=10)\nx = torch.randn(32, 3, 32, 32)  # 32 batch size, 3 color channels, 32x32 images\noutput = model(x)\nprint(output.shape)  # torch.Size([32, 10])\n</code></pre> <p>Deeper CNN Architecture</p> <p>You can define a deeper CNN architecture by first define a convolutional block and then repeating the convolutional block and the classifier.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n                              stride=stride, padding=padding)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = F.relu(x)\n        return x\n\nclass DeepCNN(nn.Module):\n    def __init__(self, input_channels, hidden_channels, output_size):\n        super().__init__()\n\n        # First block: two convolutions followed by pooling\n        self.block1 = nn.Sequential(\n            ConvBlock(input_channels, hidden_channels),\n            ConvBlock(hidden_channels, hidden_channels),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Second block: two convolutions followed by pooling\n        self.block2 = nn.Sequential(\n            ConvBlock(hidden_channels, hidden_channels*2),\n            ConvBlock(hidden_channels*2, hidden_channels*2),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Global average pooling and classifier\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(hidden_channels*2, output_size)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.global_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Initialize the model\ninput_channels, hidden_channels, output_size = 3, 64, 10\nmodel = DeepCNN(input_channels, hidden_channels, output_size)\n</code></pre>"},{"location":"chapter_neural_networks/computer_vision/","title":"Computer Vision","text":"<p>Computer vision applications such as medical imaging, autonomous vehicles, surveillance systems, and photo filters have become increasingly integrated into our daily lives and will continue to shape our future. Deep learning has revolutionized this field, dramatically improving the capabilities of computer vision systems. Today's cutting-edge computer vision technologies rely almost exclusively on deep learning approaches. This lecture explores the computer vision domain, examining basic pipelines for image classification.</p>"},{"location":"chapter_neural_networks/computer_vision/#dataset-cifar-10","title":"Dataset: CIFAR-10","text":"<p>PyTorch provides a convenient package called <code>torchvision</code> that includes common datasets, model architectures, and image transformations for computer vision. The <code>torchvision.datasets</code> module contains many popular datasets that can be downloaded with just a few lines of code.</p> <p>One of the most widely used datasets for learning computer vision is CIFAR-10. This dataset consists of 60,000 32x32 color images divided into 10 classes, with 6,000 images per class. The classes include common objects like airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The dataset is split into 50,000 training images and 10,000 test images.</p> <p></p> <p>Here's how to load the CIFAR-10 dataset using torchvision:</p> <pre><code>from torchvision import datasets\nfrom torchvision import transforms\n\n# Convert images to tensors for training\ntransform = transforms.ToTensor()\n# Download and load training dataset\ntrainset = datasets.CIFAR10(root='./data', train=True,\n                          download=True, transform=transform)\nprint(len(trainset)) # 50000\n# Download and load test dataset\ntestset = datasets.CIFAR10(root='./data', train=False,\n                          download=True, transform=transform)\nprint(len(testset)) # 10000\n</code></pre> <p>You can then visualize the first 10 images in the training dataset:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef show_image(image_tensor):\n    # Convert to numpy and transpose to (H, W, C)\n    image_np = image_tensor.numpy().transpose(1, 2, 0)\n    # Plot the image\n    plt.imshow(image_np)\n    plt.axis('off')\n    plt.show()\n\nimage, _ = trainset[0]\nprint(image.shape) # torch.Size([3, 32, 32])\nshow_image(image)\n</code></pre> <p></p>"},{"location":"chapter_neural_networks/computer_vision/#data-augmentation","title":"Data Augmentation","text":"<p>Deep neural networks typically require extensive datasets to achieve optimal performance. Data augmentation techniques address this need by artificially expanding the training dataset through controlled modifications of existing images. These transformations create new, valid training examples that help the network learn more robust features. Beyond simply increasing dataset size, augmentation encourages models to develop invariance to specific image properties. For instance, applying random cropping teaches the network to recognize objects regardless of their position within the frame. Similarly, altering brightness levels and color characteristics helps the model become less dependent on exact color information and more focused on fundamental visual patterns. This approach significantly enhances a model's ability to generalize to unseen data.</p> <pre><code>def apply_transform(img, aug, num_times=6):\n    imgs = [aug(img) for _ in range(num_times)]\n    plt.figure(figsize=(num_times * 2, 2))\n    for i, im in enumerate(imgs):\n        img_np = im.numpy().transpose(1, 2, 0)\n        plt.subplot(1, num_times, i + 1)\n        plt.imshow(img_np)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>In PyTorch, the <code>torchvision.transforms</code> module provides a set of common transformations for images\uff1a</p> <ul> <li><code>transforms.RandomHorizontalFlip()</code> or <code>transforms.RandomVerticalFlip()</code>: Flips the image horizontally or vertically.</li> </ul> <pre><code>apply_transform(image, torchvision.transforms.RandomHorizontalFlip())\n</code></pre> <p></p> <ul> <li><code>transforms.RandomRotation(degrees)</code>: Rotates the image by a random angle.</li> </ul> <pre><code>apply_transform(image, torchvision.transforms.RandomRotation(degrees=15))\n</code></pre> <p></p> <ul> <li><code>torchvision.transforms.ColorJitter</code>: Applies random brightness, contrast, saturation, and hue to the image.</li> </ul> <pre><code>apply_transform(image, torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5))\n</code></pre> <p></p> <ul> <li><code>torchvision.transforms.RandomResizedCrop</code>: Crops the image to a random size and resizes it to a fixed size.</li> </ul> <pre><code>apply_transform(image, torchvision.transforms.RandomResizedCrop(size=(32, 32), scale=(0.5, 1.5), ratio=(0.5, 1.5)))\n</code></pre> <p></p> <p>You can also combine multiple transformations:</p> <pre><code>transform = torchvision.transforms.Compose([\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.RandomRotation(degrees=15),\n    torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n    torchvision.transforms.RandomResizedCrop(size=(32, 32), scale=(0.5, 1.5), ratio=(0.5, 1.5))\n])\napply_transform(image, transform)\n</code></pre> <p></p> <p>You can load the CIFAR-10 training dataset with the augmentation:</p> <pre><code>from torchvision import transforms\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomResizedCrop(size=(32, 32), scale=(0.5, 1.5), ratio=(0.5, 1.5)),\n    transforms.ToTensor()\n])\ntrainset_aug = datasets.CIFAR10(root='./data', train=True,\n                          download=True, transform=transform)\ntrainloader = DataLoader(trainset_aug, batch_size=128, shuffle=True)\n</code></pre> <p>Every batch the <code>trainloader</code> will apply different augmentations to the images to augment the training dataset.</p>"},{"location":"chapter_neural_networks/dl_gpu/","title":"Deep Learning with GPU","text":"<p>Training deep learning models can be computationally intensive. Leveraging Graphics Processing Units (GPUs) can significantly accelerate this process. This guide introduces the basics of GPU usage in PyTorch, including the differences between CPUs and GPUs, how to move data and models between devices, and strategies for multi-GPU training.</p>"},{"location":"chapter_neural_networks/dl_gpu/#cpu-versus-gpu","title":"CPU versus GPU","text":"<p>Central Processing Units (CPUs) are designed for general-purpose computing tasks. They excel at handling a few complex threads simultaneously.\u200b</p> <p></p> <p>Graphics Processing Units (GPUs) are specialized hardware designed to handle parallel tasks efficiently. They are particularly well-suited for operations like matrix multiplications, which are common in deep learning.\u200b By utilizing GPUs for deep learning, training times can be reduced significantly compared to using CPUs alone.\u200b</p>"},{"location":"chapter_neural_networks/dl_gpu/#pytorch-with-gpu","title":"PyTorch with GPU","text":"<p>Before leveraging GPUs in PyTorch, ensure that your environment is set up correctly. We have discussed how to set up GPU cores in the class cluster in the GPU Matrix chapter. In general, you can use the command <code>nvidia-smi</code> to check the GPU status.</p> <pre><code>[jul924@ip-10-37-33-243 ~]$ salloc --partition=gpu --cpus-per-task=1 --mem=30G --time=01:00:00 srun --pty bash\nsalloc: Granted job allocation 38087\n[jul924@gpu-dy-gpu-cr-7 ~]$ nvidia-smi\nMon Mar  3 19:16:23 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA L4                      On  | 00000000:31:00.0 Off |                    0 |\n| N/A   28C    P8              16W /  72W |      0MiB / 23034MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</code></pre> <p>In PyTorch, we can check the device type by using the <code>torch.cuda.is_available()</code> function. </p> <pre><code>import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    print(\"CUDA is available. You can use GPU acceleration.\")\nelse:\n    print(\"CUDA is not available. Using CPU.\")\n</code></pre> <p>Moving data and models between devices</p> <p>To move data and models between devices, we can use the <code>to()</code> method. If you train your model on the GPU, you need to move both the training data and the model parameters to the GPU.</p> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\nx = torch.randn(3, 3)\nx = x.to(device)\nmodel = MyModel()\nmodel.to(device)\n</code></pre> <p>Best practices for <code>.to(device)</code> in PyTorch Training</p> <ul> <li> <p>Remember that move data among CPU and GPU is time-consuming. </p> </li> <li> <p>Keep model and data on the same GPU device if the GPU memory is enough.</p> </li> <li> <p>Don\u2019t move the entire dataset to the GPU at once. If the dataset is large, it won\u2019t fit into GPU memory. Instead, move each batch of data to the GPU inside the training loop.</p> </li> <li> <p>Use <code>pin_memory=True</code> in the <code>DataLoader</code>. In PyTorch's <code>DataLoader</code>, setting <code>pin_memory=True</code> ensures that the data loaded by the <code>DataLoader</code> resides in pinned (page-locked) memory. Pinned memory cannot be swapped to disk by the operating system, allowing faster data transfer to the GPU. Normally, data in pageable memory requires an extra step during transfer to the GPU: it must first be copied to an intermediate pinned memory buffer before being sent to the GPU. By directly loading data into pinned memory, this intermediate step is eliminated, resulting in faster and more efficient data transfers. You may need to set <code>pin_memory=False</code> when your system has limited RAM, as pinned memory consumes more of it. Excessive use can lead to system instability.</p> </li> </ul> <pre><code>train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n</code></pre> <p>Think of your computer\u2019s RAM (main memory) like a big shared library: Normally, the operating system can move stuff around in RAM (like rearranging books) to manage memory efficiently. That kind of RAM is called \u201cpageable memory\u201d.</p> <p>Now imagine: You want to send a box of books (data) from the library (RAM/CPU) to a delivery truck (GPU). If the books are moving around or not fixed in place (pageable), it\u2019s harder to pick them up and load them into the truck. So instead, you \u201cpin\u201d the box in one place \u2014 it\u2019s locked in position. Now the truck can quickly grab it and go!</p> <p></p> <ul> <li>Use <code>non_blocking=True</code> in the <code>to()</code> method if using GPU. When moving data from the CPU to the GPU using the <code>.to()</code> method, setting <code>non_blocking=True</code> allows the data transfer to occur asynchronously. This means the CPU can continue executing subsequent operations without waiting for the data transfer to complete. Asynchronous data transfers enable the overlap of data transfer and computation, leading to better utilization of both CPU and GPU resources. This overlap reduces idle times and can significantly speed up the training process.\u200b Notice that you need to set <code>pin_memory=True</code> in the <code>DataLoader</code> to make <code>non_blocking=True</code> effective. If the source tensor is not in pinned memory, the asynchronous transfer may be even slower.</li> </ul> <pre><code>data = data.to(device, non_blocking=True)\n</code></pre> <p>We summarize the best practices for <code>.to(device)</code> above in the following checklist.</p> Checklist When to Use <code>.to(device)</code> on model Once after model creation <code>.to(device)</code> on data Inside the training loop, batch-by-batch <code>pin_memory=True</code> in <code>DataLoader</code> When using GPU, to speed up transfer <code>non_blocking=True</code> in <code>.to()</code> Only add to batch data for faster async transfer with pinned memory Add <code>model = nn.DataParallel(model)</code> Only add for multi-GPU training <p>Here we provide an example code for training a model on GPU.</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nimport torch.optim as optim\n\n# 1. Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# 2. Define transforms and dataset (on disk)\ntransform = transforms.ToTensor()\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n# 3. Dataloader - use pin_memory to speed up CPU -&gt; GPU transfer\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n# 4. Define model and move to GPU\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28*28, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n# Add the line below for multi-GPU training\n# model = nn.DataParallel(model)\nmodel.to(device)\n# 5. Optimizer and loss\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss() # Add to(device) if your loss has internal parameters\n# 6. Training loop - move each batch to GPU\nfor epoch in range(5):\n    model.train()\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # Move data to the same device as model\n        data = data.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n        # Forward\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch} Batch {batch_idx} Loss {loss.item():.4f}\")\n</code></pre>"},{"location":"chapter_neural_networks/dl_gpu/#multi-gpu-training","title":"Multi-GPU Training","text":"<p>For larger models or datasets, utilizing multiple GPUs can further accelerate training.  The multi-GPU training is a complicated topic to beginner data scientists as it involves many concepts related to hardware. There are many packages to help you implement multi-GPU training in an easier way, e.g., Hugging Face's accelerate package which we will introduce in the next chapter. </p> <p>In this lecture, we will focus on the PyTorch's method. PyTorch offers two main approaches:\u200b</p> <ol> <li><code>DataParallel</code>: DataParallel distributes the workload across threads, with each thread managing a GPU and one GPU designated as primary. It works by copying the model to each GPU, splitting the training data into portions for each device, and then having each model copy independently perform forward and backward propagation to calculate gradients, which are then synchronized. </li> </ol> <p></p> <ol> <li><code>DistributedDataParallel</code>: This approach assigns each GPU to a separate process. After each process computes gradients independently, they synchronize by aggregating gradients across all GPUs in parallel. Because processes operate with isolated memory spaces, they communicate through Inter-Process Communication (IPC), employing an all-reduce operation to share gradient information. Following this synchronization, each process independently updates its model parameters on its respective GPU. This parallel synchronization mechanism makes <code>DistributedDataParallel</code> significantly more efficient and better suited for scaling to multiple GPUs. However, it requires more setup and is more complicated than <code>DataParallel</code>.</li> </ol> <p></p>"},{"location":"chapter_neural_networks/dl_gpu/#dataparallel-implementation","title":"<code>DataParallel</code> Implementation","text":"<p>The <code>DataParallel</code> method is easier to implement.  If you already have your code for single GPU training with model and data moving to GPU using <code>to(device)</code>, you only need to change one line of code:</p> <ul> <li>wrap your model with <code>torch.nn.DataParallel</code> and call <code>model.to(device)</code>.</li> </ul> <pre><code>import torch.nn as nn\nmodel = nn.DataParallel(model)\nmodel.to(device)\n</code></pre>"},{"location":"chapter_neural_networks/dl_gpu/#distributeddataparallel-implementation","title":"<code>DistributedDataParallel</code> Implementation","text":"<p>The <code>DistributedDataParallel</code> method is more complicated than <code>DataParallel</code> and requires more setup. If you are a beginner, we do not recommend you to use this method. You could either use <code>DataParallel</code> or directly use the Hugging Face's accelerate package to get started. However, <code>DistributedDataParallel</code> can offer you more flexibility to control the training process and have the potential to achieve better performance.</p> <p>To implement multi-GPU training by <code>DistributedDataParallel</code> method, you need to follow the following steps:</p> <ul> <li>Spawn a process per GPU: Each GPU needs its own training process. When training on multiple GPUs, you need to define a <code>train(rank, world_size)</code> function which executes on <code>rank</code>-th GPU among <code>world_size</code> GPUs</li> </ul> <pre><code>def train(rank, world_size):\n    # ... training code ...\n</code></pre> <p>Then in the main part of your code, you need to spawn a process per GPU.</p> <p><pre><code>import torch.multiprocessing as mp\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size)\n</code></pre> The code above will spawn <code>world_size</code> processes, each of which will handle a different GPU.</p> <p>Inside the <code>train(rank, world_size)</code> function which executes on <code>rank</code>-th GPU among <code>world_size</code> GPUs, you need to conduct the following steps:</p> <ul> <li>Assign a specific GPU to each process: So they don\u2019t all try to use the same one. You can tell each process which GPU to use by using the following function.</li> </ul> <pre><code>import torch.distributed as dist\n# To use GPU #rank out of world_size of GPUs\ndist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\ntorch.cuda.set_device(rank)\n</code></pre> <p>The code <code>dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)</code> sync GPUs across processes. It says \"I'm process #rank, and there are #world_size processes in total. Hey GPUs, let\u2019s team up for this job!\". Here <code>nccl</code> refers to the NVIDIA Collective Communications Library, which is a library for parallel computing with GPUs.</p> <ul> <li>Create the Dataloader: If you have created a dataset class, you can create a <code>DistributedSampler</code> to sampler as the input of the <code>DataLoader</code>.</li> </ul> <pre><code>from torch.utils.data import DataLoader, DistributedSampler\ndataset = YourDatasetClass() \nsampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\ndataloader = DataLoader(dataset, batch_size=64, sampler=sampler, num_workers=4, pin_memory=True)\n</code></pre> <p>Here <code>DistributedSampler(dataset, num_replicas=world_size, rank=rank)</code> splits the dataset into <code>world_size</code> equal chunks and return <code>rank</code>-th chunk.</p> <p>Here <code>num_workers</code> is the number of subprocesses (CPU workers) used to load the data in the background. These workers run in parallel to your GPU, preparing data while your GPU is training the current batch. It is like while the chef (GPU) is cooking, multiple assistants (workers) are chopping vegetables (loading data from disk and transforming data). The more helpers you have, the less the chef has to wait!</p> <p>If your <code>num_workers</code> is too small, the data loading will become a bottleneck and GPU needs to wait for CPU to load data. Too high <code>num_workers</code> can overload your CPU or RAM. We suggest you set <code>num_workers = (num_cpu_cores) // (num_gpus)</code>, where you can get these numbers by the following code.</p> <pre><code>import os\nimport multiprocessing\nimport torch\nnum_cpu_cores = multiprocessing.cpu_count()\nnum_gpus = torch.cuda.device_count()\nnum_workers = (num_cpu_cores) // (num_gpus) \n</code></pre> <ul> <li>Wrap the model in <code>DistributedDataParallel</code>: So gradients sync across GPUs. Here is an example code for setting up the model, loss, and optimizer.</li> </ul> <pre><code>device = torch.device(f\"cuda:{rank}\")\nmodel = YourModelClass().to(device)\nmodel = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n</code></pre> <p>At every <code>loss.backward()</code>, DDP does something magical: All GPUs talk to each other and exchange gradients. </p> <ul> <li>Training loop:</li> </ul> <pre><code>for epoch in range(5):\n    sampler.set_epoch(epoch)  # Ensure data shuffling is different each epoch\n    for batch_idx, (data, target) in enumerate(dataloader):\n        data = data.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n\n        output = model(data)\n        loss = criterion(output, target)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if rank == 0: # Only the main process (rank 0) prints the epoch completion\n        print(f\"Epoch {epoch} completed.\")\n\ndist.destroy_process_group() # Clean up the process group created by init_process_group\n</code></pre> <p>Here is the complete code for multi-GPU training.</p> <pre><code># ddp_train.py\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torchvision import models, datasets, transforms\nfrom torch.utils.data import DataLoader, DistributedSampler\n\ndef train(rank, world_size):\n    print(f\"Running DDP training on rank {rank}.\")\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n    torch.backends.cudnn.benchmark = True # Open cuDNN, only if your model dimension and batch size are fixed, see #Advanced Tips for GPU Training\n\n    # Set device\n    device = torch.device(f\"cuda:{rank}\")\n\n    # Dataset and DataLoader with DistributedSampler\n    transform = transforms.Compose([transforms.ToTensor()])\n    dataset = datasets.FakeData(transform=None)\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n    dataloader = DataLoader(dataset, batch_size=64, sampler=sampler, num_workers=4, pin_memory=True)\n\n    # Model, loss, optimizer\n    model = models.resnet18().to(device)\n    model = DDP(model, device_ids=[rank])\n    criterion = nn.CrossEntropyLoss().to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n    # Training loop\n    for epoch in range(5):\n        sampler.set_epoch(epoch)  # Ensure data shuffling is different each epoch\n        for batch_idx, (data, target) in enumerate(dataloader):\n            data = data.to(device, non_blocking=True)\n            target = target.to(device, non_blocking=True)\n\n            output = model(data)\n            loss = criterion(output, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        if rank == 0:\n            print(f\"Epoch {epoch} completed.\")\n\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size)\n</code></pre> <p>Finally, you can run the code by using the following command.</p> <p><pre><code>torchrun --nproc_per_node=4 ddp_train.py\n</code></pre> Here <code>--nproc_per_node=4</code> means you want to use 4 GPUs.</p>"},{"location":"chapter_neural_networks/dl_gpu/#advanced-tips-for-gpu-training","title":"Advanced Tips for GPU Training","text":"<p>Here we summarize some advanced tips for GPU training.</p> <ul> <li>Asynchronous Data Loading &amp; Pin Memory: We have discussed this in the Single GPU section. In summary, use <code>pin_memory=True</code> in the <code>DataLoader</code> to speed up CPU -&gt; GPU transfer and use <code>to(device, non_blocking=True)</code> method.</li> </ul> <pre><code>train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\nfor data, target in train_loader:\n    data = data.to(device, non_blocking=True)\n    target = target.to(device, non_blocking=True)\n</code></pre> <ul> <li>Gradient Accumulation: If your GPU memory is not enough, you can use gradient accumulation to train your model.</li> </ul> <pre><code>accum_steps = 4\nloss = loss / accum_steps\nloss.backward()\nif (step + 1) % accum_steps == 0:\n    optimizer.step()\n    optimizer.zero_grad()\n</code></pre> <ul> <li> <p>Align the model tensor dimensions: Try to use dimensions that are multiples of 32 or 8 for tensor sizes, batch sizes, input sizes, and all other dimensions. For operations like convolutions and matrix multiplications, it's best to align your tensor dimensions with the GPU's \"kernel settings\". If your tensor sizes are \"odd\" numbers like 33 or 65, it may cause some threads in a warp to idle (wasting resources) during execution.</p> </li> <li> <p>Automatic Mixed Precision: There are two types of precision: FP32 (32-bit floating point) and FP16 (16-bit floating point). FP16 is faster on some GPUs, but it can cause precision loss. Automatic Mixed Precision (AMP) is a technique that automatically switches between FP32 and FP16 precision during training to balance performance and accuracy. Notice AMP only works on some GPUs (e.g.,NVIDIA V100, A100, H100, etc.) </p> </li> </ul> <p>In PyTorch, you can use <code>torch.amp</code> to implement AMP. In your training loop, you only need to change two parts.</p> <pre><code>from torch.amp import autocast, GradScaler\n\nscaler = GradScaler('cuda')\nfor data, target in dataloader:\n    optimizer.zero_grad()\n    data = data.to(device)\n    target = target.to(device)\n\n    # Use autocast to automatically switch between FP32 and FP16 precision\n    with autocast():\n        output = model(data)\n        loss = loss_fn(output, target)\n\n    # Scale the loss and backward\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre> <ul> <li>Open cuDNN: If your model dimension and batch size are fixed, you can open cuDNN to speed up training using the following code.</li> </ul> <pre><code>torch.backends.cudnn.benchmark = True\n</code></pre> <p>PyTorch uses NVIDIA's cuDNN acceleration library for operations like convolutions, normalization, and pooling. For each operation (like convolution), cuDNN offers multiple implementation algorithms with different trade-offs: some are faster but memory-intensive, others are slower but memory-efficient, and some are optimized for large batches or small kernels. At the beginning of training, PyTorch can benchmark several algorithms, identify the fastest one, and consistently use it thereafter. </p> <p>This approach significantly speeds up training, but requires fixed input dimensions and batch sizes. If your input dimensions or batch sizes are changing, you may not open it.</p>"},{"location":"chapter_neural_networks/dl_gpu/#gpu-training-checklist","title":"GPU Training Checklist","text":"Component Setting / Tip Why DataLoader <code>pin_memory=True</code> Faster CPU \u2192 GPU transfer (via DMA) <code>num_workers = num_CPU_cores // num_GPUs</code> Loads data in parallel using CPU workers Multi-GPU: Use <code>DistributedSampler</code> (DDP) Ensures each GPU sees unique data Multi-GPU: Call <code>sampler.set_epoch(epoch)</code> Ensures shuffling differs across epochs in DDP Device Handling Single GPU: <code>device = torch.device(\"cuda\" if torch.cuda.is_available())</code> For single GPU training Multi-GPU: <code>device = torch.device(f\"cuda:{rank}\")</code> For multi-GPU training Use <code>.to(device)</code> for model Moves tensors to GPU (or CPU) Use <code>.to(device)</code> for data in batch loop Moves whole data to GPU will cause memory issue Use <code>.to(device, non_blocking=True)</code> Enables async transfer from CPU to GPU Model Setup <code>torch.backends.cudnn.benchmark = True</code> Speeds up convolution if input sizes are constant Use <code>model.eval()</code> during validation Disables dropout/batchnorm updates Loss &amp; Optimizer Move loss to GPU: <code>criterion.to(device)</code> Keeps computation on GPU Use <code>.zero_grad(set_to_none=True)</code> More efficient gradient reset Gradient Accumulation If GPU memory is not enough Mixed Precision Use <code>autocast()</code> and <code>GradScaler()</code> from <code>torch.cuda.amp</code> Speeds up training and reduces memory usage Multi-GPU Use <code>torchrun --nproc_per_node=NUM_GPUs</code> Launches one process per GPU Wrap model in <code>DistributedDataParallel</code> Enables gradient sync between GPUs Monitoring <code>nvidia-smi</code> / <code>wandb</code> / <code>tensorboard</code> Monitor GPU usage, memory, and training speed"},{"location":"chapter_neural_networks/dl_receipt/","title":"\ud83e\udde0 Neural Network Training: A Leaky Abstraction","text":"<p>Neural network training looks simple \u2014 but often fails silently. This guide summarizes a systematic, cautious, and debug-friendly process for training deep learning models, with tips, visual checks, and sanity tests at every stage.</p>"},{"location":"chapter_neural_networks/dl_receipt/#1-understand-the-data-before-any-model","title":"\ud83e\udde9 1. Understand the Data (Before Any Model!)","text":"<p>Before touching any model code:</p> <ul> <li>Visualize examples manually</li> <li>Look for duplicates, class imbalance, label noise</li> <li>Understand the distribution and types of variation</li> <li>Plot label histograms, outliers, and corrupted data</li> </ul> <pre><code>import matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\n# Visualize sample images\ndef show_images(images, title=\"\"):\n    grid = make_grid(images, nrow=8)\n    plt.imshow(grid.permute(1, 2, 0))\n    plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n\n# Example usage\nimages = ...  # Your dataset images\nshow_images(images)\n</code></pre> <p>\u2699\ufe0f 2. Build a Clean Training &amp; Evaluation Skeleton</p> <p>Start with a minimal working pipeline using a simple model:</p> <ul> <li>Fix random.seed</li> <li>Disable all data augmentation</li> <li>Use tiny models (e.g. 1-layer CNN or linear)</li> <li>Visualize all outputs and losses</li> </ul> <pre><code>torch.manual_seed(42)\ntorch.backends.cudnn.benchmark = True\n\n# Dummy baseline model\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28 * 28, 10)\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n</code></pre> <p>Sanity checks:</p> <ul> <li>\u2705 Plot loss/accuracy</li> <li>\u2705 Evaluate entire test set</li> <li>\u2705 Verify loss at initialization (e.g. -log(1/n_classes))</li> </ul> <p>\ud83d\udee0\ufe0f 3. Overfit a Single Batch</p> <p>This ensures your model + data + training loop are working.</p> <pre><code># Overfit on a small batch\nmodel.train()\nx, y = next(iter(train_loader))\nx, y = x.to(device), y.to(device)\n\nfor _ in range(1000):\n    y_hat = model(x)\n    loss = criterion(y_hat, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>\u2705 Your model should reach ~0 loss or 100% accuracy on this batch. If not, something is wrong.</p> <p>\ud83d\udd0d 4. Visual Debugging</p> <ul> <li>Always visualize the input right before it goes into the model</li> <li>Visualize predictions over time on a fixed batch to observe learning dynamics</li> </ul> <p>\ud83e\uddea 5. Gradients for Dependency Checks</p> <p>Use gradients to verify that your model uses the right inputs.</p> <pre><code># Check which inputs influence the output\nx.requires_grad_(True)\noutput = model(x)\nloss = output[0].sum()\nloss.backward()\n\nprint(x.grad[0])  # Should only have non-zero gradients where expected\n</code></pre> <p>\ud83e\uddf1 6. Build Up Model Complexity Step-by-Step</p> <ul> <li>Start with a reliable architecture (e.g. ResNet-18, UNet)</li> <li>Add complexity gradually (more inputs, larger images, new layers)</li> <li>Avoid learning rate decay too early. Use constant LR until convergence, then schedule if needed</li> </ul> <pre><code>scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n</code></pre> <p>\ud83e\uddf9 7. Regularization Strategies</p> <p>Once you\u2019re overfitting training data:</p> <ul> <li>\u2705 Get more real data</li> <li>\u2705 Use stronger augmentations (cutout, color jitter, etc.)</li> <li>\u2705 Apply dropout / weight decay</li> <li>\u2705 Reduce model size / input dimension</li> <li>\u2705 Early stopping based on val loss</li> </ul> <p>\ud83e\uddea 8. Tune Hyperparameters</p> <ul> <li>Prefer random search over grid search</li> </ul> <p>Tune one by one ranking from most important to least important:</p> <ul> <li>Learning rate</li> <li>Weight decay</li> <li>Dropout rate</li> <li>Batch size</li> </ul> <p>\ud83e\uddc3 9. Squeeze Out the Last Bit of Juice</p> <ul> <li>\u2705 Train longer \u2014 convergence can be slow</li> <li>\u2705 Use ensembles</li> <li>\u2705 Use test-time augmentation</li> <li>\u2705 Visualize filters and activations</li> </ul> <p>\ud83c\udfc1 Final Thoughts</p> <p>Training deep neural networks is not plug-and-play. Success comes from:</p> <ul> <li>\u2705 Deep understanding of your data</li> <li>\u2705 Careful debugging &amp; visualizations</li> <li>\u2705 Step-by-step validation</li> <li>\u2705 Patience and attention to detail</li> </ul>"},{"location":"chapter_neural_networks/dl_tips/","title":"Tips for Training Neural Networks","text":"<p>The following are tips for training neural networks based on Andrej Karpathy's blog post.</p>"},{"location":"chapter_neural_networks/dl_tips/#general-philosophy","title":"General Philosophy","text":"<ul> <li> <p>Neural networks are a leaky abstraction:</p> <ul> <li>Despite simple APIs suggesting plug-and-play convenience, successful training requires deep understanding</li> <li>Common libraries hide complexity but don't eliminate the need for foundational knowledge</li> </ul> </li> <li> <p>Neural networks fail silently:</p> <ul> <li>Errors are typically logical rather than syntactical, rarely triggering immediate exceptions</li> <li>Mistakes often subtly degrade performance, making debugging challenging</li> </ul> </li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#recommended-training-process","title":"Recommended Training Process","text":""},{"location":"chapter_neural_networks/dl_tips/#core-principles","title":"Core Principles","text":"<ul> <li>Patience and attention to detail are critical</li> <li>Start from simplest possible setup, progressively adding complexity</li> <li>Continuously validate assumptions and visualize intermediate results</li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#deep-learning-recipe","title":"Deep Learning Recipe","text":""},{"location":"chapter_neural_networks/dl_tips/#step-1-data-exploration","title":"Step 1: Data Exploration","text":"<ul> <li>Spend substantial time manually inspecting data</li> <li> <p>Look for the following data issues:</p> <ul> <li>Duplicate or corrupted data</li> <li>Imbalanced distributions</li> <li>Biases and potential patterns</li> </ul> </li> <li> <p>Think qualitatively:</p> <ul> <li>Which features seem important?</li> <li>Can you remove noise or irrelevant information?</li> <li>Understand label quality and noise level</li> </ul> </li> <li> <p>Write simple scripts to filter/sort and visualize data distributions/outliers</p> </li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#step-2-build-training-code","title":"Step 2: Build Training Code","text":"<ul> <li>Build a basic training/evaluation pipeline using a simple model (linear classifier or small CNN)</li> <li> <p>Best practices at this stage:</p> <ul> <li>Fix random seed: Ensures reproducibility</li> <li>Simplify: Disable data augmentation or other complexity initially</li> <li>Full test evaluation: Run tests on entire datasets; avoid reliance on smoothing metrics</li> <li>Verify loss at initialization: Ensure correct loss values at initialization (e.g., -log(1/n_classes))</li> <li>Proper Initialization: Set final layer biases according to data distribution</li> <li>Human baseline: Measure human-level accuracy for reference</li> <li>Input-independent baseline: Check that your model learns something beyond trivial solutions</li> <li>Overfit a tiny dataset: Confirm your network can achieve near-zero loss on a single batch\u2014essential debugging step</li> <li>Visualizations: Regularly visualize inputs immediately before entering the model, predictions over training time, and loss dynamics</li> <li>Check dependencies with gradients: Use backward pass to debug vectorization and broadcasting issues</li> <li>Generalize incrementally: Write simple, explicit implementations first; vectorize/generalize later</li> </ul> </li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#step-3-overfitting","title":"Step 3: Overfitting","text":"<ul> <li> <p>Goal of the overfitting first for smaller batch size is to:</p> <ul> <li>Ensure your model can at least perfectly memorize the training set (overfit)</li> <li>If your model can't overfit, you likely have bugs or incorrect assumptions</li> <li>Tips for this stage:<pre><code>- Don't be creative initially: Use established architectures first (e.g., ResNet-50 for images)\n- Use Adam optimizer initially: Learning rate ~ 3e-4; Adam is forgiving compared to SGD\n- Add complexity carefully: Introduce inputs or complexity gradually\n- Disable learning rate decay initially: Use a fixed learning rate until very late\n</code></pre> </li> </ul> </li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#step-4-regularization","title":"Step 4: Regularization","text":"<ul> <li>Once you've achieved overfitting, regularize to improve validation accuracy</li> <li> <p>Recommended regularization methods:</p> <ul> <li>Add more real data: Most effective regularizer</li> <li>Data augmentation: Simple augmentation can dramatically help</li> <li>Creative augmentation: Domain randomization, synthetic/partially synthetic data, GAN-generated samples</li> <li>Pretrained models: Almost always beneficial</li> <li>Avoid overly ambitious unsupervised methods: Stick with supervised learning</li> <li>Reduce input dimensionality: Eliminate irrelevant or noisy features</li> <li>Simplify architecture: Remove unnecessary parameters (e.g., average pooling vs. fully connected layers)</li> <li>Smaller batch size: Stronger regularization effect with batch normalization</li> <li>Dropout carefully: Use spatial dropout (dropout2d) for CNNs cautiously (interacts badly with batch norm)</li> <li>Weight decay: Strengthen weight regularization</li> <li>Early stopping: Halt training based on validation performance</li> <li>Larger model with early stopping: Larger networks sometimes generalize better when stopped early</li> </ul> </li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#step-5-hyperparameter-tuning","title":"Step 5: Hyperparameter Tuning","text":"<ul> <li>Use random search, not grid search: Better coverage, especially with many hyperparameters</li> <li>Bayesian optimization tools can help, but practical gains might be limited. Focus on intuition and experimentation</li> </ul> <p>Tune one by one ranking from most important to least important:</p> <ul> <li>Learning rate</li> <li>Weight decay</li> <li>Dropout rate</li> <li>Batch size</li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#step-6-squeezing-out-final-performance","title":"Step 6: Squeezing Out Final Performance","text":"<ul> <li>Ensemble methods: Reliable 2% accuracy improvement</li> <li>Distillation: Compress ensemble into single model (knowledge distillation)</li> <li>Longer training: Don't prematurely stop training; often performance continues improving subtly</li> </ul>"},{"location":"chapter_neural_networks/dl_tips/#final-checks","title":"Final Checks","text":"<ul> <li>Visualize first-layer weights (should look meaningful, not random noise)</li> <li>Check intermediate activations for artifacts or unexpected patterns</li> </ul>"},{"location":"chapter_neural_networks/dl_workflow/","title":"Deep Learning Training Workflow","text":""},{"location":"chapter_neural_networks/dl_workflow/#training-template","title":"Training Template","text":"<p>A well-organized deep learning project can significantly improve development efficiency, readability, and reproducibility. Below is a comprehensive template for structuring your deep learning training project with separate Python files for different components.</p> <pre><code>project_root/\n\u2502\n\u251c\u2500\u2500 data/                      # Data storage\n\u2502\n\u251c\u2500\u2500 models/                    # Saved model checkpoints \n\u2502   \u2514\u2500\u2500 checkpoints/           # Model checkpoints during training\n\u2502\n\u251c\u2500\u2500 src/                       # Source code\n\u2502   \u251c\u2500\u2500 dataset.py             # Dataset and data loading code\n\u2502   \u251c\u2500\u2500 model.py               # Model architecture definitions\n\u2502   \u251c\u2500\u2500 train.py               # Training loop and logic\n\u2502   \u251c\u2500\u2500 main.py                # Entry point for training\n\u2502   \u251c\u2500\u2500 config.py              # Configuration parameters\n\u2502   \u2514\u2500\u2500 utils.py               # Utility functions\n\u2502\n\u251c\u2500\u2500 sandbox/                 # Jupyter notebooks for exploration\n\u2502\n\u251c\u2500\u2500 results/                   # Training results, logs, etc.\n\u2502   \u251c\u2500\u2500 logs/                  # Training logs\n\u2502   \u2514\u2500\u2500 visualizations/        # Generated plots and visualizations\n\u2502\n\u251c\u2500\u2500 requirements.txt           # Project dependencies\n\u2514\u2500\u2500 README.md                  # Project documentation\n</code></pre> <ul> <li><code>config.py</code>: Configuration parameters for the training.  This should be the only file that you need to change when you want to try different configurations.</li> <li><code>dataset.py</code>: Dataset and data loading code. Define your own dataset class by inheriting from <code>torch.utils.data.Dataset</code>.</li> <li><code>model.py</code>: Model architecture definitions. Define your own model class by inheriting from <code>torch.nn.Module</code>.</li> <li><code>train.py</code>: Training loop and logic. We suggest you to define a <code>Trainer</code> class that handles the training loop and the evaluation loop.</li> <li><code>utils.py</code>: Utility functions for visualization, logging, etc.</li> <li><code>main.py</code>: Entry point for training.</li> </ul> <p>You should maintain a template code for your future projects with the above structure. We have discussed how to define the dataloader, model, and optimizer in the previous lecture. Here we will mention some parts of them again under the context of the deep learning training workflow.</p>"},{"location":"chapter_neural_networks/dl_workflow/#dataset-class","title":"Dataset Class","text":"<p>In the <code>dataset.py</code> file, you should define your own dataset class by inheriting from <code>torch.utils.data.Dataset</code>.</p> <p>You need to implement the following methods:</p> <ul> <li><code>__len__</code>: Return the length of the dataset.</li> <li><code>__getitem__</code>: Return the item at the given index.</li> </ul> <p>Here is an example to generate a dataset from linear model \\(y = 3x + 2 + \\epsilon\\) with Gaussian noise \\(\\epsilon \\sim \\mathcal{N}(0, 2)\\).</p> <pre><code>import torch\nfrom torch.utils.data import Dataset\n\nclass LinearModelDataset(Dataset):\n    def __init__(self, num_samples=100):\n        self.x = torch.randn(num_samples, 1)\n        self.y = 3 * self.x + 2 + torch.randn(self.x.size()) * 2\n    def __len__(self): # Return the length of the dataset\n        return len(self.x)\n\n    def __getitem__(self, idx): # Return the item at the given index\n        # Add any preprocessing here if needed\n        return self.x[idx], self.y[idx]\n</code></pre> <p>Then you can use <code>torch.utils.data.DataLoader</code> to load the dataset for mini-batch training by setting the <code>batch_size</code> and <code>shuffle</code> parameters.</p> <pre><code>from torch.utils.data import DataLoader\ndataset = LinearModelDataset(128)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True) \nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for data, target in dataloader:\n        # data is a batch of input data x\n        # target is a batch of target data y\n        # Write your training process per batch here\n</code></pre>"},{"location":"chapter_neural_networks/dl_workflow/#trainer-class","title":"Trainer Class","text":"<p>In the <code>train.py</code> file, you should define a <code>Trainer</code> class that handles the training. Below is a simplest example for the structure of the <code>Trainer</code> class.</p> <pre><code>class Trainer:\n    def __init__( # Initialize the trainer with all the necessary components\n        self, \n        model, \n        train_loader, \n        val_loader, \n        optimizer, \n        criterion,\n        hparams # other hyperparameters needed for training\n        ):\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n\n    def train(self):\n        for epoch in range(self.hparams.num_epochs):\n            self.model.train()\n            for batch_idx, (data, target) in enumerate(self.train_loader):\n                # Forward pass\n                output = self.model(data)\n                loss = self.criterion(output, target)\n                # Backward pass\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                # Add more code on logging, metrics, etc.\n\n    # Define other functions for evaluation, checkpoint saving, etc.\n</code></pre> <p>You may consider adding more methods to the <code>Trainer</code> class including:</p> <ul> <li>Evaluation: Define a method to evaluate the model on the validation set.</li> <li>Checkpoint: Define a method to save the model checkpoint.</li> <li>Logging: Define a method to log the training process. We will discuss about logging in the next section.</li> <li>Git commit: Define a method to commit the code for the current training.</li> <li>Visualization: Define a method to visualize the training process.</li> </ul> <p>Instead of defining your own <code>Trainer</code> class, you can use existing packages for training, e.g., pytorch-lightning and  Hugging Face Trainer.</p>"},{"location":"chapter_neural_networks/dl_workflow/#model-class","title":"Model Class","text":"<p>In the <code>model.py</code> file, you should define your own model class by inheriting from <code>torch.nn.Module</code>.  You have learned how to define a model class for many neural networks in the previous lecture. </p>"},{"location":"chapter_neural_networks/dl_workflow/#configuration","title":"Configuration","text":"<p>In the <code>config.py</code> file, you should define the configuration parameters for the training.  This should be the only file that you need to change when you want to try different configurations. </p> <p>Below is an example to use dictionaries to store the configuration parameters.</p> <pre><code># config.py\ntrain_config = {\n    # Directories\n    \"checkpoint_dir\": \"models/checkpoints\",\n    \"log_dir\": \"results/logs\",\n    # Training parameters\n    \"optimizer\": \"Adam\",\n    \"learning_rate\": 1e-3,\n    \"epochs\": 10,\n    \"batch_size\": 64\n    # Other parameters\n    \"seed\": 42\n}\n# Configuration parameters for TinyVGG model\nconfig_model = {\n    \"input_channels\": 3,\n    \"num_classes\": 10,\n    \"conv1_channels\": 64,\n    \"conv2_channels\": 128,\n    \"kernel_size\": 3,\n    \"padding\": 1\n}\n</code></pre> <p>People may use different ways to store the configuration parameters. Some people may use <code>yaml</code> to store the configuration parameters. You should find out the best way for your project.</p>"},{"location":"chapter_neural_networks/dl_workflow/#logging","title":"Logging","text":"<p>TensorBoard is a powerful visualization toolkit for TensorFlow that also works seamlessly with PyTorch. It allows you to track and visualize various aspects of your training process, making it easier to understand, debug, and optimize your models.</p> <p>There are other logging tools including Weights &amp; Biases and Comet which is commercialized but free for basic usage.</p> <p>If you're using PyTorch, you'll need the torch.utils.tensorboard module which provides a PyTorch interface to TensorBoard:</p> <pre><code>from torch.utils import tensorboard\n</code></pre> <p>You can initialize a TensorBoard writer:</p> <pre><code>logger = tensorboard.SummaryWriter(log_dir=\"results/logs\")\n</code></pre> <p>You can then start the TensorBoard server:</p> <pre><code>tensorboard --logdir=results/logs\n</code></pre> <p>and then open the TensorBoard in your browser at http://localhost:6006/</p> <p>You can add different types of training results to the TensorBoard including:</p> <ul> <li>Scalar: Add scalar values by <code>logger.add_scalar</code>. You typically use it to log the training loss, accuracy, etc, e.g.,</li> </ul> <pre><code>logger.add_scalar(\"train_loss\", train_loss, global_step=epoch)\nlogger.add_scalar(\"val_loss\", val_loss, global_step=epoch)\n</code></pre> <p></p> <ul> <li>Image: Add images by <code>logger.add_image</code>. You can visualize the input images, model features, the confusion matrix, etc.</li> </ul> <pre><code>logger.add_image(tag=\"input_image\", figure=input_image, global_step=epoch)\n</code></pre> <p></p> <ul> <li>Graph: Add graph by <code>logger.add_graph</code>. This is super useful for visualizing the model architecture via the computational graph. The following code is an example to add the computational graph of the model to the TensorBoard.</li> </ul> <pre><code>dataiter = iter(train_loader)\ndata, _ = next(dataiter)\nlogger.add_graph(model, data)\n</code></pre> <p></p> <ul> <li>Hyperparameters: Add hyperparameters by <code>logger.add_hparams</code>.</li> </ul> <pre><code>hparam_dict = {\n    \"learning_rate\": 1e-3,\n    \"batch_size\": 64,\n    \"epochs\": 10\n}\nmetric_dict = {\n    \"train_loss\": train_loss,\n    \"val_loss\": val_loss\n}\nlogger.add_hparams(hparam_dict, metric_dict)\n</code></pre> <p>This is super useful for comparing different configurations and you can use it to tune the hyperparameters for the best performance logged in <code>metric_dict</code>.</p> <p></p>"},{"location":"chapter_neural_networks/fine_tuning/","title":"Fine-tuning","text":"<p>Fine-tuning is a powerful technique in computer vision where a pre-trained model (typically trained on a large dataset like ImageNet) is adapted to a new, often smaller dataset. Instead of training a model from scratch, fine-tuning leverages the knowledge already captured in the pre-trained model's weights. This approach is particularly effective when you have limited training data. The process typically involves freezing the early layers of the network (which capture generic features like edges and textures) while retraining the later layers (which capture more task-specific features). Fine-tuning generally requires less computational resources and training time compared to training from scratch, and often results in better performance, especially for datasets similar to the one used for pre-training.</p> <p>PyTorch's <code>torchvision</code> package  provides multiple pre-trained models. You can find the full list here. We will use ResNet18 as an example.</p> <pre><code>import torch\nimport torchvision.models as models\n\n# Load pre-trained ResNet18\nresnet18 = models.resnet18(pretrained=True)\n\n# For inference\nresnet18.eval()\ninput_tensor = torch.randn(1, 3, 224, 224)\noutput = resnet18(input_tensor)\n\n# Print the model architecture\nprint(resnet18)\n\n# ResNet(\n#   (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n#   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#   (relu): ReLU(inplace=True)\n#   (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n#   (layer1): Sequential(\n#     (0): BasicBlock(\n#       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n#       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#       (relu): ReLU(inplace=True)\n#       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n#       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#     )\n#     (1): BasicBlock(\n#       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n#       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#       (relu): ReLU(inplace=True)\n#       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n#       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n#     )\n#   )\n#   (layer2): Sequential(\n#     (0): BasicBlock(\n#       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n#       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n# ...\n#   )\n#   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n#   (fc): Linear(in_features=512, out_features=1000, bias=True)\n# )\n</code></pre> <p>Once you print the model architecture, you can see the final fully connected layer is a linear layer with 1000 output features. </p>"},{"location":"chapter_neural_networks/fine_tuning/#fine-tuning-last-layer","title":"Fine-tuning last layer","text":"<p>To fine-tune the pre-trained ResNet on a new task, with new number of classes (say you want to classify 10 classes of dog types instead of 1000 general categories), we need to replace the final fully connected layer with a new linear layer with the number of output features equal to the number of classes in the new task. Then we need to unfreeze the last layer using <code>param.requires_grad = True</code> and freeze all other layers using <code>param.requires_grad = False</code>.</p> <pre><code># For fine-tuning on a new task\nnum_classes = 10  # New number of classes\nresnet18.fc = nn.Linear(512, num_classes)  # Replace the final fully connected layer\nfor param in resnet18.parameters():\n    param.requires_grad = False  # Freeze all layers\n\nfor param in resnet18.fc.parameters():\n    param.requires_grad = True  # Unfreeze only the last layer\n\n# Use resnet18 for the training ...\n</code></pre>"},{"location":"chapter_neural_networks/nn/","title":"Neural Networks","text":""},{"location":"chapter_neural_networks/nn/#linear-classifier","title":"Linear Classifier","text":"<p>Given the input \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)\\), a linear model with multiple outcomes \\(\\mathbf{O} = (O_1, O_2, \\ldots, O_m)\\) is given by:</p> \\[ \\mathbf{O} = \\mathbf{W} \\mathbf{X} + \\mathbf{b} \\] <p>where \\(\\mathbf{W} = (w_{ij})\\) is a \\(m \\times d\\) matrix of weights, and \\(\\mathbf{b} = (b_1, b_2, \\ldots, b_m)\\) is a vector of biases.</p> <p>If the goal is to classify the input \\(\\mathbf{X}\\) into one of \\(m\\) classes, we can use the softmax function to convert the outputs to probabilities to have the linear classifier model</p> \\[ \\mathbf{Y} = \\text{softmax}(\\mathbf{O}) = \\text{softmax}(\\mathbf{\\mathbf{W} \\mathbf{X} + \\mathbf{b}}),  \\] \\[ \\text{where } Y_i = \\frac{\\exp(O_i)}{\\sum_{j=1}^m \\exp(O_j)} \\text{ is the probability} P(Y=i|\\mathbf{X}) \\]"},{"location":"chapter_neural_networks/nn/#cross-entropy-loss","title":"Cross-entropy loss","text":"<p>We have the data \\((\\mathbf{X}_1, \\mathbf{Y}_1), (\\mathbf{X}_2, \\mathbf{Y}_2), \\ldots, (\\mathbf{X}_n, \\mathbf{Y}_n)\\), where \\(\\mathbf{Y}_i\\) is the one-hot encoded target vector for the \\(i\\)-th training example, i.e., \\(Y_{ij} = 1\\) if the \\(i\\)-th training example belongs to class \\(j\\), and \\(0\\) otherwise. The linear classifier model predicts the probability \\(\\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{\\mathbf{W} \\mathbf{X} + \\mathbf{b}})\\). So the negative log-likelihood of the linear classifier is given by:</p> \\[ -\\log P(\\mathbf{Y}|\\mathbf{X}, \\mathbf{W}, \\mathbf{b}) = -  \\sum_{i=1}^n \\log \\prod_{j=1}^m P(Y_{ij} = 1|\\mathbf{X}_i, \\mathbf{W}, \\mathbf{b})^{Y_{ij}} = - \\sum_{i=1}^n \\sum_{j=1}^m Y_{ij} \\log \\hat{Y}_{ij} \\] <p>Therefore, for any predicted probability vector \\(\\hat{\\mathbf{Y}}\\), the cross-entropy loss is given by:</p> \\[ L(\\mathbf{Y}, \\hat{\\mathbf{Y}}) = -  \\sum_{j=1}^m Y_{j} \\log \\hat{Y}_{j} \\] <p>In PyTorch, we can use the <code>nn.CrossEntropyLoss</code> function to compute the cross-entropy loss.</p> <p>Cross-entropy loss in PyTorch</p> <p>The <code>nn.CrossEntropyLoss</code> function expects raw logits, not softmaxed outputs.</p> <p>That means for linear classifier \\(O = Wx + b\\), we predict \\(\\hat{y} = \\text{softmax}(O)\\), but we should pass the raw logits \\(O\\) to the loss function, not the softmaxed outputs \\(\\hat{y}\\).</p> <pre><code>num_classes, num_features = 10, 100\nx = torch.randn(num_features)\ny_true = torch.randint(0, num_classes, (1,))\nmodel = nn.Linear(num_features, num_classes)\nO = model(x) # raw logits\ny_pred = torch.softmax(O, dim=1) # softmaxed outputs\ncriterion = nn.CrossEntropyLoss()\n# You should pass the raw logits $O$ to the loss function, not the softmaxed outputs $y_pred$\nloss = criterion(O, y_true) \n</code></pre>"},{"location":"chapter_neural_networks/nn/#neural-network-architectures","title":"Neural Network Architectures","text":""},{"location":"chapter_neural_networks/nn/#one-hidden-layer-neural-network","title":"One-hidden-layer Neural Network","text":"<p>We consider the one-hidden-layer neural network:</p> \\[ \\begin{align*} \\mathbf{H} &amp;= \\sigma(\\mathbf{W}^{(1)} \\mathbf{X} + \\mathbf{b}^{(1)})\\\\ \\mathbf{O} &amp;= \\sigma(\\mathbf{W}^{(2)} \\mathbf{H} + \\mathbf{b}^{(2)}) \\end{align*} \\] <p>where \\(\\sigma\\) is a non-linear activation function. </p> <p></p>"},{"location":"chapter_neural_networks/nn/#activation-functions","title":"Activation functions","text":"<p>The activation functions introduce non-linearity into neural networks, allowing them to learn and model complex non-linear relationships in the data that would be impossible with just linear transformations.</p> <p>Here are popular activation functions:</p> <ul> <li>Sigmoid: \\(\\sigma(x) = \\frac{1}{1 + \\exp(-x)}\\)</li> <li>ReLU: \\(\\sigma(x) = \\max(0, x)\\)</li> <li>Tanh: \\(\\sigma(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\\)</li> <li>Leaky ReLU: \\(\\sigma(x) = \\max(0.01x, x)\\)</li> </ul> <p></p> <p>Among these, ReLU is the most popular activation function in deep neural networks as it is computationally efficient and has non-vanishing gradient.  In comparison, the sigmoid and tanh activation functions have the problem of vanishing gradient when the input is far away from the origin, which makes the training of deep neural networks less efficient. We will discuss the gradient vanishing problem in the next section.</p> <p></p>"},{"location":"chapter_neural_networks/nn/#multi-layer-neural-network","title":"Multi-layer Neural Network","text":"<p>We can stack multiple one-hidden-layer neural networks to form a multi-layer neural network:</p> \\[ O = W^{(L)} \\sigma(W^{(L-1)} ... \\sigma(W^{(2)} \\sigma(W^{(1)} X + b^{(1)}) + b^{(2)}) ... + b^{(L-1)}) + b^{(L)} \\] <p>Multiple hidden layers allow neural networks to learn hierarchical representations of the data. The early layers typically learn low-level features (like edges and textures in images), while deeper layers combine these to detect higher-level patterns and more abstract concepts. This hierarchical feature extraction enables the network to build increasingly complex and meaningful representations of the input data. For example, in image recognition, the first layer might detect edges, the second layer might combine edges into simple shapes, and deeper layers might recognize complex objects by combining these simpler patterns.</p> <p></p> <p>We suggest you to explore the Neural Network Playground to understand how the depth of the neural network and other training hyperparameters affect the performance of the model. You can see the how different layers of the neural network can learn different features of the data and how different activation functions can affect the performance of the model.</p> <p></p>"},{"location":"chapter_neural_networks/nn/#gradient-vanishing","title":"Gradient Vanishing","text":"<p>A significant challenge when training deep neural networks (DNNs) is the vanishing gradient issue. This phenomenon occurs when gradients flowing back to the initial layers become extremely small during backpropagation. Consequently, parameters in these early layers barely update, causing training to progress very slowly or completely stall. This problem primarily stems from specific activation function choices and the optimization techniques employed in deep architectures.</p> <p>The non-linear activation functions like sigmoid and tanh, while essential for modeling complex relationships, can actually hinder training. These functions tend to saturate\u2014producing near-zero gradients when inputs are either very large or very small. This saturation effect contributes substantially to gradient vanishing. The situation worsens during backpropagation because the chain rule multiplies these tiny gradient values together, further diminishing the signal that reaches the network's earlier layers.</p> <p></p> <p>How to avoid gradient vanishing?</p> <ul> <li>Use the ReLU  or Leaky ReLU activation function, which has a non-vanishing gradient when the input is positive. </li> <li>Monitor the gradient norm, especially for the early layers, during training to check if it is vanishing.</li> <li>We will also discuss the other approaches like Batch Normalization and Residual Network to avoid gradient vanishing.</li> </ul>"},{"location":"chapter_neural_networks/nn/#pytorch-for-neural-networks","title":"PyTorch for Neural Networks","text":"<p>PyTorch provides a convenient API for building neural networks using the <code>torch.nn</code> module.  The basic building block of the <code>torch.nn</code> module includes</p> <ul> <li><code>nn.Linear(in_features, out_features)</code>: a linear layer.</li> <li><code>nn.ReLU()</code>: a ReLU activation function.</li> <li><code>nn.Sigmoid()</code>: a sigmoid activation function.</li> <li><code>nn.Tanh()</code>: a tanh activation function.</li> </ul> <p>In PyTorch, we can simply use the <code>nn.Sequential</code> function to stack multiple layers together:</p> <pre><code>input_size, hidden_size, output_size = 784, 256, 10\nnet = nn.Sequential(nn.Linear(input_size, hidden_size),\n                    nn.ReLU(),\n                    nn.Linear(hidden_size, output_size))\n</code></pre> <p>You can even build a multi-layer neural network by adding layers to the model:</p> <pre><code>net = nn.Sequential()\nnet.append(nn.Linear(input_size, hidden_size))\nnet.append(nn.ReLU())\nnet.append(nn.Linear(hidden_size, output_size))\n\n# You can even use for loop to add layers\ndepth = 3\nnet = nn.Sequential()\nfor _ in range(depth):\n    net.append(nn.ReLU())\n    net.append(nn.Linear(10, 10))\n</code></pre> <p>However, we suggest to use the <code>nn.Module</code> class to define a custom neural network class, as it is more flexible for you to generate multiple instances of the same neural network with different parameters. You need to define a model class that inherits from <code>nn.Module</code> and at least define two functions:</p> <ul> <li><code>__init__</code>: to define and initialize the network parameters.</li> <li><code>forward</code>: to define the forward pass of the network.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TwoLayerNet(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size): # Define and initialize the network parameters\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size)\n        )\n\n    def forward(self, x): # Define the forward pass of the network\n        return self.net(x)\n\n# Initialize the model\ninput_size, hidden_size, output_size = 784, 256, 10\nnet = TwoLayerNet(input_size, hidden_size, output_size)\n\n# Print the model\nfor name, param in net.named_parameters():\n   if param.requires_grad:\n       print(f\"{name}: {param.shape}\")\n\n# net.0.weight: torch.Size([256, 784])\n# net.0.bias: torch.Size([256])\n# net.2.weight: torch.Size([10, 256])\n# net.2.bias: torch.Size([10])\n</code></pre> <p>Custom the neural network</p> <p>You can even define in the <code>forward</code> function on how the model should be executed. PyTorch provide the API <code>torch.nn.functional</code> for common functions. The following way to define the model is equivalent to the previous one but it is more flexible to design your own model. For example, we need to consider a special activation function in the second hidden layer</p> \\[ \\sigma(x;\\alpha) = \\begin{cases}     \\frac{1}{1 + \\alpha \\exp(- x)} &amp; \\text{if } x \\leq 0 \\\\     x &amp; \\text{if } x &gt; 0 \\end{cases} \\] <p>where \\(\\alpha\\) is a parameter also can be learned.</p> <p>Notice that you need to use functions in <code>torch</code> to define your own activation function, otherwise the autograd will not work.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TwoLayerNet(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        self.linear3 = nn.Linear(hidden_size, output_size)\n        self.alpha = nn.Parameter(torch.ones(1))\n\n    def my_activation(self, x):\n        return torch.where(x &lt;= 0, 1 / (1 + self.alpha * torch.exp(-x)), x)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = F.relu(x)\n        x = self.linear2(x)\n        x = self.my_activation(x)\n        x = self.linear3(x)\n        return x\n</code></pre> <p>Stack multiple networks</p> <p>We can also define a building block of the neural network, and repeat it multiple times to form a multi-layer neural network.</p> <p><pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# First define a building block of one-hidden-layer neural network\nclass Block(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# Then define a multi-layer neural network by repeating the building block\nclass MultiLayerNet(nn.Module):\n    def __init__(self, input_size, hidden_size, depth, output_size):\n        super().__init__()\n        self.input_linear = nn.Linear(input_size, hidden_size)\n        self.blocks = nn.Sequential(*[Block(hidden_size, hidden_size) for _ in range(depth)]) # repeat the building block depth times\n        self.output_linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.input_linear(x)\n        x = F.relu(x)\n        x = self.blocks(x)\n        x = self.output_linear(x)\n        return x\n</code></pre> Notice that we use <code>*</code> to unpack the list of blocks in <code>nn.Sequential(*[Block(hidden_size, hidden_size) for _ in range(depth)])</code> as <code>nn.Sequential</code></p>"},{"location":"chapter_neural_networks/pitfall/","title":"PyTorch Pitfalls","text":"<p>This section serves as a collection of common pitfalls and best practices when using PyTorch. You may not encounter all of them, but it's still useful to know what to look out for.</p>"},{"location":"chapter_neural_networks/pitfall/#loss-function","title":"Loss function","text":"<p>The cross entropy loss function <code>nn.CrossEntropyLoss()</code>  in PyTorch expects raw logits, not softmaxed outputs.</p> <p>That means for linear classifier \\(O = Wx + b\\), we predict \\(\\hat{y} = \\text{softmax}(O)\\), but we should pass the raw logits \\(O\\) to the loss function, not the softmaxed outputs \\(\\hat{y}\\).</p>"},{"location":"chapter_neural_networks/pitfall/#common-pitfalls","title":"Common Pitfalls","text":"<ul> <li>You passed softmaxed outputs to a loss that expects raw logits</li> </ul> <pre><code># Wrong\npredictions = torch.softmax(model(data), dim=1)\nloss = nn.CrossEntropyLoss()(predictions, targets)\n# Correct\npredictions = model(data)\nloss = nn.CrossEntropyLoss()(predictions, targets)\n</code></pre> <ul> <li>Inconsistent handling of class dimension in classification tasks</li> </ul> <pre><code># Wrong: targets include class dim\n  predictions = model(data)  # [batch_size, num_classes]\n  targets = F.one_hot(targets, num_classes)  # [batch_size, num_classes]\n  loss = nn.CrossEntropyLoss()(predictions, targets)  # Error!\n\n# Correct: targets are class indices\n  targets = targets  # [batch_size] with class indices\n  loss = nn.CrossEntropyLoss()(predictions, targets)\n</code></pre>"},{"location":"chapter_neural_networks/pitfall/#dropout","title":"Dropout","text":"<p>Dropout is only applied during training. When evaluating the model on the validation set or test set, you should turn it off.</p>"},{"location":"chapter_neural_networks/pitfall/#common-pitfalls_1","title":"Common Pitfalls","text":"<ul> <li>You should not use <code>F.dropout()</code> in the <code>forward</code> method, otherwise the model will implement dropout in evaluation mode as well. Instead, use <code>nn.Dropout()</code> in the model definition.</li> </ul> <pre><code># Wrong\nimport torch.nn.functional as F\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        x = F.dropout(x, p=0.5)\n\n# Correct\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = self.dropout(x)\n</code></pre>"},{"location":"chapter_neural_networks/pitfall/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>nn.Dropout()</code> in the model definition and never use <code>F.dropout()</code>.</li> <li>Always use <code>model.train()</code> before the training and use <code>model.eval()</code> in evaluation.</li> </ul>"},{"location":"chapter_neural_networks/pitfall/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>When accumulating gradients across multiple batches, remember to zero the gradients before each backward pass to avoid incorrect gradient accumulation.</p>"},{"location":"chapter_neural_networks/pitfall/#common-pitfalls_2","title":"Common Pitfalls","text":"<ul> <li>You forgot to <code>.zero_grad()</code>  before <code>.backward()</code></li> </ul> <pre><code># Wrong\noptimizer.zero_grad()\nfor batch in dataloader:\n    loss = model(batch)\n    loss.backward()\n\n# Correct\nfor batch in dataloader:\n    loss = model(batch)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre> <ul> <li>Accumulating losses and taking the gradient</li> </ul> <pre><code># Problematic pattern\nfor batch in dataloader:\n    outputs = model(batch)\n    current_loss = nn.MSELoss()(outputs, batch.target)\n    loss += current_loss.item()  # In-place operation!\n\nloss.backward()  \n</code></pre> <p>There are two problems. </p> <p>First, Calling <code>.item()</code> detaches the value from the computation graph, so when you do: <code>loss += current_loss.item()</code>, you are backpropagating on a float\u2014not a proper tensor with gradients.</p> <p>Second, <code>loss += current_loss</code> is an in-place operation, and it can break the computation graph. You should use <code>loss = loss + current_loss</code> to keep the computation graph intact.</p>"},{"location":"chapter_neural_networks/pitfall/#best-practices_1","title":"Best Practices","text":"<ul> <li>Zero gradients between backward passes. By default, always use this code snippet:</li> </ul> <pre><code>optimizer.zero_grad()\nloss.backward()\noptimizer.step()\n</code></pre> <ul> <li>If you need to write your own optimizer or manipulate gradients, e need to disable the gradient tracking for parameter updates by <code>with torch.no_grad()</code>.</li> </ul> <pre><code>with torch.no_grad():\n    x -= 0.1 * x.grad\n</code></pre> <ul> <li>The only exception is when you need to update the parameters, e.g. when you are using gradient accumulation. Sometimes, to save the memory, we may not call <code>optimizer.step()</code> until accumulating the gradients for several batches:</li> </ul> <pre><code>for i, batch in enumerate(dataloader):\n    loss = model(batch)\n    loss.backward()\n    # Don't call optimizer.step() here\n    if i % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"chapter_neural_networks/pitfall/#in-place-operations","title":"In-place operations","text":"<p>In-place operations in PyTorch modify tensors directly rather than creating new ones. While they can save memory, they come with important limitations and potential pitfalls. Most of the methods end with <code>_</code> in PyTorch are in-place operations. Examples of common in-place operations:</p> <ul> <li><code>tensor.add_(value)</code></li> <li><code>tensor.mul_(value)</code></li> <li><code>tensor.copy_(source)</code></li> <li><code>tensor.zero_()</code></li> <li><code>tensor.zero_()</code></li> </ul> <p>In PyTorch, </p> <ul> <li><code>x += 1</code> is an in-place operation that modifies the tensor directly, equivalent to calling <code>x.add_(1)</code>. The underscore suffix in PyTorch methods indicates in-place operations.</li> <li><code>x = x + 1</code> creates a new tensor with the result and assigns it to the variable <code>x</code>. The original tensor is not modified.</li> </ul>"},{"location":"chapter_neural_networks/pitfall/#common-pitfalls_3","title":"Common Pitfalls","text":"<ul> <li>Breaking the computational graph</li> </ul> <p>The most serious pitfall is modifying tensors that are part of an active computational graph:</p> <p><pre><code>x = torch.tensor([1.0], requires_grad=True)\ny = x * 2\nx.add_(1)  # In-place modification of x\nz = y + 3  # y depends on the original value of x\nz.backward()  # RuntimeError: leaf variable has been modified by an inplace operation\n</code></pre> When you modify <code>x</code> in-place, the relationship between <code>x</code> and <code>y</code> is invalidated because <code>y</code> was computed using the original value of <code>x</code>.</p>"},{"location":"chapter_neural_networks/pitfall/#best-practices_2","title":"Best Practices","text":"<ul> <li>Avoid in-place operations on tensors that require gradients</li> <li>Use <code>x.clone()</code> or <code>x.copy_()</code> to create a new tensor with the same value as <code>x</code>, but not part of the computational graph:</li> <li>When modifying parameters in-place, do so after optimizer steps and use <code>with torch.no_grad()</code>.</li> </ul>"},{"location":"chapter_neural_networks/pitfall/#shape-manipulation","title":"Shape Manipulation","text":"<p>Use <code>squeeze()</code> when: - You specifically want to remove dimensions of size 1 - You need to normalize tensor shape to remove singleton dimensions - You're matching shapes for operations that don't support broadcasting - You're working with models that add extra dimensions (like unsqueeze)</p> <pre><code># Good use of squeeze() - removing specific singleton dimensions\nx = torch.randn(10, 1, 20, 1)\nsqueezed = x.squeeze()  # Removes all dimensions of size 1: [10, 20]\nsqueezed_specific = x.squeeze(1)  # Only removes dim 1: [10, 20, 1]\n</code></pre> <p>Use <code>flatten()</code> when: - You specifically want to flatten consecutive dimensions - You want more readable and self-documenting code - You want to preserve batch dimensions (using start_dim) - You need the specific semantic meaning of flattening</p> <pre><code># Good use of flatten() - clear intention\nx = torch.randn(32, 3, 224, 224)  # [batch, channels, height, width]\nflattened = x.flatten(start_dim=1)  # [32, 3*224*224] - preserve batch dimension\n</code></pre> <p>Use <code>reshape()</code> when: - You're unsure about contiguity of your tensor - Your code needs to work with potentially non-contiguous tensors - You want safer code that won't throw contiguity errors - You need a general-purpose reshaping solution</p> <pre><code># Good use of reshape() - more robust\nx = some_function_that_might_return_non_contiguous_tensor()\nreshaped = x.reshape(batch_size, num_features)  # Works regardless of contiguity\n</code></pre> <p>Use <code>view()</code> when: - Your tensor is definitely contiguous - You need maximum performance (no data copying) - You're reshaping in a straightforward way without changing element order - You're in a performance-critical section of code</p> <pre><code># Good use of view()\nx = torch.randn(32, 3, 224, 224)  # Fresh tensor is contiguous\nflattened = x.view(32, -1)  # Efficiently reshape without copying\n</code></pre>"},{"location":"chapter_neural_networks/pitfall/#decision-flowchart","title":"Decision Flowchart","text":"<ol> <li> <p>Are you removing dimensions of size 1?</p> <ul> <li>If yes \u2192 Use <code>squeeze()</code></li> </ul> </li> <li> <p>Are you flattening consecutive dimensions?</p> <ul> <li>If yes \u2192 Use <code>flatten()</code> for readability</li> </ul> </li> <li> <p>Are you unsure about tensor contiguity?</p> <ul> <li>If yes \u2192 Use <code>reshape()</code> for safety</li> </ul> </li> <li> <p>Is the tensor definitely contiguous and performance critical?</p> <ul> <li>If yes \u2192 Use <code>view()</code> for maximum performance</li> </ul> </li> </ol>"},{"location":"chapter_neural_networks/pitfall/#best-practices_3","title":"Best Practices","text":"<ul> <li> <p>Be Explicit About Your Model's Output Shape <pre><code>def forward(self, x):\n    # Be explicit about output shape\n    x = self.linear(x)  # Shape: [batch_size, 1]\n    return x.squeeze(1)  # Shape: [batch_size]\n</code></pre></p> </li> <li> <p>Use Shape Assertions for Critical Layers <pre><code>def forward(self, x):\n    assert x.dim() == 2, f\"Expected input to have 2 dimensions, got {x.dim()}\"\n    output = self.linear(x)\n    output = output.squeeze(1)\n    assert output.dim() == 1, f\"Expected output to have 1 dimension, got {output.dim()}\"\n    return output\n</code></pre></p> </li> </ul>"},{"location":"chapter_neural_networks/pitfall/#common-pitfalls_4","title":"Common Pitfalls","text":"<p>Here are some common pitfalls when manipulating tensor shapes in PyTorch:</p> <p>Contiguity-Related Pitfalls</p> <ul> <li> <p>Using <code>view()</code> on non-contiguous tensors <pre><code>x = torch.randn(4, 5).transpose(0, 1)  # Non-contiguous after transpose\nx.view(-1)  # ERROR: view size is not compatible with input tensor's size and stride\n</code></pre> Solution: Use <code>reshape()</code> or call <code>contiguous()</code> first: <code>x.contiguous().view(-1)</code></p> </li> <li> <p>Forgetting that operations like <code>transpose()</code>, <code>permute()</code>, and slicing create non-contiguous tensors <pre><code>x = torch.randn(10, 20, 30)\ny = x.permute(2, 0, 1)  # Now non-contiguous\n</code></pre> Solution: Never use <code>view()</code> unless you are sure the tensor is contiguous.</p> </li> </ul> <p>Dimension-Related Pitfalls</p> <ul> <li>Incorrectly calculating dimensions for reshape <pre><code>x = torch.randn(10, 3, 224, 224)\n# Wrong: miscalculated dimensions\nx.reshape(10, 150528)  # Should be 3*224*224 = 150528\n# Correct: use -1 for automatic calculation\nx.reshape(10, -1) # or x.flatten(start_dim=1)\n</code></pre> Solution: Never compute the dimensions manually. Use <code>-1</code> for automatic calculation or clearly document the math</li> </ul> <p>Broadcasting and Operation Pitfalls</p> <p>Pytorch supports broadcasting, which means that it will automatically expand tensors to the same shape when performing operations. Though it is convenient some times, it can also lead to unexpected results.</p> <ul> <li> <p>Unexpected broadcasting behavior <pre><code>a = torch.randn(10, 1)\nb = torch.randn(1, 20)\nc = a * b  # Results in shape [10, 20] through broadcasting\n</code></pre> Solution: Be explicit about intended broadcasting with <code>expand</code> or <code>repeat</code></p> </li> <li> <p>Dimension mismatch between predictions and targets <pre><code>model = nn.Linear(10, 1)\ndata = torch.randn(10)\ntarget_data = torch.randn(10)\npredictions = model(data)  # Shape: [batch_size, 1]\ntargets = target_data      # Shape: [batch_size]\n# Wrong: this will not raise an error, but the loss will be incorrect by broadcasting\nloss = nn.MSELoss()(predictions, targets)  \n# Correct: \npredictions = model(data).squeeze()\nloss = nn.MSELoss()(predictions, targets)\n</code></pre></p> </li> </ul> <p>Solution: In your <code>forward</code> method, make sure to squeeze the output of the model to match the target shape.</p>"},{"location":"chapter_neural_networks/regularization/","title":"Regularization of Neural Networks","text":"<p>Regularization techniques help prevent overfitting in neural networks by constraining the model's complexity, enabling better generalization to unseen data. This lecture examines several key regularization methods used in modern neural networks.</p>"},{"location":"chapter_neural_networks/regularization/#overfitting","title":"Overfitting","text":"<p>Overfitting is a problem that arises when the model fits the training data too well, making it unable to predict well using new data. </p> <p></p> <p>Modern neep neural network models have a large number of parameters, which makes them prone to overfitting. Therefore, we need to use regularization techniques to prevent overfitting.</p>"},{"location":"chapter_neural_networks/regularization/#double-descent","title":"Double Descent","text":"<p>The classical bias-variance trade-off suggests that as model complexity increases, bias decreases (the model can fit the training data better) but variance increases (the model becomes more sensitive to fluctuations in the training data). You will see a typical U-shaped curve of the test error as we increase the model complexity.</p> <p></p> <p>Howeever, in deep learning, researchers have revealed a more complex phenomenon called double descent which was first proposed by Belkin et al. (2019), which challenges the classical bias-variance trade-off. In double descent:</p> <ol> <li>The test error initially follows the classical U-shaped curve as model complexity increases</li> <li>However, at a critical threshold (often when the model has just enough capacity to fit the training data perfectly (overparameterized)), test error spikes dramatically</li> <li>Surprisingly, as model complexity continues to increase beyond this threshold, test error begins to decrease again</li> </ol> <p>This creates a \"double descent\" curve where test error decreases, then increases, then decreases again. This phenomenon suggests that in many deep learning scenarios, larger models can actually generalize better, contrary to traditional statistical wisdom.</p> <p></p> <p>Double descent has been observed across various neural network architectures and helps explain why modern overparameterized networks can generalize well despite having far more parameters than training examples. However, this does not mean double descent is always the case for deeper neural networks. In practice, we still need to implement regularization techniques if the model is hard to generalize.</p>"},{"location":"chapter_neural_networks/regularization/#l2-regularization","title":"L2 Regularization","text":"<p>L2 regularization, also known as weight decay in Pytorch, is one of the most common regularization techniques. It adds a penalty term to the loss function that is proportional to the squared magnitude of the weights.</p> <p>Neural networks with large weights often create sharp transitions in the decision boundary, making them more likely to overfit the training data. L2 regularization encourages the network to use smaller weights, resulting in smoother decision boundaries and better generalization.</p> <p>Given the original loss function \\(L_0(\\theta)\\) where \\(\\theta\\) represents all model parameters, the L2 regularized loss is:</p> \\[ L(\\theta) = L_0(\\theta) + \\frac{\\lambda}{2} \\|\\theta\\|_2^2 \\] <p>where \\(\\lambda\\) is the regularization strength hyperparameter. The gradient of this term with respect to each weight is simply \\(\\lambda w\\), which means during gradient descent, each weight update includes a term that shrinks the weight proportionally to its magnitude:</p> \\[ w \\leftarrow w - \\eta \\left( \\frac{\\partial L_0}{\\partial w} + \\lambda w \\right) = (1 - \\eta\\lambda)w - \\eta\\frac{\\partial L_0}{\\partial w} \\] <p>where \\(\\eta\\) is the learning rate. This effectively scales the weight by a factor slightly less than 1 at each update, hence the name \"weight decay.\"</p> <p>PyTorch Implementation</p> <p>In PyTorch, L2 regularization is implemented simply by adding the <code>weight_decay</code> parameter to the optimizer:</p> <pre><code># Create optimizer with L2 regularization (weight_decay)\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n</code></pre> <p>The <code>weight_decay</code> parameter corresponds to the \\(\\lambda\\) in the mathematical formulation.</p>"},{"location":"chapter_neural_networks/regularization/#dropout","title":"Dropout","text":"<p>Dropout is a powerful regularization technique that randomly \"drops out\" (sets to zero) a fraction of neurons during each training iteration, effectively training a different sub-network each time.</p> <p>When neurons co-adapt and develop complex dependencies during training, the network may become overly specialized to the training data. Dropout forces neurons to learn more robust features by preventing them from relying too heavily on specific neurons. It can be viewed as implicitly training an ensemble of many sub-networks, which are averaged at test time.</p> <p>For a layer with output \\(\\mathbf{y}\\), dropout applies:</p> \\[ \\mathbf{y} = \\frac{\\mathbf{m} \\odot \\mathbf{a}}{p} \\] <p>where \\(\\mathbf{a}\\) is the pre-dropout activation, \\(\\odot\\) is element-wise multiplication, and \\(\\mathbf{m}\\) is a binary mask where each element is drawn from a Bernoulli distribution with probability \\(p\\) (keep probability):</p> \\[ m_i \\sim \\text{Bernoulli}(p) \\] <p></p> <p>PyTorch Implementation</p> <p>In PyTorch, dropout is implemented using the <code>nn.Dropout</code> module:</p> <pre><code>import torch.nn as nn\n\nclass TwoLayerNet(nn.Module):\n    def __init__(self, D_in, H, D_out): \n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(H, D_out)\n        )\n\n    def forward(self, x): \n        return self.net(x)\n</code></pre> <p>Dropout only during training</p> <p>The dropout is only active during training. You need to call <code>model.train()</code> before training to activate dropout and <code>model.eval()</code> during evaluation to deactivate dropout. </p> <p>You should use <code>torch.nn.Dropout</code>  instead of <code>torch.nn.functional.dropout</code> when defining your model with dropout. If you use <code>torch.nn.functional.dropout</code>, the dropout will be applied even during evaluation.</p>"},{"location":"chapter_neural_networks/regularization/#batch-normalization","title":"Batch Normalization","text":"<p>Deep neural networks can suffer from internal covariate shift, where the distribution of each layer's inputs changes during training as the parameters of previous layers change. This slows down training and may lead to vanishing/exploding gradients. Batch normalization addresses this by normalizing layer inputs, allowing higher learning rates, reducing the sensitivity to initialization, and acting as a form of regularization.</p> <p>For a mini-batch of activations \\(\\mathbf{x} = \\{x_1, x_2, ..., x_m\\} = \\mathcal{B}\\) for a particular neuron, batch normalization performs:</p> \\[    \\text{BN}(\\mathbf{x}) = \\gamma \\odot \\frac{\\mathbf{x} - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}} + \\beta \\] <p>where \\(\\mu_{\\mathcal{B}}\\) and \\(\\sigma_{\\mathcal{B}}\\) are the batch mean and variance of \\(\\mathbf{x}\\), \\(\\epsilon\\) is a small constant for numerical stability, \\(\\gamma\\) and \\(\\beta\\) are learnable parameters that allow the network to undo the normalization if needed.</p> <p>PyTorch Implementation</p> <p>In PyTorch, batch normalization is implemented using <code>nn.BatchNorm1d</code>, <code>nn.BatchNorm2d</code>, or <code>nn.BatchNorm3d</code> depending on the input dimensions:</p> <pre><code>import torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.BatchNorm1d(hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <p>Batch normalization is typically applied before the activation function, though some research suggests applying it after the activation can be beneficial in certain cases.</p>"},{"location":"chapter_neural_networks/resnet/","title":"Residual Networks","text":"<p>As neural networks grew deeper, researchers encountered a counterintuitive problem: adding more layers to a deep network led to higher training error, not just higher testing error. This phenomenon, known as the degradation problem, could not be explained by overfitting alone.</p> <p>The issue wasn't just vanishing gradients (which can be partially addressed by proper initialization and batch normalization), but rather the difficulty in optimizing very deep networks. Traditional deep networks struggle to learn identity mappings when they would be optimal, leading to degraded performance as depth increases. The following figure shows a training curve (non-bold line) and testing curve (bold line) on the ImageNet dataset with a network of depth 18 and 34. The left plot uses a traditional deep network and you can see that the validation error increases when model becomes deeper. </p> <p></p> <p>Theoretically, a deeper network should be able to perform at least as well as a shallower one (by learning identity mappings in the added layers), but in practice, directly stacking more layers made optimization more difficult.</p>"},{"location":"chapter_neural_networks/resnet/#residual-networks_1","title":"Residual Networks","text":"<p>To overcome the degradation problem, Kaiming He et al. (2015) proposed a new architecture called Residual Networks (ResNet) which uses skip connections (also called shortcut connections) to help the network learn identity mappings.</p> <p>The key insight of ResNet is the introduction of residual learning through skip connections. Instead of hoping that stacked layers directly fit a desired underlying mapping \\(F(x)\\), we explicitly let these layers fit a residual mapping </p> \\[ H(x) = F(x) + x. \\] <p>This reformulation makes it easier for the network to learn identity mappings (when optimal) by simply pushing the weights of the residual function \\(F(x)\\) toward zero.</p> <p></p>"},{"location":"chapter_neural_networks/resnet/#skip-connections","title":"Skip connections","text":"<p>When the input and output dimensions are the same, the skip connection is simply combination: \\(H(x) = F(x) + x\\).</p> <p>When the dimensions of \\(x\\) and \\(F(x)\\) are different, the skip connection is a linear projection of the input to match the dimensions of the output: \\(H(x) = F(x) + Wx\\). In practice, we use a 1x1 convolution to match the dimensions of the input and output:</p> <pre><code># You can choose the stride to match the dimensions of the input and output\nY = nn.functional.conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)(X)\noutput = Y + F(X)\n</code></pre> <p></p> <p>In order to make the skip connection robust to different dimensions, we use a 1x1 convolution to match the dimensions of the input and output.</p> <p>Here is the suggested practice for implementing the residual block:</p> <ul> <li>Use a 1x1 convolution to match the dimensions of the input and output.</li> <li>Add an <code>if</code> statement to check if the dimensions of the input and output are the same. If they are, use the identity skip connection. If they are not, use the 1x1 convolution to match the dimensions of the input and output.</li> <li>If you need to downsample the input, always downsample in the first convolutional layer of the residual block controlled by <code>stride</code> and use the same <code>stride</code> for the 1x1 convolution skip connection.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # Robust shortcut logic for downsampling or channel mismatch\n        if in_channels != out_channels or stride != 1:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n                          stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels) # Batch normalization is used to stabilize the output of the skip connection\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return torch.relu(out)\n</code></pre>"},{"location":"chapter_neural_networks/resnet/#resnet-architecture","title":"ResNet Architecture","text":"<p>We will use ResNet18 as an example to understand the architecture of ResNet.</p> <p>ResNet18 consists of:</p> <ol> <li>Initial Convolution: 7\u00d77 Conv, 64 filters, stride 2</li> <li>Max Pooling: 3\u00d73 MaxPool, stride 2</li> <li>4 Stages of Residual Blocks:</li> <li>Stage 1: 2 residual blocks, 64 filters</li> <li>Stage 2: 2 residual blocks, 128 filters</li> <li>Stage 3: 2 residual blocks, 256 filters </li> <li>Stage 4: 2 residual blocks, 512 filters</li> <li>Global Average Pooling</li> <li>Fully Connected Layer: 512 to num_classes</li> </ol> <p></p> <p>Each stage (except the first) begins with a residual block that has stride 2 to perform downsampling. The spatial dimensions are halved while the number of filters is doubled, maintaining computational complexity per layer. The bashed line in the above figure is the skip connection with different dimensions. And the solid line is the identity skip connection.</p> <p>With the <code>ResidualBlock</code> defined in the previous section, we can implement the ResNet18 architecture:</p> <pre><code>class ResNet18(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Stage 1\n        self.layer1 = nn.Sequential(\n            ResidualBlock(64, 64),\n            ResidualBlock(64, 64)\n        )\n\n        # Stage 2\n        self.layer2 = nn.Sequential(\n            ResidualBlock(64, 128, stride=2),\n            ResidualBlock(128, 128)\n        )\n\n        # Stage 3\n        self.layer3 = nn.Sequential(\n            ResidualBlock(128, 256, stride=2),\n            ResidualBlock(256, 256)\n        )\n\n        # Stage 4\n        self.layer4 = nn.Sequential(\n            ResidualBlock(256, 512, stride=2),\n            ResidualBlock(512, 512)\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"chapter_optimization/","title":"Index","text":""},{"location":"chapter_optimization/#overview","title":"Overview","text":"<p>This chapter covers optimization methods. We will focus on the optimization algorithms motivated by data analysis and machine learning.</p> <p></p>"},{"location":"chapter_optimization/#lectures","title":"Lectures","text":"<ul> <li>Convexity</li> <li>Rate of Convergence</li> <li>PyTorch Basics</li> <li>Gradient Descent</li> <li>Accelerated Gradient Descent</li> <li>Stochastic Gradient Descent</li> <li>Proximal Gradient Descent</li> <li>Mirror Descent</li> <li>Nesterov's Smooth Method</li> <li>Duality and ADMM</li> </ul>"},{"location":"chapter_optimization/agd/","title":"Accelerated Gradient Descent","text":"<p>Looking at the trajectory of gradient descent, we notice it often follows a zigzag pattern. This happens because the steepest descent direction only uses local information about the objective function, making it shortsighted. Can we design an algorithm that converges faster?</p> <p></p> <p>We might consider these strategies to improve upon gradient descent: - Use the history of the trajectory - Add momentum for a smoother path</p> <p>Instead of only using the local gradient at \\(x_t\\), we can combine \\(-\\nabla f(x_t)\\) with information from previous points, like \\(x_t - x_{t-1}\\). This creates a more stable direction with less zigzagging. Nesterov proposed the following accelerated gradient descent algorithm: This idea is also called \"momentum\", borrowed the intuition from the physics.</p> <p></p> <p>Nesterov's Accelerated Gradient Descent (AGD)</p> <p>Initialize \\(x_0\\)\uff0c\\(m_0 = 0\\), and choose \\(\\beta_t = \\frac{t}{t+3}\\), then:</p> \\[ \\begin{align*} m_{t+1} &amp;= \\beta_t m_t + \\eta_t \\nabla f(x_{t}+\\beta_t m_t)\\\\ x_{t+1} &amp;= x_t - m_{t+1} \\end{align*} \\] <p>Nesterov's AGD is a \u201clookahead\u201d version of the momentum method by looking ahead for the gradient at \\(y_{t+1}  = x_t + \\beta_t m_t = x_{t+1} + \\beta_t (x_{t+1} - x_t)\\).</p> <p>Here is the implementation of Nesterov's AGD in PyTorch:</p> <pre><code>def f(x):\n    ... # define the objective function\n\n# Initialize parameters\nx = torch.zeros(dim, requires_grad=True)  # Initialize x\nm = torch.zeros(dim)  # Initialize momentum m\n\n# AGD parameters\nlr = 0.1  # Learning rate (\u03b7_t)\nnum_iters = 100  # Number of iterations\nfor t in range(num_iters):\n    beta_t = t / (t + 3)  # Compute \u03b2_t\n\n    # Compute the gradient at x_t + \u03b2_t * m_t\n    y = x + beta_t * m  # Look-ahead step\n    loss = f(y)\n    loss.backward()\n\n    with torch.no_grad():\n        # Update momentum\n        m_new = beta_t * m + lr * y.grad\n        # Update x\n        x -= m_new\n        # Update variables for the next iteration\n        m.copy_(m_new)  # In-place update of m\n        x.grad.zero_()  # Reset gradient\n</code></pre> <p>Notice you need to use in-place operations for momentum like <code>m.copy_(m_new)</code> to maintain the connection to the computational graph. The momentum term <code>m</code> is used in the look-ahead step <code>y = x + beta_t * m</code>, and this relationship must be maintained correctly.</p> <p></p> <p>General momentum update: The momentum method has the following update:</p> \\[ \\begin{align*} m_{t+1} &amp;= \\beta m_t + \\nabla f(x_{t})\\\\ x_{t+1} &amp;= x_t - \\eta_t m_{t+1} \\end{align*} \\] <p>Unlike gradient descent, momentum method uses the history of the trajectory:</p> \\[ m_{t+1} = \\beta m_t +  \\nabla f(x_{t}) = \\beta^2 m_{t-1} + \\beta \\nabla f(x_{t-1}) + \\nabla f(x_{t}) = \\cdots = \\sum_{i=0}^{t} \\beta^{t-i} \\nabla f(x_{i}) \\] <p>Therefore, the momentum is a weighted average of the gradients in the history trajectory. </p> <p>Because it uses historical information, AGD's trajectory is not monotonically decreasing toward the minimum.</p> <p></p> <p>Theorem (Convergence of AGD): Let \\(f\\) be convex and \\(L\\)-smooth. If we choose \\(\\eta_t = 1/L\\), then Nesterov's accelerated gradient descent achieves:</p> \\[ f(x_t) - f(x^*) \\leq \\frac{2L\\|x_0 - x^*\\|_2^2}{(t+1)^2} \\] <p>This means AGD has a convergence rate of \\(O(1/t^2)\\), which is faster than gradient descent's \\(O(1/t)\\) rate. In other words, for convex and smooth functions, AGD can achieve \\(\\epsilon\\)-accuracy within \\(O(1/\\sqrt{\\epsilon})\\) steps. </p>"},{"location":"chapter_optimization/convexity/","title":"Convex Optimization","text":""},{"location":"chapter_optimization/convexity/#convex-problems","title":"Convex Problems","text":"<p>Many statistical estimators can be formulated as optimization problems. A classic example is the Lasso estimator:</p> \\[ \\min_{\\beta \\in \\mathbb{R}^d} \\frac{1}{2n}\\|Y-X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1, \\] <p>where \\(\\|\\beta\\|_1 = \\sum_{j=1}^d |\\beta_j|\\) is the \\(\\ell_1\\) norm penalty to encourage sparsity of \\(\\beta\\), i.e., making most of the coefficients of \\(\\beta\\) zero.</p> <p>This is an unconstrained problem as we find the minimizer over the entire \\(\\mathbb{R}^d\\). For comparison, the constrained Lasso is formulated as:</p> \\[ \\min_{\\beta} \\|Y - X\\beta\\|_2^2, \\text{ subject to } \\|\\beta\\|_1 \\leq t \\] <p>which has the constraint \\(\\|\\beta\\|_1 \\leq t\\).</p> <p>Notice that the loss function of Lasso is not smooth because of the \\(\\ell_1\\) penalty. We will mainly focus on the smooth objective functions and constraints and the content involving non-smooth functions will be optional.</p>"},{"location":"chapter_optimization/convexity/#gradient-and-hessian","title":"Gradient and Hessian","text":"<p>When dealing with a \\(d\\)-dimensional function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\), optimization typically involves its gradient \\(\\nabla f \\in \\mathbb{R}^d\\) and Hessian matrix \\(\\nabla^2 f \\in \\mathbb{R}^{d\\times d}\\). For modern data analysis, it's preferable to use only gradients and avoid Hessian matrices because:</p> <ul> <li>Vector operations are much faster than matrix operations</li> <li>When \\(d\\) is extremely large (e.g., millions in genomics datasets), storing a \\(d \\times d\\) matrix becomes impractical</li> </ul> <p>Optimization algorithms that only use gradients are called first-order methods.  And the algorithms that use both gradients and Hessian matrices are called second-order methods.</p>"},{"location":"chapter_optimization/convexity/#convex-sets-and-functions","title":"Convex Sets and Functions","text":"<p>Definition (Convex set and function): A set \\(\\mathcal{X}\\) is convex if </p> \\[    (1-\\gamma)x + \\gamma y \\in \\mathcal{X}, \\text{ for any } x, y \\in \\mathcal{X}, \\gamma \\in [0,1] \\] <p>A function \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\) is convex if </p> \\[    f((1-\\gamma)x + \\gamma y) \\leq (1-\\gamma)f(x) + \\gamma f(y), \\text{ for any } x,y \\in \\mathcal{X}, \\gamma \\in [0,1] \\] <p>Geometrically, a set is convex if it contains all line segments between any two points in the set. A function is convex if its graph lies below or on any chord connecting two points on the graph.</p> <p> </p> <p>Definition (Convex optimization): An optimization problem is called convex optimization if it can be formulated as:</p> \\[ \\begin{align} &amp;\\min_x f(x)\\\\ &amp;\\text{subject to } x \\in \\mathcal{X} \\end{align} \\] <p>where both \\(f\\) and \\(\\mathcal{X}\\) are convex.</p> <p>Both the unconstrained and constrained Lasso are examples of convex optimization problems.</p> <p>Many optimization problems in data analysis have nice properties. We will formally define the nice properties called \"strongly convex\" and \"smooth\" below, but informally speaking, an objective function \\(f(x)\\) is good enough if it can be upper and lower bounded by a quadratic function:</p> \\[  f(x) + \\nabla f(x)^\\top(y-x) + \\frac{\\mu}{2}\\|x-y\\|_2^2 \\leq f(y) \\leq f(x) + \\nabla f(x)^\\top(y-x) + \\frac{L}{2}\\|x-y\\|_2^2  f(y)  \\] <p>for any points \\(x\\) and \\(y\\).</p> <p>Most of time \\(f\\) has Hessian matrix \\(\\nabla^2 f(x)\\), and the above inequality is equivalent to the eigenvalue condition:</p> \\[ 0&lt; \\mu \\leq \\lambda_{\\min}(\\nabla^2 f(x)) \\leq \\lambda_{\\max}(\\nabla^2 f(x)) \\leq L &lt; \\infty \\] <p>However, in the so-called \"high-dimensional statistics\" case, when the sample size \\(n\\) is much smaller than the number of parameters \\(d\\), the objective function \\(f\\) is usually only convex but not strongly convex, e.g., the Lasso in the beginning.</p> <p>The following figure shows the illustration of the above inequality. </p>"},{"location":"chapter_optimization/convexity/#strong-convexity-condition","title":"Strong Convexity Condition","text":"<p>For optimization algorithms to converge efficiently, the function \\(f(x)\\) should not only be convex but also exhibit a certain level of curvature. This is formalized with the concept of strong convexity.</p> <p>Definition (Strongly Convex): A continuously differentiable function \\(f\\) is \\(\\mu\\)-strongly convex if:</p> \\[ f(y) \\geq f(x) + \\nabla f(x)^\\top(y-x) + \\frac{\\mu}{2}\\|x-y\\|_2^2 \\] <p>for any points \\(x\\) and \\(y\\).</p> <p>The expression \\(f(x) + \\nabla f(x)^\\top(y-x)\\) represents the first-order Taylor expansion of \\(f(y)\\). Strong convexity means that the function \\(f(y)\\) is bounded below by its first-order approximation plus a quadratic term \\(\\frac{\\mu}{2}\\|x-y\\|_2^2\\).</p> <p>For functions with second derivatives, the second-order Taylor expansion gives:</p> \\[ f(y) \\approx f(x) + \\nabla f(x)^\\top(y-x) + \\frac{1}{2}(y-x)^\\top \\nabla^2 f(x)(y-x) \\] <p>Comparing this with the strong convexity definition, we can see that strong convexity implies the Hessian \\(\\nabla^2 f(x)\\) must be sufficiently large. In fact, strong convexity is equivalent to the following properties:</p> <ul> <li>If \\(f\\) has second derivatives, strong convexity is equivalent to \\(\\nabla^2 f(x) \\succeq \\mu \\mathbf{I}_d\\) for any \\(x\\), which ensures that \\(f\\) has a minimum level of curvature.</li> <li>Strong convexity is also equivalent to \\(\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\geq \\mu\\|x-y\\|_2\\), meaning the gradient of \\(f\\) changes at least linearly with respect to the distance between \\(x\\) and \\(y\\).</li> </ul> <p>For example, the quadratic function \\(f(x) = \\frac{1}{2}x^\\top Q x\\) is \\(\\mu\\)-strongly convex if the smallest eigenvalue of \\(Q\\) is at least \\(\\mu\\).</p> <p>In contrast, a linear function \\(f(x) = a^\\top x + b\\) is not strongly convex because it lacks curvature.</p>"},{"location":"chapter_optimization/convexity/#smoothness-condition","title":"Smoothness Condition","text":"<p>For gradient descent to converge effectively, the function \\(f(x)\\) cannot be too steep or irregular. We formalize this requirement with the concept of \\(L\\)-smoothness.</p> <p>Definition (\\(L\\)-smooth): A continuously differentiable function \\(f\\) is \\(L\\)-smooth if:</p> \\[ f(y) \\leq f(x) + \\nabla f(x)^\\top(y-x) + \\frac{L}{2}\\|x-y\\|_2^2 \\] <p>for any points \\(x\\) and \\(y\\).</p> <p>The expression \\(f(x) + \\nabla f(x)^\\top(y-x)\\) represents the first-order Taylor expansion of \\(f(y)\\). So \\(L\\)-smoothness means that the difference between the function and its first-order approximation can be bounded by the quadratic term \\(\\frac{L}{2}\\|y-x\\|_2^2\\).</p> <p>For functions with second derivatives, the second-order Taylor expansion gives:</p> \\[ f(y) \\approx f(x) + \\nabla f(x)^\\top(y-x) + \\frac{1}{2}(y-x)^\\top \\nabla^2 f(x)(y-x) \\] <p>Comparing this with the \\(L\\)-smoothness definition, we can see that \\(L\\)-smoothness implies the Hessian \\(\\nabla^2 f(x)\\) cannot be too large. In fact, \\(L\\)-smoothness is equivalent to the following properties:</p> <ul> <li>If \\(f\\) has second derivatives, \\(L\\)-smoothness is equivalent to \\(\\nabla^2f(x) \\preceq L\\mathbf{I}_d\\) for any \\(x\\), which prevents \\(f\\) from being too curved.</li> <li>\\(L\\)-smoothness is also equivalent to \\(\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\leq L\\|x-y\\|_2\\), meaning the gradient of \\(f\\) is Lipschitz continuous and doesn't change too drastically.</li> </ul> <p>For example, the least squares loss \\(f(\\beta) = \\frac{1}{2}\\|Y - X \\beta\\|_2^2\\) has Hessian matrix \\(\\nabla^2 f(x) = X X^\\top\\). This loss is \\(L\\)-smooth if the maximum eigenvalue of \\(X X^\\top\\) is less than \\(L\\).</p> <p>In contrast, the \\(\\ell_1\\)-norm \\(\\|x\\|_1\\) is not smooth because its derivative is not continuous at \\(x=0\\).</p>"},{"location":"chapter_optimization/convexity/#subgradient","title":"Subgradient","text":"<p>In high-dimensional optimization, first-order methods are preferred, which requires understanding gradient properties. However, many convex functions are not smooth and don't have gradients everywhere. For example, the \\(\\ell_1\\)-norm \\(\\|x\\|_1\\) is not differentiable at points where any component equals zero.</p> <p>To handle non-smooth convex functions, we can generalize the concept of gradient using subgradients.</p> <p>Definition (Subgradient): Let \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\). We say \\(g \\in \\mathbb{R}^d\\) is a subgradient of \\(f\\) at \\(x\\) if for any \\(y \\in \\mathcal{X}\\):</p> \\[ f(y) - f(x) \\geq g^\\top(y-x) \\] <p>We denote the set of all subgradients of \\(f\\) at \\(x\\) as \\(\\partial f(x)\\).</p> <p>Geometrically, a subgradient at point \\(x\\) defines a supporting hyperplane to the function's graph. The hyperplane tangent to \\(f\\) at \\(x\\) is \\(p(y) = g^\\top(y-x)+ f(x)\\). The inequality \\(f(y) - f(x) \\geq g^\\top(y-x)\\) means that the function \\(f\\) lies above this hyperplane.</p> <p> </p> <p>Notice that \\(\\partial f(x)\\) is a set, not a single vector. When a function is convex, subgradients always exist. For differentiable convex functions, the gradient is the unique subgradient: \\(\\partial f(x) = \\{\\nabla f(x)\\}\\). However, for non-differentiable points, there may be multiple subgradients.</p>"},{"location":"chapter_optimization/convexity/#example-subgradient-of-ell_1-norm","title":"Example: Subgradient of \\(\\ell_1\\)-norm","text":"<p>The \\(\\ell_1\\)-norm is one of the most widely used non-smooth convex functions. Let's start with the one-dimensional case \\(f(x) = |x|\\):</p> <ul> <li>When \\(x \\neq 0\\), \\(f\\) is differentiable and \\(\\partial |x| = \\{{\\rm sign}(x)\\}\\)</li> <li>When \\(x = 0\\), \\(f\\) is non-differentiable and \\(\\partial |x| = [-1,1]\\) (all values in this interval are valid subgradients)</li> </ul> <p>For the multi-dimensional \\(\\ell_1\\)-norm \\(\\|x\\|_1\\), if \\(g \\in \\partial \\|x\\|_1\\), then: - \\(g_j = {\\rm sign}(x_j)\\) for components where \\(x_j \\neq 0\\) - \\(g_j \\in [-1,1]\\) for components where \\(x_j = 0\\)</p>"},{"location":"chapter_optimization/convexity/#optimality-conditions-in-convex-optimization","title":"Optimality Conditions in Convex Optimization","text":"<p>One important property of convex functions is that local minima are always global minima:</p> <p>Proposition (Local minima are global minima): Let \\(f\\) be convex. If \\(x^*\\) is a local minimum of \\(f\\), then \\(x^*\\) is its global minimum. This happens if and only if \\(0 \\in \\partial f(x^*)\\).</p> <p>This is one reason why convex optimization is generally easier than non-convex optimization.</p> <p>For constrained problems, we have the following first-order optimality condition:</p> <p>Proposition (First-order optimality condition): Given a convex set \\(\\mathcal{X}\\) and a convex differentiable function \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\), \\(x^* \\in \\arg\\min_x f(x)\\) subject to \\(x \\in \\mathcal{X}\\) if and only if:</p> \\[ \\nabla f(x^*)^\\top(y - x^*) \\geq 0, \\text{ for all } y \\in \\mathcal{X} \\] <p>Geometrically, this means that the angle between the gradient \\(\\nabla f(x^*)\\) and any feasible direction \\((y - x^*)\\) is non-acute.</p> <p>The optimality conditions for convex optimization can be summarized as follows:</p> Unconstrained Constrained Zero-order \\(f(x^*) \\leq f(y)\\) \\(f(x^*) \\leq f(y)\\) First-order \\(0 \\in \\partial f(x^*)\\) \\(\\nabla f(x^*)^\\top(y - x^*) \\geq 0\\) <p>These optimality conditions are powerful tools for: 1. Checking whether a given point is optimal 2. Deriving algorithms to find optimal solutions 3. Understanding the geometric properties of optimal solutions </p>"},{"location":"chapter_optimization/duality_and_admm/","title":"Duality and ADMM","text":""},{"location":"chapter_optimization/duality_and_admm/#composite-objective-function","title":"Composite Objective Function","text":"<p>We have introduced proximal gradient descent to solve optimization problems of the form \\(\\min_x f(x) + g(x)\\), where \\(f\\) is smooth but \\(g\\) is not differentiable. A key step in proximal gradient descent is solving the proximal operator:</p> \\[ {\\rm prox}_g(x) = \\arg\\min_{z\\in\\mathbb{R}^d}\\left\\{\\frac{1}{2} \\|z-x\\|_2^2 + g(z)\\right\\} \\] <p>For example, when \\(g(x) = \\lambda \\|x\\|_1\\), the proximal operator is soft-thresholding. For the fused Lasso problem:</p> \\[ \\min_{\\beta} \\|Y - X\\beta\\|_2^2 + \\lambda \\|D\\beta\\|_1 \\] <p>where \\(D\\) is some matrix representing the differential map, applying proximal gradient descent requires solving:</p> \\[ \\arg\\min_{z\\in\\mathbb{R}^d}\\left\\{\\frac{1}{2} \\|z-x\\|_2^2 + \\lambda \\|Dz\\|_1\\right\\} \\] <p>Unlike the \\(\\ell_1\\)-norm, this problem lacks a closed-form solution, making proximal gradient descent inefficient for fused Lasso.</p> <p>We discuss solving composite objective functions. Generally, we are interested in the following composite optimization problem:</p> \\[ \\min_{x,y} f(x) + g(y), \\text{ s.t. } Ax + By = c \\] <p>for some convex \\(f\\) and \\(g\\). The fused Lasso can be reduced to this form by letting \\(f(\\beta) = \\|Y - X\\beta\\|_2^2\\), \\(g(y) = \\lambda \\|y\\|_1\\), so:</p> \\[ \\min_{\\beta} \\|Y - X\\beta\\|_2^2 + \\lambda \\|D\\beta\\|_1 \\Longleftrightarrow \\min_{\\beta,y} f(\\beta) + g(y), \\text{ s.t. } D\\beta - y = 0 \\] <p>Another example is basis pursuit:</p> \\[ \\min_{\\beta} \\|\\beta\\|_1 \\text{ s.t. } X\\beta - Y = 0 \\]"},{"location":"chapter_optimization/duality_and_admm/#duality","title":"Duality","text":"<p>To solve the composite optimization problem, we start by reviewing duality in optimization. We define the Lagrange multiplier function of the problem as:</p> \\[ L(x,y, \\lambda) = f(x) + g(y) + \\lambda^\\top(Ax + By -c) \\] <p>This allows us to convert the primal problem to the dual problem:</p> \\[ \\max_{\\lambda} h(\\lambda), \\text{ where } h(\\lambda) = \\min_{x,y}L(x,y, \\lambda) \\] <p>Duality is crucial in optimization. The following theorem implies we can solve the primal problem via the dual problem.</p> <p>Theorem (Strong Duality): Given convex functions \\(f,g\\), the primal problem has the solution:</p> \\[ (x^*, y^*) = \\arg\\min_{x,y} f(x) + g(y), \\text{ s.t. } Ax + By = c \\] <p>Consider the dual problem \\(\\lambda^* = \\arg\\max_{\\lambda} h(\\lambda)\\), where \\(h(\\lambda) = \\min_{x,y}L(x,y, \\lambda)\\). We can solve \\((x^*, y^*)\\) via:</p> \\[ (x^*, y^*) = \\arg\\min_{x,y} L(x,y, \\lambda^*) \\] <p>As \\(L(x,y, \\lambda^*)\\) is decomposable, we further have:</p> \\[ x^* = \\arg\\min_x f(x) + \\lambda^{*\\top} Ax \\text{ and } y^* = \\arg\\min_y g(y) + \\lambda^{*\\top} By \\] <p>The strong duality property is useful for converting a linear constraint problem to an unconstrained dual problem. The following example shows how to find the dual problem of Lasso.</p>"},{"location":"chapter_optimization/duality_and_admm/#example-duality-of-lasso","title":"Example: Duality of Lasso","text":"<p>As Lasso is an unconstrained problem, we first convert it to the standard form:</p> \\[ \\min_\\beta \\frac{1}{2}\\|Y - X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1 \\iff \\min_{\\beta, z} \\frac{1}{2}\\|z\\|_2^2 + \\lambda\\|\\beta\\|_1, \\text{ s.t. } Y - X\\beta = z \\] <p>The Lagrange function is:</p> \\[ L(\\beta, z, \\mu) = \\frac{1}{2}\\|z\\|^2_2 + \\lambda\\|\\beta\\|_1 + \\mu^\\top (Y - X\\beta -z) \\] <p>Notice that \\(L(\\beta, z, \\mu)\\) is decomposable:</p> \\[ \\min_{\\beta,z}L(\\beta, z, \\mu) = \\min_\\beta \\left\\{\\lambda\\|\\beta\\|_1 - \\mu^\\top X\\beta \\right\\}+ \\min_z \\left\\{\\frac{1}{2}\\|z\\|^2_2 - \\mu^\\top z\\right\\} + \\mu^\\top Y \\] <p>The second problem is quadratic, and we have:</p> \\[ z^* = \\arg\\min_z \\left\\{\\frac{1}{2}\\|z\\|^2_2 - \\mu^\\top z \\right\\} \\implies z^* - \\mu = 0 \\] <p>Applying the first-order optimality condition to the first problem, we get:</p> \\[ \\beta^* = \\arg\\min_\\beta \\left\\{\\lambda\\|\\beta\\|_1 - \\mu^\\top X\\beta \\right\\} \\implies 0 = -X^\\top \\mu + \\lambda g, \\text{ for some } g \\in \\partial \\|\\beta^*\\|_1 \\] <p>As \\(\\|g\\|_\\infty \\le 1\\), the equality \\(0 = -X^\\top \\mu + \\lambda g\\) has a finite solution only if \\(\\lambda \\geq \\|X^\\top \\mu\\|_\\infty\\). Moreover, if \\(\\lambda \\geq \\|X^\\top \\mu\\|_\\infty\\), multiplying \\(\\beta^*\\) on both sides of \\(0 = -X^\\top \\mu + \\lambda g\\) gives:</p> \\[ 0 = -\\beta^{*\\top}X^\\top \\mu + \\lambda \\|\\beta^*\\|_1 \\] <p>Therefore, we have:</p> \\[ \\min_\\beta \\left\\{\\lambda\\|\\beta\\|_1 - \\mu^\\top X\\beta \\right\\} = \\begin{cases} 0, &amp;\\text{if } \\lambda \\geq \\|X^\\top \\mu\\|_\\infty; \\\\ -\\infty, &amp;\\text{if } \\lambda &lt; \\|X^\\top \\mu\\|_\\infty. \\end{cases} \\] <p>Combining the results, the dual problem becomes:</p> \\[ \\max_{\\lambda} \\min_{\\beta,z}L(\\beta, z, \\mu) = \\max_{\\lambda}\\left\\{-\\frac{1}{2}\\|\\mu\\|_2^2 + \\mu^\\top Y\\right\\} \\text{ s.t. } \\lambda \\geq \\|X^\\top \\mu\\|_\\infty \\] <p>Therefore, the dual problem of Lasso is:</p> \\[ \\max_\\mu L(\\beta^*, z^*, \\mu) = \\max_\\mu -\\frac{1}{2}\\|\\mu\\|_2^2 + \\mu^\\top Y \\text{ s.t. } \\|X^\\top \\mu\\|_\\infty \\le \\lambda \\] <p>This is equivalent to:</p> \\[ \\text{**Duality of Lasso**}: \\min_{\\mu} \\|Y - \\mu\\|_2^2, \\text{ s.t. } \\|X^\\top \\mu\\|_\\infty \\le \\lambda \\] <p> </p> <p>By the duality, the dual variable \\(\\mu^* = z^* = Y - X\\beta^*\\) is the residual. This gives a new geometric insight into Lasso, illustrating that Lasso is a projection, similar to OLS.</p>"},{"location":"chapter_optimization/duality_and_admm/#alternating-direction-method-of-multipliers-admm","title":"Alternating Direction Method of Multipliers (ADMM)","text":"<p>Consider an equivalent form of the problem by adding a quadratic term:</p> \\[ \\left\\{\\begin{aligned}&amp;\\min_{x,y} f(x) + g(y)\\\\&amp;\\text{ s.t. } Ax + By = c,\\end{aligned}\\right\\} \\Longleftrightarrow \\left\\{\\begin{aligned}&amp;\\min_{x,y}f(x) + g(y) + \\frac{\\rho}{2}\\|Ax + By -c\\|_2^2\\\\&amp;\\text{ s.t. }Ax + By = c.\\end{aligned}\\right\\} \\] <p>We introduce the augmented Lagrangian:</p> \\[ L_{\\rho} (x,y,\\lambda) = f(x) + g(y) + \\lambda^\\top (Ax + By -c) +\\frac{\\rho}{2}\\|Ax + By -c\\|_2^2 \\] <p>To solve the dual problem \\(\\max_\\lambda \\min_{x,y} L_{\\rho} (x,y,\\lambda)\\), we propose updating the primal and dual variables alternately:</p> <p>Primal step:</p> \\[ \\begin{cases}x_{t+1}= \\arg\\min_x L_{\\rho}(x, y_t, \\lambda_t);\\\\y_{t+1}= \\arg\\min_y L_{\\rho}(x_{t+1}, y, \\lambda_t);\\end{cases} \\] <p>Dual step:</p> \\[ \\lambda_{t+1}= \\arg\\max_\\lambda L_{\\rho}(x_{t+1}, y_{t+1}, \\lambda) -\\frac{1}{2\\rho}\\|\\lambda - \\lambda_t\\|_2^2 \\] <p>We add a proximal term in the dual step because the augmented Lagrangian \\(L_{\\rho}(x_{t+1}, y_{t+1}, \\lambda)\\) is linear with respect to \\(\\lambda\\), making \\(\\max_{\\lambda} L_{\\rho}(x_{t+1}, y_{t+1}, \\lambda) = \\infty\\). The proximal term prevents \\(\\lambda_{t+1}\\) from deviating too much from \\(\\lambda_t\\).</p> <p>Plugging the augmented Lagrangian into the primal and dual steps, we have the following algorithm.</p> <p>Alternating Direction Method of Multipliers (ADMM)</p> \\[ \\begin{align*}x_{t+1} &amp;= \\arg\\min_x f(x) + \\frac{\\rho}{2} \\|Ax + By_t - c + \\lambda_t/\\rho\\|_2^2\\\\y_{t+1} &amp;= \\arg\\min_y g(x) + \\frac{\\rho}{2} \\|Ax_{t+1} + By - c + \\lambda_t/\\rho\\|_2^2\\\\\\lambda_{t+1} &amp;= \\lambda_t + \\rho(Ax_{t+1} + By_{t+1} - c)\\end{align*} \\] <p>If \\(f\\) and \\(g\\) are closed convex functions, the ADMM algorithm converges.</p> <p>ADMM not first-order method</p> <p>ADMM is not a first-order method. The sub-problem in the primal step use the functions \\(f\\) and \\(g\\) instead of just their gradients like the proximal gradient descent. So ADMM may converge faster than first-order methods in terms of the number of iterations, but the computational cost per iteration might be higher.</p>"},{"location":"chapter_optimization/duality_and_admm/#example-fused-lasso","title":"Example: Fused Lasso","text":"<p>Applying ADMM to the fused Lasso:</p> \\[ \\min_{\\beta } \\frac{1}{2}\\|Y - X\\beta\\|_2^2 + \\lambda \\|D\\beta\\|_1 \\iff \\min_{\\beta,z } \\frac{1}{2}\\|Y - X\\beta\\|_2^2 + \\lambda \\|z\\|_1 \\text{ s.t. } D\\beta - z=0 \\] <p>The updating rule is:</p> \\[ \\begin{align*}\\beta_{t+1}&amp;= \\arg\\min_\\beta \\frac{1}{2}\\|Y - X\\beta\\|_2^2 + \\frac{\\rho}{2}\\|D\\beta -z_t + \\lambda_t/\\rho\\|_2^2\\\\z_{t+1}&amp;= \\arg\\min_z \\lambda\\|z\\|_1 + \\frac{\\rho}{2}\\|D\\beta_{t+1} -z + \\lambda_t/\\rho\\|_2^2\\\\\\lambda_{t+1} &amp;= \\lambda_t + \\rho(D\\beta_{t+1} - z_{t+1})\\end{align*} \\] <p>This yields the algorithm:</p> \\[ \\begin{align*}\\beta_{t+1}&amp;= (X^\\top X + \\rho D^\\top D)^{-1} (X^\\top Y + \\rho D^\\top z_{t} - D^\\top\\lambda_{t})\\\\z_{t+1}&amp;= \\text{SoftThreshold}(D\\beta_{t+1}+ \\lambda_t/\\rho, \\lambda/\\rho)\\\\\\lambda_{t+1} &amp;= \\lambda_t + \\rho(D\\beta_{t+1} - z_{t+1})\\end{align*} \\] <p>When \\(D = I\\), the algorithm reduces to ADMM for Lasso. Unlike FISTA, ADMM involves matrix inversion, which may be time-consuming when \\(d\\) is large.</p> <p>Here is the implementation of ADMM for fused Lasso in PyTorch. Notice we can implement the tips in Linear Algebra to pre-compute the Cholesky decomposition of the matrix \\((X^\\top X + \\rho D^\\top D)\\) to make it more efficient.</p> <pre><code>import torch\n\n# Soft-thresholding function\ndef soft_thresholding(y, threshold):\n    return torch.sign(y) * torch.clamp(torch.abs(y) - threshold, min=0)\n\n# Define problem parameters\ntorch.manual_seed(42)\nn, d = 10, 5  # Number of samples (n) and features (d)\nX = torch.randn(n, d)  # Design matrix\nY = torch.randn(n)  # Response vector\n\n# Define difference matrix D (for fused lasso)\nD = torch.eye(d) - torch.eye(d, k=1)  # First-order difference matrix\n\n# Initialize variables\nbeta = torch.zeros(d)  # Coefficients\nz = torch.zeros(d - 1)  # Auxiliary variable\nlam = torch.zeros(d - 1)  # Lagrange multiplier\n\n# ADMM parameters\nlambda_ = 0.1  # Regularization parameter\nrho = 1.0  # ADMM penalty parameter\nnum_iters = 100  # Number of iterations\n\n# Compute Cholesky decomposition of (X^T X + rho D^T D)\nXtX = X.T @ X\nDtD = D.T @ D\nA = XtX + rho * DtD  # Regularized system matrix\n\n# Cholesky decomposition (more stable than direct inversion)\nL = torch.linalg.cholesky(A)  # Compute Cholesky factor L\n\nfor t in range(num_iters):\n    # Solve for beta using Cholesky decomposition\n    b_rhs = X.T @ Y + rho * D.T @ z - D.T @ lam  # Right-hand side\n    y = torch.linalg.solve_triangular(L, b_rhs, upper=False)  # Forward substitution\n    beta = torch.linalg.solve_triangular(L.T, y, upper=True)  # Backward substitution\n\n    # Update z using soft-thresholding\n    z = soft_thresholding(D @ beta + lam / rho, lambda_ / rho)\n\n    # Update lambda (dual variable)\n    lam += rho * (D @ beta - z)\n</code></pre> <p>When \\(d\\) is large, we can further improve the efficiency by using the Woodbury matrix identity to solve the linear system  $$ (X^TX + \u03c1D<sup>TD)</sup> = (\u03c1D<sup>TD)</sup> - (\u03c1D<sup>TD)</sup>X^T(I_n + X(\u03c1D<sup>TD)</sup>X<sup>T)</sup>X(\u03c1D<sup>TD)</sup> $$ The computation will be much more efficient, especially when for the Lasso case \\(D = I\\).</p>"},{"location":"chapter_optimization/duality_and_admm/#example-graphical-lasso","title":"Example: Graphical Lasso","text":"<p>Given i.i.d. samples \\(X_1, \\ldots, X_n \\sim N(0, \\Sigma)\\), we estimate the precision matrix \\(\\Theta = \\Sigma^{-1}\\) via graphical Lasso:</p> \\[ \\begin{align*}&amp;\\min_{\\Theta } -\\log \\det \\Theta + \\text{tr}{(\\Theta^\\top \\hat{\\Sigma})} + \\lambda\\|\\Theta\\|_{1,1} \\\\\\Updownarrow\\\\&amp;\\min_{\\Theta, \\Psi } -\\log \\det \\Theta + \\text{tr}{(\\Theta^\\top \\hat{\\Sigma})} + \\lambda\\|\\Psi\\|_{1,1} \\text{ s.t. } \\Theta = \\Psi\\end{align*} \\] <p>Applying ADMM, we get:</p> \\[ \\begin{align*}\\Theta_{t+1}&amp;= \\arg\\min_\\Theta -\\log \\det \\Theta + \\text{tr}{(\\Theta^\\top \\hat{\\Sigma})} + \\frac{\\rho}{2}\\|\\Theta - \\Psi_t+ \\Lambda_t/{\\rho}\\|_{\\rm F}^2\\\\\\Psi_{t+1}&amp;= \\arg\\min_\\Psi\\lambda\\|\\Psi\\|_{1,1} + \\frac{\\rho}{2}\\|\\Theta_{t+1} - \\Psi+ \\Lambda_t/{\\rho}\\|_{\\rm F}^2\\\\\\Lambda_{t+1}&amp;= \\Lambda_t + \\rho(\\Theta_{t+1} - \\Psi_{t+1} )\\end{align*} \\] <p>where the Frobenius norm \\(\\|A\\|_{\\rm F}^2 = \\sum_{jk}A_{jk}^2\\). This simplifies to:</p> \\[ \\begin{align*}\\Theta_{t+1}&amp;= \\mathcal{F}_{\\rho}(\\Psi_t- \\Lambda_t/{\\rho} - \\widehat \\Sigma/{\\rho} )\\\\\\Psi_{t+1}&amp;= \\text{SoftThreshold}(\\Theta_{t+1}+ \\Lambda_t/{\\rho}, \\lambda/\\rho)\\\\\\Lambda_{t+1}&amp;= \\Lambda_t + \\rho(\\Theta_{t+1} - \\Psi_{t+1} )\\end{align*} \\] <p>where for the spectral decomposition \\(X = UDU^\\top\\), \\(\\mathcal{F}_{\\rho}(X) = U{\\rm diag}\\{\\lambda_i + \\sqrt{\\lambda_i + 4/\\rho}\\}U^\\top\\).</p>"},{"location":"chapter_optimization/duality_and_admm/#example-consensus-optimization","title":"Example: Consensus Optimization","text":"<p>We design a divide-and-conquer ADMM for Lasso for massive data when \\(n\\) is ultra-large. The Lasso:</p> \\[ \\min_{\\beta } \\sum_{i=1}^n (Y_i - X_i\\beta)^2 + \\lambda\\|\\beta\\|_1 \\] <p>can be formulated as the general block separable form:</p> \\[ \\min_x \\sum_{i=1}^n f_i(x) \\iff \\min_{x_i,z}\\sum_{i=1}^n f_i(x) \\text{ s.t. } x_i = z \\text{ for all } i = 1, \\ldots, n \\] <p>Applying ADMM, we get:</p> <p>ADMM for Consensus Optimization</p> \\[ \\begin{align*} \\text{Divide: } x_i^{t+1}&amp;= \\arg\\min_{x_i} f_i(x_i) + \\frac{\\rho}{2}\\|x_i - z^t + \\lambda_i^t/{\\rho}\\|_2^2, \\forall i = 1, \\ldots, n\\\\ \\text{Gather: } z^{t+1} &amp;=\\frac{1}{n}\\sum_{i=1}^n (x_i^{t+1}+ \\lambda_i^t/{\\rho})\\\\ \\text{Broadcast: } \\lambda_i^{t+1} &amp; = \\lambda_i^t + \\rho(x_i^{t+1}- z^{t+1} ), \\forall i = 1, \\ldots, n \\end{align*} \\] <p>In the first step, we update each \\(x_i^{t+1}\\) in parallel, then gather all local iterates, and finally broadcast the updated dual variables back to each core. </p>"},{"location":"chapter_optimization/gradient_descent/","title":"Gradient Descent","text":"<p>We'll explore algorithms for solving convex optimization problems of the form:</p> \\[ \\min_{x \\in \\mathcal{X}} f(x), \\text{ where $f$ and $\\mathcal{X}$ are convex} \\] <p>Our goal is to find the minimizer \\(x^* = \\arg\\min_{x\\in \\mathcal{X}}f(x)\\). Let's start with the unconstrained case where \\(\\mathcal{X} = \\mathbb{R}^d\\).</p> <p>If we begin our search at some point \\(x_0\\), we want to move in a direction that decreases the value of \\(f(x)\\). When moving along direction \\(d\\) with a small step size \\(\\tau\\), the decrease in the objective function is:</p> \\[ \\lim_{\\tau \\rightarrow 0}\\frac{f(x_0+\\tau d) - f(x_0)}{\\tau} = \\nabla f(x_0)^\\top d \\] <p>To maximize this decrease, we should choose \\(d = -\\nabla f(x_0)\\). This means the negative gradient \\(-\\nabla f(x_0)\\) provides the steepest descent direction when we only have local information about \\(f(x)\\) at point \\(x_0\\). This insight leads to the gradient descent algorithm.</p> <p>Gradient Descent Algorithm</p> <p>Starting at point \\(x_0 \\in \\mathbb{R}^d\\), iterate as follows:</p> \\[ x_{t+1} = x_t -\\eta_t\\nabla f(x_t) \\] <p>where \\(\\eta_t\\) is the step size (also called learning rate).</p> <p>Here is the implementation of gradient descent in PyTorch:</p> <pre><code>import torch\ndef f(x): # objective function\n    return x**2\n\n# Initialize x\nx = torch.tensor([2.0], requires_grad=True)\nlr = 0.1 # learning rate\nfor t in range(100):\n    y = f(x)\n    y.backward()\n    with torch.no_grad():\n        x -= lr * x.grad\n    x.grad.zero_()\n</code></pre>"},{"location":"chapter_optimization/gradient_descent/#convergence-of-gradient-descent","title":"Convergence of Gradient Descent","text":"<p>The following theorem shows how quickly gradient descent converges:</p> <p>Theorem (Convergence of gradient descent): Let \\(f\\) be convex and \\(L\\)-smooth. If we choose \\(\\eta_t = 1/L\\), then gradient descent achieves:</p> \\[ f(x_t) - f(x^*) \\leq \\frac{2L\\|x_0 - x^*\\|_2^2}{t} \\] <p>This means gradient descent has a convergence rate of \\(O(1/t)\\), or equivalently, it can achieve \\(\\epsilon\\)-accuracy (\\(f(x_t) - f(x^*) \\leq \\epsilon\\)) within \\(O(1/\\epsilon)\\) steps.</p>"},{"location":"chapter_optimization/gradient_descent/#learning-rate","title":"Learning Rate","text":"<p>Choosing the learning rate \\(\\eta_t\\) is vital for the convergence of optimization algorithms. The theorem above suggests that we can choose \\(\\eta_t = 1/L\\) for all iterations. However, this is not practical because it requires knowing the smoothness parameter \\(L\\) in advance. Also different algorithms may have different strategies for choosing \\(\\eta_t\\).</p> <p>In practice, we have different strategies for choosing \\(\\eta_t\\). The tricky part is to choose \\(\\eta_t\\) to be small enough to ensure convergence, but not too small to slow down the convergence.</p> <p>Dynamic learning rate. One common strategy is make the learning rate decreasing over time. The common choices are:</p> \\[ \\begin{align*} \\eta_t &amp;= \\eta_i, \\text{ for } t_i \\leq t &lt; t_{i+1} &amp; \\text{Piecewise constant}\\\\ \\eta_t &amp;= \\eta_0 e^{-\\lambda t} &amp; \\text{Exponentially decay}\\\\ \\eta_t &amp;= \\eta_0 (1+\\beta t)^{-\\alpha} &amp; \\text{Polynomial decay}\\\\ \\end{align*} \\] <p>A popular choice is the polynomial decay with \\(\\alpha = 0.5\\). We refer to the paper Izmailov et al., 2018 for more sophisticated strategies for choosing \\(\\eta_t\\).</p> <p>Backtracking Line search. Another strategy is to choose \\(\\eta_t\\) by a line search. We aim to find the learning rate \\(\\eta\\) along the direction of \\(p\\) such that \\(f(x+\\eta p)\\) is sufficiently smaller than \\(f(x)\\).</p> <p>One popular choice is the Armijo rule: Given initial step size \\(\\eta_0\\), reduction factor \\(\\beta \\in (0,1)\\), and constant \\(c \\in (0,1)\\):</p> <ol> <li>Set \\(\\eta = \\eta_0\\)</li> <li>While \\(f(x+\\eta p) &gt; f(x) + c\\eta \\nabla f(x)^\\top p\\):     Reduce step size: \\(\\eta = \\beta\\eta\\)</li> <li>Return \\(\\eta\\)</li> </ol> <p>The algorithm starts with a large step size and gradually reduces it until finding an \\(\\eta\\) that gives sufficient decrease. Common choices are \\(\\beta = 0.5\\) and \\(c = 0.1\\).</p> <p>Backtracking line search requires multiple evaluations of \\(f(x+\\eta p)\\) which can be expensive. For large-scale problems like deep learning, it is not practical.</p>"},{"location":"chapter_optimization/gradient_descent/#frank-wolfe-algorithm","title":"Frank-Wolfe Algorithm","text":"<p>Now let's consider the constrained problem \\(\\min_{x \\in \\mathcal{X}} f(x)\\). The standard gradient descent might cause \\(x_t\\) to leave the constraint set \\(\\mathcal{X}\\). As shown in the figure below, we still want to find the steepest descent direction, but we need to ensure that \\(x_t\\) remains in \\(\\mathcal{X}\\) for all iterations.</p> <p></p> <p>Starting with \\(x_0 \\in \\mathcal{X}\\), we want to find the steepest descent direction \\(d = x - x_0\\) with \\(x \\in \\mathcal{X}\\) by solving:</p> \\[ \\arg\\min_{x \\in \\mathcal{X}}\\langle\\nabla f(x_0), x - x_0\\rangle = \\arg\\min_{x \\in \\mathcal{X}} \\langle\\nabla f(x_0), x \\rangle \\] <p>This leads to the Frank-Wolfe algorithm:</p> <p>Frank-Wolfe Algorithm</p> \\[ \\begin{align*} y_t &amp;= \\arg\\min_{x \\in \\mathcal{X}} \\langle\\nabla f(x_t), x \\rangle\\\\ x_{t+1} &amp;= x_t + \\eta_t (y_t - x_t) \\end{align*} \\] <p>where \\(\\eta_t\\) is the step size.</p> <p>We will use the Frank-Wolfe algorithm when the first sub-optimization problem is easy to solve. We have the following two examples.</p>"},{"location":"chapter_optimization/gradient_descent/#example-power-iteration","title":"Example: Power Iteration","text":"<p>The leading eigenvector of a positive semi-definite matrix \\(A\\) is the solution to:</p> \\[ \\max_{\\|x\\|_2 \\leq 1} x^\\top A x = \\min_{\\|x\\|_2 \\leq 1} -x^\\top A x \\] <p>Although \\(f(x) = -x^\\top A x\\) is concave (not convex) when \\(A \\succeq 0\\), we can still apply the Frank-Wolfe algorithm. We need to solve:</p> \\[ y_t = \\arg\\min_{\\|x\\|_2 \\leq 1} \\langle\\nabla f(x_t), x \\rangle = -\\frac{\\nabla f(x_t)}{\\|\\nabla f(x_t)\\|_2} = \\frac{Ax_t}{\\|Ax_t\\|_2} \\] <p>where we use the fact that \\(\\arg\\max_{\\|x\\|_2 \\leq 1} \\langle y, x \\rangle = y/\\|y\\|_2\\), which can be shown by the plot below.</p> <p></p> <p>This gives us the update rule:</p> \\[ x_{t+1} = x_t + \\eta_t\\left(\\frac{Ax_t}{\\|Ax_t\\|_2} - x_t\\right) = (1-\\eta_t)x_t + \\eta_t\\frac{Ax_t}{\\|Ax_t\\|_2} \\] <p>Choosing the optimal step size \\(\\eta_t = 1\\) yields \\(x_{t+1} = \\frac{Ax_t}{\\|Ax_t\\|_2}\\), which is the power iteration method for finding the leading eigenvector of \\(A\\).</p>"},{"location":"chapter_optimization/gradient_descent/#example-constrained-lasso","title":"Example: Constrained Lasso","text":"<p>The following two constrained Lasso formulations are equivalent:</p> \\[ \\min_{\\beta} \\|Y - X\\beta\\|_2^2 \\text{ s.t. } \\|\\beta\\|_1 \\leq \\lambda \\Longleftrightarrow \\min_{\\beta} \\|Y/\\lambda - X\\beta\\|_2^2 \\text{ s.t. } \\|\\beta\\|_1 \\leq 1 \\] <p>So without loss of generality, we can assume \\(\\lambda = 1\\). Therefore, we aim to solve a more general constrained least squares problem:</p> \\[ \\min_{\\beta} f(\\beta) \\text{ s.t. } \\|\\beta\\|_1 \\leq 1, \\] <p>where \\(f\\) is any convex function. Applying the Frank-Wolfe algorithm, we need to solve:</p> \\[ y_t = \\arg\\min_{\\|\\beta\\|_1 \\leq 1} \\langle\\nabla f(\\beta_t), \\beta \\rangle \\] <p>where \\(\\nabla f(\\beta_t) = -2X^\\top(Y - X\\beta_t)\\).</p> <p>Using the variational form of the \\(\\ell_1\\)-norm, we know that:</p> \\[ \\arg\\max_{\\|y\\|_1 \\leq 1} \\langle y,x \\rangle = (0,\\dots, 0,\\text{sign}(x_{j^*}),0, \\dots, 0)^\\top \\] <p>where \\(j^* = \\arg\\max_{1 \\leq j \\leq d}|x_j|\\).</p> <p>The visualization of the variational form of the \\(\\ell_1\\)-norm is shown below.</p> <p></p> <p>This gives us:</p> \\[ \\begin{align*} (y_{t})_j &amp;=  \\begin{cases} -\\text{sign}(\\nabla_j f(\\beta_t)), &amp; \\text{if } j = \\arg\\max_k |\\nabla_k f(\\beta_t)|\\\\ 0, &amp; \\text{otherwise} \\end{cases}\\\\ \\beta_{t+1} &amp;= (1-\\eta_t)\\beta_t + \\eta_ty_t \\end{align*} \\] <p>Since only one entry of \\(y_t\\) is nonzero, each iteration adds at most one nonzero entry to \\(\\beta_{t+1}\\). Starting with \\(\\beta_0 = 0\\), we have \\(\\|\\beta_t\\|_0 \\leq t\\), making the algorithm efficient and providing insight into why the \\(\\ell_1\\) constraint promotes sparsity.</p> <p>Here is the implementation of the Frank-Wolfe algorithm for the constrained Lasso problem:</p> <pre><code>import torch\n# Define the function f(x) = ||x||^2 (sum of squares)\ndef f(x):\n    return torch.sum(x**2)\n\n# Initialize multi-dimensional x\nx = torch.tensor([2.0, -3.0, 1.5], requires_grad=True)  # Example 3D vector\n# Learning rate\nlr = 0.1\nfor t in range(100):\n    # Compute function value\n    y = f(x)\n    # Compute gradient\n    y.backward()\n    with torch.no_grad():\n        # Compute the coordinate with the largest absolute gradient\n        max_idx = torch.argmax(torch.abs(x.grad))  \n\n        # Construct y_t based on Frank-Wolfe rule\n        y_t = torch.zeros_like(x)\n        y_t[max_idx] = -torch.sign(x.grad[max_idx])  # Only update the max index\n\n        # Update using convex combination step\n        x_new = (1 - lr) * x + lr * y_t\n        x.copy_(x_new)\n        x.grad.zero_()  \n</code></pre>"},{"location":"chapter_optimization/gradient_descent/#convergence-of-frank-wolfe","title":"Convergence of Frank-Wolfe","text":"<p>Theorem (Convergence rate of Frank-Wolfe algorithm): Let \\(f\\) be convex and \\(L\\)-smooth. If we choose \\(\\eta_t = \\frac{2}{t+2}\\), then the Frank-Wolfe algorithm achieves:</p> \\[ f(x_t) - f(x^*) \\leq \\frac{2Ld_\\mathcal{X}^2}{t+2} \\] <p>where \\(d_\\mathcal{X}^2 = \\sup_{x,y \\in \\mathcal{X}}\\|x-y\\|_2^2\\).</p> <p>Like gradient descent, Frank-Wolfe has a convergence rate of \\(O(1/t)\\) for convex and smooth functions, requiring \\(O(1/\\epsilon)\\) steps to achieve \\(\\epsilon\\)-accuracy. However, unlike gradient descent's constant step size, Frank-Wolfe uses a diminishing step size that doesn't depend on the smoothness parameter \\(L\\).</p>"},{"location":"chapter_optimization/mirror_descent/","title":"Mirror Descent","text":""},{"location":"chapter_optimization/mirror_descent/#bregman-divergence","title":"Bregman Divergence","text":"<p>In the previous lecture, we introduced the proximal perspective of gradient descent. To minimize \\(f(x)\\), we approximate the objective function \\(f(x)\\) around \\(x=x_t\\) using a quadratic function:</p> \\[ f(x) \\approx f(x_t)+\\langle \\nabla f(x_t), x-x_t\\rangle + \\frac{1}{2\\eta_t}\\|x-x_t\\|_2^2 \\] <p>This is composed of the first-order Taylor expansion and a proximal term. For constrained optimization \\(\\min_{x\\in \\mathcal{X}}f(x)\\), starting at \\(x_t\\), we update the next step by minimizing this quadratic approximation:</p> \\[ x_{t+1} = \\arg\\min_{x\\in\\mathcal{X}}\\left\\{ f(x_t)+\\langle \\nabla f(x_t), x-x_t\\rangle + \\frac{1}{2\\eta_t}\\|x-x_t\\|_2^2 \\right\\} \\] <p>If there are no constraints, i.e., \\(\\mathcal{X} = \\mathbb{R}^d\\), this step simplifies to \\(x_{t+1} = x_t -\\eta_t\\nabla f(x_t)\\). Otherwise, it becomes projected gradient descent. Without the proximal term, it reduces to the Frank-Wolfe algorithm. The proximal term \\(\\frac{1}{2\\eta_t}\\|x-x_t\\|_2^2\\) prevents \\(x_{t+1}\\) from straying too far from \\(x_t\\). A natural question arises: why use the \\(\\ell_2\\)-norm in the proximal term? Can we use another distance?</p>"},{"location":"chapter_optimization/mirror_descent/#example-quadratic-optimization","title":"Example: Quadratic Optimization","text":"<p>Consider the quadratic optimization problem:</p> \\[ \\min_{x\\in \\mathbb{R}^d}f(x) = \\min_{x\\in \\mathbb{R}^d} \\frac{1}{2}(x - x^*)^\\top Q (x - x^*) \\] <p>where \\(Q\\) is a positive definite matrix.</p> <p>Using the \\(\\ell_2\\)-norm in the proximal term, we have gradient descent:</p> \\[ x_{t+1} = x_t - \\eta_t Q(x_t-x^*) \\] <p> In figure above, the trajectory of gradient descent is zigzag. This zigzag pattern occurs because the \\(\\ell_2\\)-norm is not the ideal distance for the objective \\(f(x)\\). The contour is scaled by matrix \\(Q\\). What if we use the norm \\(\\|x\\|_Q^2 = x^\\top Q x\\) in the proximal term? Then we update \\(x_{t+1}\\) as:</p> \\[ x_{t+1} = x_t - \\eta_t Q^{-1}\\nabla f(x_t) = x_t - \\eta_t (x_t-x^*) \\] <p> In figure above, the descent direction directly points to the minimizer \\(x^*\\), resulting in a much faster algorithm.</p>"},{"location":"chapter_optimization/mirror_descent/#bregman-divergence_1","title":"Bregman Divergence","text":"<p>The previous example shows the need for a better distance fitting the problem's geometry. This motivates the definition of Bregman divergence.</p> <p>Definition (Bregman Divergence): Let \\(\\varphi: \\mathcal{X} \\rightarrow \\mathbb{R}\\) be a convex and differentiable function. The Bregman divergence between \\(x\\) and \\(z\\) is:</p> \\[ D_{\\varphi}(x,z) = \\varphi(x) - \\varphi(z) - \\langle \\nabla\\varphi(z), x - z\\rangle \\] <p>By convexity, \\(D_{\\varphi}(x,z) \\ge 0\\), making it a type of distance. The Bregman divergence generalizes the quadratic norm \\(\\|\\cdot\\|_Q\\).</p> <p>The table below shows some common Bregman divergences.</p> Function Name \\(\\phi(x)\\) \\(\\text{dom } \\phi\\) \\(D_{\\phi}(x, y)\\) Squared norm \\(\\frac{1}{2} x^2\\) \\((-\\infty, +\\infty)\\) \\(\\frac{1}{2} (x - y)^2\\) Shannon entropy \\(x \\log x - x\\) \\([0, +\\infty)\\) \\(x \\log \\frac{x}{y} - x + y\\) Bit entropy \\(x \\log x + (1 - x) \\log(1 - x)\\) \\([0, 1]\\) \\(x \\log \\frac{x}{y} + (1 - x) \\log \\frac{1 - x}{1 - y}\\) Burg entropy \\(-\\log x\\) \\((0, +\\infty)\\) \\(\\frac{x}{y} - \\log \\frac{x}{y} - 1\\) Hellinger \\(-\\sqrt{1 - x^2}\\) \\([-1, 1]\\) \\((1 - xy)(1 - y^2)^{-1/2} - (1 - x^2)^{1/2}\\) Exponential \\(\\exp x\\) \\((-\\infty, +\\infty)\\) \\(\\exp x - (x - y + 1) \\exp y\\) Inverse \\(1/x\\) \\((0, +\\infty)\\) \\(\\frac{1}{x} + \\frac{x}{y^2} - \\frac{2}{y}\\)"},{"location":"chapter_optimization/mirror_descent/#mirror-descent_1","title":"Mirror Descent","text":"<p>Replacing the \\(\\ell_2\\)-norm in the proximal term with the Bregman divergence, we have:</p> \\[ x_{t+1} = \\arg\\min_{x\\in\\mathcal{X}}\\left\\{ \\langle \\nabla f(x_t), x\\rangle + \\frac{1}{\\eta_t}D_{\\varphi}(x,x_t) \\right\\} \\] <p>This leads to the mirror descent algorithm:</p> <p>Mirror Descent Algorithm:</p> \\[ x_{t+1} = \\arg\\min_{x\\in\\mathcal{X}}\\left\\{ \\langle \\nabla f(x_t), x\\rangle + \\frac{1}{\\eta_t}D_{\\varphi}(x,x_t) \\right\\} \\] <p>This example shows the importance of considering Bregman divergence due to the objective function's geometry. However, in most cases, we need a proper Bregman divergence due to the constraint's geometry. The following example illustrates choosing the right Bregman divergence under a specific constraint.</p>"},{"location":"chapter_optimization/mirror_descent/#example-probability-simplex","title":"Example: Probability Simplex","text":"<p>In many statistical problems, we estimate the probability mass function of a discrete distribution. The parameters are constrained to the probability simplex:</p> \\[ \\Delta = \\left\\{x \\in \\mathbb{R}^d ~\\Big|~ \\sum_{i=1}^d x_i = 1, x_i \\ge 0 \\text{ for all } i = 1, \\ldots, d\\right\\} \\] <p></p> <p>A widely used Bregman divergence for the probability simplex uses \\(\\varphi\\) as the negative entropy:</p> \\[ \\varphi(x) = \\sum_{i=1}^d x_i\\log x_i \\] <p>The corresponding Bregman divergence becomes the Kullback\u2013Leibler (KL) divergence:</p> \\[ D_{\\varphi}(x,z) = \\sum_{i=1}^d x_i\\log \\frac{x_i}{z_i} \\] <p></p> <p>This is also known as \\(D_{\\rm KL}(x\\|z)\\). The maximum log-likelihood estimator is essentially finding a distribution \\(P_{\\theta}\\) closest under the KL-divergence to the true distribution \\(P_{\\theta^*}\\).</p> <p>Therefore, for the constrained optimization problem </p> \\[ \\min_{x\\in \\Delta} f(x) \\] <p>the mirror descent algorithm with the KL-divergence is:</p> \\[ x_{t+1} = \\arg\\min_{x\\in\\Delta}\\left\\{ \\langle \\nabla f(x_t), x\\rangle + \\frac{1}{\\eta_t}D_{\\rm KL}(x\\|x_t) \\right\\} \\] <p>And it has a closed-form solution:</p> <p>Mirror Descent for Probability Simplex</p> \\[ x_{t+1}  = \\text{softmax}\\left(\\frac{1}{\\eta_t}\\nabla f(x_t)\\right) = \\frac{\\exp\\left(\\frac{1}{\\eta_t}\\nabla f(x_t)\\right)}{\\sum_{i=1}^d \\exp\\left(\\frac{1}{\\eta_t}\\nabla f(x_t)_i\\right)} \\] <p>The algorithm can be implemented as:</p> <pre><code>def f(x): # define the objective function\n    ...\n\nx = torch.ones(dim, requires_grad=True) / dim  # Initialize x in the probability simplex\n\n# Mirror Descent Parameters\nlr = 0.1  # Learning rate (\u03b7_t)\nnum_iters = 100  # Number of iterations\nfor t in range(num_iters):\n    # Compute function value at x_t\n    loss = f(x)\n    # Compute gradient using autograd\n    loss.backward()\n    with torch.no_grad():\n        # Compute softmax mirror descent update\n        x_new = torch.softmax((1 / lr) * x.grad, dim=0)\n        # Update variables\n        x.copy_(x_new)  # In-place update to maintain tracking\n        # Zero out gradients for the next iteration\n        x.grad.zero_()\n</code></pre>"},{"location":"chapter_optimization/mirror_descent/#summary-for-constrained-optimization","title":"Summary for Constrained Optimization","text":"<p>Now consider the constrained optimization problem \\(\\min_{x\\in \\Delta} f(x)\\). We have learned three algorithms for solving constrained optimization:</p> <ol> <li>Frank-Wolfe Algorithm: Essentially mirror descent with \\(\\varphi = 0\\). It involves solving a linear programming sub-problem.</li> <li>Projected Gradient Descent: Uses the \\(\\ell_2\\)-norm as the proximal term, involving a quadratic programming sub-problem.</li> <li>Mirror Descent: Using KL-divergence \\(D_{\\rm KL}(x\\|x_t)\\), the sub-problem has a closed-form solution.</li> </ol>"},{"location":"chapter_optimization/nesterov_smooth/","title":"Nesterov's Smoothing","text":"<p>We introduced the proximal algorithm to solve problems of the form:</p> \\[ \\min_x f(x) + h(x) \\] <p>When \\(f\\) is smooth, the accelerated proximal gradient descent has a convergence rate of \\(O(1/t^2)\\). What if the objective is not smooth? For example, the square-root Lasso:</p> \\[ \\min_\\beta \\|Y - \\mathbb X\\beta\\|_2 + \\lambda\\|\\beta\\|_1 \\] <p>The \\(\\ell_2\\)-norm \\(\\|x\\|_2\\) is not differentiable at \\(0\\). Nesterov's smoothing idea involves approximating the non-smooth objective function with a smooth one and minimizing the smooth approximation using gradient descent.</p> <p>Definition: A convex function \\(f\\) is \\((\\alpha, \\beta)\\)-smoothable if for any \\(\\mu &gt; 0\\), there exists a convex approximation \\(f_{\\mu}\\) such that:</p> <ol> <li>\\(f_{\\mu}(x) \\leq f(x) \\leq f_{\\mu}(x) + \\beta\\mu\\), for all \\(x\\).</li> <li>\\(f_{\\mu}\\) is \\(\\frac{\\alpha}{\\mu}\\)-smooth.</li> </ol>"},{"location":"chapter_optimization/nesterov_smooth/#example-ell_1-norm","title":"Example: \\(\\ell_1\\)-norm","text":"<p>Approximate the absolute value \\(f(z) = |z|\\) using the Huber loss:</p> \\[ h_{\\mu}(z) = \\begin{cases} z^2/(2\\mu), &amp;\\text{if } |z|\\leq \\mu; \\\\ |z| - \\mu/2, &amp;\\text{otherwise}. \\end{cases} \\] <p>The Huber loss is \\(\\frac{1}{\\mu}\\)-smooth, making \\(|z|\\) \\((1, \\frac{1}{2})\\)-smoothable. For the \\(\\ell_1\\)-norm \\(f(z) = \\|z\\|_1\\), we approximate it by \\(\\sum_{i=1}^d h_{\\mu}(z_i)\\), making it \\((1, \\frac{d}{2})\\)-smoothable.</p>"},{"location":"chapter_optimization/nesterov_smooth/#example-ell_2-norm","title":"Example: \\(\\ell_2\\)-norm","text":"<p>Approximate the \\(\\ell_2\\)-norm \\(f(x) = \\|x\\|_2\\) by:</p> \\[ f_{\\mu}(x) = \\sqrt{\\|x\\|_2^2 + \\mu^2} - \\mu \\] <p>This makes \\(\\|x\\|_2\\) \\((1,1)\\)-smoothable, with dimension-free smoothing parameters.</p> <p>The following theorem shows the convergence rate using Nesterov's smoothing idea:</p> <p>Theorem: Given \\(F(x) = f(x) + h(x)\\), where \\(f\\) is \\((\\alpha, \\beta)\\)-smoothable and \\(h\\) is convex, let \\(f_\\mu\\) be the \\(\\frac{1}{\\mu}\\)-smooth approximation to \\(f\\). Applying accelerated proximal gradient descent to \\(F_\\mu(x) = f_{\\mu}(x) + h(x)\\) with \\(\\mu = \\epsilon/(2\\beta)\\) achieves \\(\\epsilon\\)-accuracy if \\(t \\gtrsim \\frac{\\sqrt{\\alpha\\beta}}{\\epsilon}\\).</p> <p>For the square-root Lasso, applying accelerated proximal gradient descent to:</p> \\[ \\min_{\\beta} \\sqrt{\\|Y  - \\mathbb X \\beta\\|_2^2 + \\mu^2} + \\lambda\\|\\beta\\|_1 \\] <p>achieves \\(\\epsilon\\)-accuracy within \\(O(1/\\epsilon)\\) steps, assuming the maximum singular value of the design matrix \\(\\mathbb{X}\\) is bounded. </p>"},{"location":"chapter_optimization/proximal_gradient_descent/","title":"Proximal Gradient Descent","text":""},{"location":"chapter_optimization/proximal_gradient_descent/#proximal-perspective","title":"Proximal Perspective","text":"<p>In the previous lecture, we introduced gradient descent and accelerated gradient algorithms for solving unconstrained optimization problems. We showed the convergence rates of these algorithms when the objective function is smooth. However, in problems like Lasso:</p> \\[ \\min_{\\beta} \\frac{1}{2}\\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\] <p>the \\(\\ell_1\\)-norm penalty term is not smooth. Many high-dimensional \\(M\\)-estimators can be formulated as:</p> \\[ \\min_{x \\in \\mathbb{R}^d} F(x) = \\min_{x\\in\\mathbb{R}^d} f(x) + h(x) \\] <p>where \\(f(x)\\) is the loss function, typically convex and smooth, and \\(h(x)\\) is the penalty term, convex but usually non-differentiable. Directly applying (sub)-gradient descent to these problems deteriorates the convergence rate due to the non-smooth part.</p> <p>We focus on algorithms for solving this type of composite loss, aiming for convergence rates similar to gradient descent for smooth functions. Before introducing the new algorithm, let's gain insight into gradient descent:</p> \\[ x_{t+1} = x_t - \\eta_t \\nabla f(x_t) \\] <p>Previously, we motivated gradient descent by showing \\(- \\nabla f(x_t)\\) as the steepest descent direction. An alternative perspective is approximating \\(f(x)\\) around \\(x=x_t\\) with a quadratic function:</p> \\[ f(x) \\approx f(x_t)+\\langle \\nabla f(x_t), x-x_t\\rangle + \\frac{1}{2\\eta_t}\\|x-x_t\\|_2^2 \\] <p>Instead of minimizing \\(f(x)\\), we minimize its quadratic approximation:</p> \\[ x_{t+1} = \\arg\\min_{x\\in\\mathbb{R}^d}\\left\\{ f(x_t)+\\langle \\nabla f(x_t), x-x_t\\rangle + \\frac{1}{2\\eta_t}\\|x-x_t\\|_2^2 \\right\\} \\] <p></p> <p>This problem has a closed-form solution, which is exactly gradient descent:</p> \\[ x_{t+1} = x_t - \\eta_t \\nabla f(x_t) \\] <p>From the proximal perspective, gradient descent minimizes a local quadratic approximation of the objective function in each iteration.</p> <p>Now, let's return to the composite loss \\(F(x) = f(x)+h(x)\\). We modify the proximal perspective of gradient descent as:</p> \\[ x_{t+1} = \\arg\\min_{x\\in\\mathbb{R}^d}\\left\\{ f(x_t)+\\langle \\nabla f(x_t), x-x_t\\rangle + \\frac{1}{2\\eta_t}\\|x-x_t\\|_2^2 + h(x) \\right\\} \\] <p>This leads to the following algorithm for solving \\(\\min_{x\\in\\mathbb{R}^d} f(x)+h(x)\\).</p> <p>Proximal Gradient Descent:</p> <p>Define the proximal operator as:</p> \\[ {\\rm prox}_h(x) = \\arg\\min_{z\\in\\mathbb{R}^d}\\left\\{\\frac{1}{2} \\|z-x\\|_2^2 + h(z)\\right\\} \\] <p>The proximal gradient descent can be written as:</p> <p>Proximal Gradient Descent</p> \\[ x_{t+1} = {\\rm prox}_{\\eta_t h}\\big(x_t - \\eta_t\\nabla f(x_t)\\big) \\]"},{"location":"chapter_optimization/proximal_gradient_descent/#examples-of-proximal-gradient-descent","title":"Examples of Proximal Gradient Descent","text":""},{"location":"chapter_optimization/proximal_gradient_descent/#example-constrained-optimization","title":"Example: Constrained Optimization","text":"<p>Although proximal gradient descent is designed for unconstrained problems, we can reformulate constrained optimization \\(\\min_{x \\in \\mathcal{X}} f(x)\\) as the unconstrained composite form \\(\\min_{x\\in\\mathbb{R}^d} f(x)+h(x)\\), where the indicator function is:</p> \\[ h(x) = \\begin{cases} 0, &amp;\\text{if } x\\in \\mathcal{X}; \\\\ \\infty, &amp;\\text{if } x\\notin \\mathcal{X}. \\end{cases} \\] <p>Solving the proximal operator:</p> \\[ {\\rm prox}_h(x) = \\arg\\min_{z\\in\\mathbb{R}^d}\\frac{1}{2}\\|z-x\\|_2^2 + h(z) = \\arg\\min_{z\\in \\mathcal{X}}\\|z-x\\|_2^2 \\] <p>This projects \\(x\\) onto the constraint \\(\\mathcal{X}\\). Thus, proximal gradient descent gives us the projected gradient descent algorithm for constrained optimization.</p> <p>Projected Gradient Descent:</p> \\[ \\begin{align*} y_t &amp;= x_t - \\eta_t\\nabla f(x_t); \\\\ x_{t+1} &amp;= \\arg\\min_{x\\in \\mathcal{X}}\\|y_t - x\\|_2^2 \\end{align*} \\] <p> </p> <p>The above two figures show the comparison between the projected gradient descent and the Frank-Wolfe algorithm.</p>"},{"location":"chapter_optimization/proximal_gradient_descent/#example-lasso","title":"Example: Lasso","text":"<p>The \\(\\ell_1\\)-penalized optimization has the objective function \\(\\min_x f(x)+ \\lambda \\|x\\|_1\\). The proximal operator becomes the soft-threshold operator:</p> \\[ [{\\rm prox}_{h}(x)]_j = [\\text{Soft-Threshold}(x,\\lambda)]_j = \\begin{cases} x_j-\\lambda, &amp;\\text{if } x_j \\ge \\lambda; \\\\ x_j+\\lambda, &amp;\\text{if } x_j \\le -\\lambda; \\\\ 0, &amp;\\text{otherwise}. \\end{cases} \\] <p>for all \\(j = 1, \\ldots, d\\).</p> <p> </p> <p>The above two figures show the soft-thresholding operator.</p> <p>Solves the Lasso problem \\(\\min_{x \\in \\mathbb{R}^d} f(x)+\\lambda \\|x\\|_1\\) as:</p> <p>Iterative Shrinkage-Thresholding Algorithm (ISTA)</p> \\[ \\begin{align*} y_{t} &amp;= x_t-\\eta_t\\nabla f(x_t); \\\\ x_{t+1} &amp;= \\text{Soft-Threshold}(y_t, \\lambda\\eta_t) \\end{align*} \\] <p>Here is the implementation of ISTA by PyTorch:</p> <pre><code>import torch\n\n# Define function f(x) = 0.5 * ||Ax - b||^2 (L2 loss)\ndim = 5\nA = torch.randn(dim, dim)  # Random matrix A\nb = torch.randn(dim)  # Random vector b\ndef f(x):\n    return 0.5 * torch.norm(A @ x - b)**2  # Quadratic loss function\n\n# Soft-thresholding function\ndef soft_thresholding(y, threshold):\n    return torch.sign(y) * torch.clamp(torch.abs(y) - threshold, min=0)\n\nx = torch.zeros(dim, requires_grad=True)  # Initialize x\n\n# ISTA Parameters\nlr = 0.1  # Learning rate (eta_t)\nlambda_ = 0.1  # Regularization parameter\nnum_iters = 100  # Number of iterations\n\nfor t in range(num_iters):\n    loss = f(x)\n    loss.backward()\n    with torch.no_grad():\n        # Gradient descent step\n        y_t = x - lr * x.grad\n        # Soft-thresholding step\n        x_new = soft_thresholding(y_t, lambda_ * lr) \n        # Update variables\n        x.copy_(x_new)  # In-place update to maintain tracking\n        # Zero out gradients for the next iteration\n        x.grad.zero_()\n</code></pre> <p>Even if the objective function \\(F(x) = f(x) + h(x)\\) has a non-smooth \\(h(x)\\), the following theorem shows that proximal gradient descent has the same convergence rate \\(O(1/t)\\) as gradient descent.</p> <p>Theorem (Convergence rate of proximal gradient descent): Suppose \\(f\\) is convex and \\(L\\)-smooth and \\(h\\) is convex. If \\(\\eta_t = 1/{L}\\), the proximal gradient descent achieves:</p> \\[ F(x_t) - F(x^*)\\leq \\frac{L\\|x_0 - x^*\\|_2^2}{2t} \\]"},{"location":"chapter_optimization/proximal_gradient_descent/#accelerated-proximal-gradient-descent","title":"Accelerated Proximal Gradient Descent","text":"<p>Theorem shows that proximal gradient descent has a convergence rate \\(O(1/t)\\), similar to gradient descent. In the previous lecture, we introduced Nesterov's accelerated gradient descent, which converges faster with a rate \\(O(1/t^2)\\).</p> <p>We can apply Nesterov's idea to proximal gradient descent, resulting in the following algorithm.</p> <p>Accelerated Proximal Gradient Descent</p> <p>Initialize \\(x_0 = y_0\\):</p> \\[ \\begin{align*} x_{t+1} &amp;= {\\rm prox}_{\\eta_t h}\\big(y_t - \\eta_t \\nabla f(y_t)\\big); \\\\ y_{t+1} &amp;= x_{t+1} + \\frac{\\lambda_{t}-1}{\\lambda_{t+1}}(x_{t+1}- x_t) \\end{align*} \\] <p>where \\(\\lambda_0 = 1, \\lambda_t = \\frac{1 + \\sqrt{1+4\\lambda_{t-1}^2}}{2}\\).</p> <p>For example, we can accelerate ISTA for Lasso \\(\\min_{x} f(x)+ \\lambda \\|x\\|_1\\) using the following algorithm.</p> <p>Solves the Lasso problem \\(\\min_{x \\in \\mathbb{R}^d} f(x)+\\lambda \\|x\\|_1\\) as:</p> <p>Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)</p> \\[ \\begin{align*} x_{t+1} &amp;= \\text{Soft-Threshold}(y_t -\\eta_t\\nabla f(y_t), \\lambda\\eta_t); \\\\ y_{t+1} &amp;= x_{t+1} + \\frac{\\lambda_{t}-1}{\\lambda_{t+1}}(x_{t+1}- x_t) \\end{align*} \\] <p>where \\(\\lambda_0 = 1, \\lambda_t = \\frac{1 + \\sqrt{1+4\\lambda_{t-1}^2}}{2}\\) and \\(x_0=y_0\\).</p> <p>Here is the implementation of FISTA:</p> <pre><code># FISTA parameters\nlr = 0.1  # Learning rate \nlambda_ = 0.1 # l1 regularization parameter\n# Momentum term\nlambda_t = 1.0\n\nfor t in range(100):\n    # Compute function value at y_t\n    loss = f(y)\n    # Compute gradient using autograd\n    loss.backward()\n    with torch.no_grad():\n        # Gradient descent step + soft-thresholding\n        x_new = soft_thresholding(y - lr * y.grad, lambda_ * lr)\n        # Update momentum parameter\n        lambda_new = (1 + torch.sqrt(1 + 4 * lambda_t ** 2)) / 2\n        # Compute y_{t+1} using acceleration\n        y = x_new + ((lambda_t - 1) / lambda_new) * (x_new - x)\n        # Update variables for the next iteration\n        x.copy_(x_new)  # In-place update to keep autograd tracking\n        lambda_t = lambda_new\n        y.grad.zero_()\n</code></pre> <p>The convergence rate of accelerated proximal gradient descent is \\(O(1/t^2)\\). </p>"},{"location":"chapter_optimization/pytorch_basics/","title":"Introduction to PyTorch","text":"<p>PyTorch is a popular deep learning library that provides tensor computation with GPU acceleration and automatic differentiation capabilities.</p>"},{"location":"chapter_optimization/pytorch_basics/#basic-operations","title":"Basic Operations","text":"<p>Create and manipulate:</p> <pre><code>import torch\nimport numpy as np\nimport pandas as pd\n\n# Creating a basic PyTorch tensor\ntensor = torch.tensor([1, 2, 3, 4, 5])\n\n# Tensor Operations\nsquared_tensor = tensor ** 2     # Element-wise squaring\nsum_value = tensor.sum()         # Calculate sum\n\n# Creating tensors with specific properties\nzeros = torch.zeros(3, 4)                 # Tensor of zeros\nones = torch.ones(2, 3)                   # Tensor of ones\nrandom_tensor = torch.rand(2, 3)          # Random values between 0 and 1\nrange_tensor = torch.arange(0, 10, 2)     # Range with step size\n</code></pre> <p>Reshaping and manipulating:</p> <pre><code># Reshaping\noriginal = torch.arange(6)\nreshaped = original.reshape(2, 3)\nflattened = reshaped.flatten()\nreshaped.shape      # torch.Size([2, 3])\nflattened.shape     # torch.Size([6])\n\n# Concatenation\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([4, 5, 6])\nconcat_result = torch.cat([tensor1, tensor2])\nstacked_result = torch.stack([tensor1, tensor2])  # New dimension\n</code></pre> <p>Indexing and masking:</p> <pre><code># Indexing\nvalues = torch.tensor([1, 2, 3, 4, 5, 6])\nsubset = values[2:5]\n\n# Boolean masking\nmask = values &gt; 3\nfiltered = values[mask]  # Returns tensor([4, 5, 6])\n\n# Finding indices matching condition\nindices = torch.where(values % 2 == 0)\n</code></pre> <p>Converting Between NumPy, Pandas, and PyTorch:</p> <pre><code># NumPy array to PyTorch tensor\nnp_array = np.array([1, 2, 3])\ntensor_from_np = torch.from_numpy(np_array)\n\n# PyTorch tensor to NumPy array\ntensor = torch.tensor([4, 5, 6])\nnp_from_tensor = tensor.numpy()\n\n# Pandas DataFrame to PyTorch tensor\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\ntensor_from_df = torch.tensor(df.values)\n\n# PyTorch tensor to Pandas DataFrame\ntensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\ndf_from_tensor = pd.DataFrame(tensor_2d.numpy())\n</code></pre>"},{"location":"chapter_optimization/pytorch_basics/#linear-algebra","title":"Linear Algebra","text":"<p>PyTorch provides a comprehensive linear algebra library in <code>torch.linalg</code>. You can find the official documentation here. The functions basically have the same names as the ones in NumPy and SciPy.</p> <p>Below we list some of the most commonly used functions for matrix operations, solving linear equations, and eigenvalue problems.</p> <pre><code># Matrix multiplication\nA = torch.tensor([[1, 2], [3, 4]])\nB = torch.tensor([[5, 6], [7, 8]])\nC = A @ B  # or torch.matmul(A, B)\n\n# Solving linear equations\nb = torch.tensor([5, 6])\nx = torch.linalg.solve(A, b)\n\n# Eigenvalues and eigenvectors\neigenvalues, eigenvectors = torch.linalg.eig(A)\n\n# Use torch.linalg.eigh for symmetric/Hermitian matrices\n\n# Singular value decomposition\nU, S, Vh = torch.linalg.svd(A, full_matrices=False)\n</code></pre> <p>Notice that <code>torch.linalg.solve</code> does not provide the option to use a specific solver. It will use the LU decomposition by default. If your matrix is symmetric positive definite, you can use <code>torch.linalg.cholesky</code> to solve the linear equations.</p> <pre><code>A = torch.tensor([[1, 2], [2, 5]])\nL = torch.linalg.cholesky(A)\ny = torch.linalg.triangular_solve(L, b, upper=False)\nx = torch.linalg.triangular_solve(L.T, y, upper=True)\n</code></pre>"},{"location":"chapter_optimization/pytorch_basics/#automatic-differentiation","title":"Automatic Differentiation","text":"<p>PyTorch's automatic differentiation system (autograd) enables gradient-based optimization for training neural networks.</p> <pre><code># Basic autograd example\nx = torch.tensor([2.0, 3.0], requires_grad=True)  # Enable gradient tracking\ny = torch.sum(x * x)  # y = x^2\n\n# Compute gradient of z with respect to x\ny.backward()\n\n# Access gradients\nx.grad == 2 * x  # Should be 2*x: tensor([4., 6.])\n</code></pre> <p>You need to zero the gradients by <code>x.grad.zero_()</code> before computing the gradient at a new point or for a new function. Otherwise, PyTorch will accumulate the gradients.</p> <pre><code># Zeroing gradients before computing new ones\nx.grad.zero_()\ny = torch.sum(x**3)\ny.backward()\nx.grad == 3*x**2\n</code></pre> <p>Partial Derivatives:</p> <pre><code># Computing partial derivatives\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\nz = torch.sum(x * y)\nz.backward()\nprint(x.grad)  # Should be tensor([4., 5., 6.])\n</code></pre> <p>Gradients with control flow:</p> <pre><code>def f(x):\n    y = x * 2\n    while y.norm() &lt; 1000:\n        y = y * 2\n    return y\n\nx = torch.tensor([0.5], requires_grad=True)\ny = f(x)\ny.backward()\nprint(x.grad)  # Gradient depends on control flow path taken\n</code></pre>"},{"location":"chapter_optimization/pytorch_basics/#gradient-update","title":"Gradient Update","text":"<p>When implementing parameter updates like gradient descent in PyTorch, it's crucial to use <code>torch.no_grad()</code> to prevent autograd from tracking operations. Here is an example of what happens if we update the parameter without <code>torch.no_grad()</code>.</p> <pre><code># BAD: Update without torch.no_grad()\nx = torch.tensor([2.0], requires_grad=True)\ny = x**2\ny.backward()\nx = x - 0.1 * x.grad  # Creates a new tensor, loses gradient connection\nprint(x.grad)  # None - gradient information is lost!\n\n# GOOD: Update with torch.no_grad()\nx = torch.tensor([2.0], requires_grad=True)\ny = x**2\ny.backward()\nwith torch.no_grad():\n    x -= 0.1 * x.grad  # Updates in-place without building computational graph\n</code></pre> <p>Notice that we need to disable the gradient tracking for parameter updates by <code>with torch.no_grad()</code>. Otherwise, the parameter will be part of the computation graph and the gradient will be disconnected from the original gradient.</p> <p>Pitfall of Parameter Updates in PyTorch</p> <p>Always use <code>torch.no_grad()</code> when manually updating parameters in optimization algorithms in PyTorch. And use <code>x.grad.zero_()</code> to zero out the gradients before computing the new ones.</p> <p>Also, you should not write <code>x -= 0.1 * x.grad</code> as <code>x = x - 0.1 * x.grad</code> because it will</p> <ul> <li>Creates a brand new tensor and assigns it to variable <code>x</code>, which is inefficient for memory usage.</li> <li>The new tensor <code>x</code> loses the connection to the computational graph</li> <li>The right-hand side is an expression involving <code>x.grad</code> which has <code>requires_grad=True</code>. PyTorch will start tracking gradients for the parameter update itself</li> </ul> <p>In general, you should update the parameters by in-place operations. For simple gradient update, you can use <code>x -= 0.1 * x.grad</code>. For general parameter updates, you can first compute the value by a new variable <code>x_new = update_rule(x,x.grad)</code> and then use <code>x.copy_(x_new)</code> or <code>x[:] = x_new</code> to copy the value back to <code>x</code>. When you use <code>x = g(x,x.grad)</code>, you're creating a completely new tensor and assigning it to the variable <code>x</code>. This breaks the computational graph connection to the original tensor.</p> <p>We still use the gradient descent update as an example below. You can refer to the Frank-Wolfe Algorithm or Proximal Gradient Descent for more realistic examples.</p> <pre><code>x = torch.tensor([2.0], requires_grad=True)\ny = x**2\ny.backward()\nx_new = x - 0.1 * x.grad # Update x using x_new to avoid recreating x\nx.copy_(x_new) # or x[:] = x_new\n</code></pre> <p>Pitfall of In-place Operations in PyTorch</p> <p>If the algorithm has updating rule like \\(x_{t+1} = g(x_t,\\nabla f(x_t))\\) for some function \\(g\\), avoid using <code>x = g(x,x.grad)</code> in the updating step. You should use in-place operations like <code>x.copy_(g(x,x.grad))</code> or <code>x[:] = g(x,x.grad)</code> instead. </p> <p>We will introduce more about how to use PyTorch to implement the optimization algorithms with setting up the dataloader, model, and optimizer in the future lecture.</p>"},{"location":"chapter_optimization/rate_of_convergence/","title":"Rate of Convergence","text":"<p>In numerical analysis and optimization, understanding the rate of convergence is crucial for evaluating the efficiency of algorithms. The rate of convergence describes how quickly a sequence approaches its limit. Here, we explore different types of convergence rates: linear, sublinear, superlinear, and quadratic.</p> <p></p> <p>Linear Convergence:</p> <p>A sequence \\(\\{x_k\\}\\) is said to converge linearly to \\(x^*\\) if there exists a constant \\(0 &lt; c &lt; 1\\) such that:</p> \\[ \\|x_{k+1} - x^*\\| \\leq c \\|x_k - x^*\\| \\] <p>Then we have \\(\\|x_{k+1} - x^*\\| = O(c^k)\\).</p> <p>Linear convergence implies that the error reduces by a constant factor in each iteration. This is typical for many first-order optimization methods, such as gradient descent with a fixed step size.</p> <p>Sublinear Convergence:</p> <p>Sublinear convergence occurs when the rate of convergence is slower than linear. A sequence \\(\\{x_k\\}\\) converges sublinearly if:</p> \\[ \\lim_{k \\to \\infty} \\frac{\\|x_{k+1} - x^*\\|}{\\|x_k - x^*\\|} = 1 \\] <p>For example, \\(\\|x_{k+1} - x^*\\| = O(k^q)\\) for \\(q &lt; 0\\).</p> <p>Sublinear convergence is common in the first order optimization methods without strong convexity, such as gradient descent with diminishing step sizes.</p> <p>Superlinear Convergence:</p> <p>A sequence \\(\\{x_k\\}\\) converges superlinearly to \\(x^*\\) if:</p> \\[ \\lim_{k \\to \\infty} \\frac{\\|x_{k+1} - x^*\\|}{\\|x_k - x^*\\|} = 0 \\] <p>Then we have \\(\\|x_{k+1} - x^*\\| = O(q^{k^2})\\), for some \\(q &lt; 1\\).</p> <p>Superlinear convergence indicates that the sequence approaches the limit faster than any linear rate. This is often seen in methods like Newton's method when close to the solution.</p> <p>Quadratic Convergence:</p> <p>Quadratic convergence is a special case of superlinear convergence where the error term squares at each iteration. A sequence \\(\\{x_k\\}\\) converges quadratically if:</p> \\[ \\|x_{k+1} - x^*\\| \\leq c \\|x_k - x^*\\|^2 \\] <p>Then we have \\(\\|x_{k+1} - x^*\\| = O(q^{k^2})\\), for some \\(q &lt; 1\\).</p> <p>Quadratic convergence is highly desirable as it implies rapid convergence near the solution. Newton's method exhibits quadratic convergence under certain conditions.</p>"},{"location":"chapter_optimization/sgd/","title":"Stochastic Gradient Descent","text":"<p>Stochastic Gradient Descent (SGD) is a popular optimization algorithm in machine learning and deep learning. </p> <p>The SGD problem is to solve:</p> \\[ \\min_{x\\in X} F(x) = \\frac{1}{n} \\sum_{i=1}^n f_i(x) \\] <p>where \\(n\\) is the sample size and it is typically too large to compute the full gradient \\(\\nabla F(x)\\) in one go.</p> <p>Recalling the idea of randomized linear algebra, we can replace the full gradient with its unbiased estimator: \\(\\nabla f_{\\xi}(x)\\) where \\(\\xi\\) is uniformly sampled from \\(\\{1, 2, \\cdots, n\\}\\). </p> <p>This motivates the basic SGD algorithm:</p> \\[ x_{t+1} = x_t - \\eta_t g_t \\] <p>where in the following of the note, \\(g_t\\) is used to denote any unbiased estimator of \\(\\nabla F(x_t)\\), i.e.,</p> \\[ \\mathbb{E}[g_t] = \\nabla F(x_t). \\]"},{"location":"chapter_optimization/sgd/#mini-batch-gradient-descent","title":"Mini-Batch Gradient Descent","text":"<p>Mini-batch gradient descent is a variant of SGD that uses a small batch of data points to compute the gradient, balancing the efficiency of SGD with the stability of full-batch gradient descent.</p> <p>Mini-Batch Gradient Descent Algorithm</p> <p>For a mini-batch \\(\\mathcal{B}_t \\subset \\{1, 2, \\cdots, n\\}\\), update the parameters as follows:</p> \\[ x_{t+1} = x_t - \\eta_t \\frac{1}{|\\mathcal{B}_t|} \\sum_{i\\in\\mathcal{B}_t} \\nabla f_i(x_t) \\] <p>Mini-batch gradient descent reduces the variance of parameter updates, leading to more stable convergence.</p>"},{"location":"chapter_optimization/sgd/#pytorch-optimizer-pipeline","title":"PyTorch Optimizer Pipeline","text":"<p>Mini-batch gradient descent can be implemented in PyTorch by the <code>DataLoader</code> class. The <code>DataLoader</code> is a PyTorch class that provides a way to load data from a dataset into a batch. Then the stochastic gradient descent can be implemented by the <code>torch.optim.SGD</code> function.</p>"},{"location":"chapter_optimization/sgd/#pytorch-dataset-and-dataloader","title":"PyTorch Dataset and DataLoader","text":"<p>In order to use the <code>DataLoader</code> class, you first need to create a dataset class that is a subclass of <code>torch.utils.data.Dataset</code>. For this purpose, you need to implement the following methods in your dataset class:</p> <ul> <li><code>__len__</code>: Return the length of the dataset.</li> <li><code>__getitem__</code>: Return the item at the given index.</li> </ul> <p>Here is an example to generate a dataset from linear model \\(y = 3x + 2 + \\epsilon\\) with Gaussian noise \\(\\epsilon \\sim \\mathcal{N}(0, 2)\\).</p> <pre><code>import torch\nfrom torch.utils.data import Dataset\n\nclass LinearModelDataset(Dataset):\n    def __init__(self, num_samples=100):\n        self.x = torch.randn(num_samples, 1)\n        self.y = 3 * self.x + 2 + torch.randn(self.x.size()) * 2\n    def __len__(self): # Return the length of the dataset\n        return len(self.x)\n\n    def __getitem__(self, idx): # Return the item at the given index\n        # Add any preprocessing here if needed\n        return self.x[idx], self.y[idx]\n</code></pre> <p>Then you can use <code>torch.utils.data.DataLoader</code> to load the dataset for mini-batch gradient descent by setting the <code>batch_size</code> and <code>shuffle</code> parameters.</p> <pre><code>from torch.utils.data import DataLoader\n\n# Define the dataset\ndataset = LinearModelDataset(128)\n\n# Define the DataLoader\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n</code></pre>"},{"location":"chapter_optimization/sgd/#pytorch-model-class","title":"PyTorch Model Class","text":"<p>Then you can define the model class as a subclass of <code>torch.nn.Module</code>. You need to implement the following methods in your model class:</p> <ul> <li><code>__init__</code>: Define and initialize the parameters of the model. In this method, you need to add <code>super().__init__()</code> to call the constructor of the parent class.</li> <li><code>forward(self, x)</code>: Define the output of the model with respect to the input <code>x</code>.</li> </ul> <pre><code># Define the model class. Here we use a simple linear regression model as an example.\nclass LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        \"\"\"\n        This is used to define and initialize the parameters of the model.\n        All parameters in model should be nn.Parameter.\n        Here we define a linear regression model with weight and bias.\n        We will discuss other types of model class in the next chapter of deep learning.\n        \"\"\"\n        super().__init__() # Call the constructor of the parent class\n        # Initialize weight and bias as learnable parameters\n        self.weight = nn.Parameter(torch.randn(output_dim, input_dim))\n        self.bias = nn.Parameter(torch.randn(output_dim))\n\n    def forward(self, x):\n        \"\"\"\n        This is used to define the mathematics of the model.\n        \"\"\"\n        return x @ self.weight.T + self.bias\n</code></pre>"},{"location":"chapter_optimization/sgd/#pytorch-optimizer","title":"PyTorch Optimizer","text":"<p>PyTorch provides a lot of optimizers in <code>torch.optim</code>. For the mini-batch gradient descent, you can use <code>torch.optim.SGD</code> with the following parameters:</p> <ul> <li><code>params</code>: Pass all the trainable parameters from your model to the optimizer.</li> <li><code>lr</code>: Set the learning rate (step size).</li> </ul> <p>You can then set the optimizer to the model by the following code: <pre><code>model = LinearRegressionModel(1, 1) # Define the 1d linear regression model\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n</code></pre></p> <p>Here <code>model.parameters()</code> will return all the trainable parameters from the model.</p> <p>Now we are ready to assemble all the components to implement the mini-batch gradient descent:</p> <pre><code># Define the loss function, e.g., MSELoss\nloss_fn = nn.MSELoss()\n# Define the training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward pass\n        outputs = model(batch) # Compute the output of the model\n        loss = loss_fn(outputs, batch) # Compute the loss \n        # Backward pass\n        optimizer.zero_grad() # Zero the gradients of the model parameters\n        loss.backward() # Compute the gradient of the loss with respect to the model parameters\n        optimizer.step() # Update the model parameters: x_{t+1} = x_t - lr * g_t\n</code></pre> <p>The core code above are the three steps of the mini-batch gradient descent:</p> <ul> <li><code>optimizer.zero_grad()</code>: Zero the gradients of the model parameters.</li> <li><code>loss.backward()</code>: Compute the gradient of the loss with respect to the model parameters.</li> <li><code>optimizer.step()</code>: Update the model parameters</li> </ul> <p>Most of the time, you can just simply use these three lines for implementing any optimizer from <code>torch.optim</code>.</p> <p>We can write our own mini-batch gradient descent optimizer by the following code:</p> <pre><code>lr = 0.01 # Set the learning rate\n# 1. Manually zero the gradients\nfor param in model.parameters():\n    if param.grad is not None:\n        param.grad.zero_()\n# 2. Compute gradients\nloss.backward()\n# 3. Update parameters using explicit SGD rule\nwith torch.no_grad():  # Prevent tracking history for parameter updates\n    for param in model.parameters():\n        if param.grad is not None:\n            # The core SGD update: \u03b8 = \u03b8 - lr * gt\n            param.data = param.data - lr * param.grad\n</code></pre>"},{"location":"chapter_optimization/sgd/#adaptive-gradient-descent","title":"Adaptive Gradient Descent","text":"<p>Adaptive gradient descent algorithms adjust the learning rate for each parameter individually, allowing for more efficient optimization. If the partial derivative \\(\\partial f(x) / \\partial x_j\\) is smaller, the \\(j\\)-th entry \\(x_{t,j}\\) is less likely to be updated and we tend to take a larger step. On the other hand, if the partial derivative \\(\\partial f(x) / \\partial x_j\\) is larger, the \\(j\\)-th entry \\(x_{t,j}\\) was updated more significantly and we tend to take a smaller step. The idea of adaptive gradient descent is to measure the standard deviation of the partial derivatives and adjust the learning rate accordingly.</p>"},{"location":"chapter_optimization/sgd/#adagrad","title":"Adagrad","text":"<p>Adagrad adapts the learning rate based on the standard deviation of the historical gradient information. Let \\(x_{t,j}\\) be the \\(j\\)-th component of \\(x_t\\), then Adagrad updates each entry of \\(x_t\\) as follows:</p> \\[ \\begin{align*} G_{t,j} &amp;= G_{t-1,j} + g_{t,j}^2\\\\ x_{t+1,j} &amp;= x_{t,j} - \\frac{\\eta}{\\sqrt{G_{t,j} + \\epsilon}} g_{t,j} \\end{align*} \\] <p>for all \\(j=1,2,\\cdots,d\\), where \\(G_{t,j}\\) is the sum of the squares of past gradients, and \\(\\epsilon\\) is a small constant to prevent division by zero. For the notation simplicity, we will write the above Adagrad update as:</p> \\[ \\begin{align*} G_{t} &amp;= G_{t-1} + g_{t}^2\\\\ x_{t+1} &amp;= x_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} g_{t} \\end{align*} \\] <p>where \\(g_{t}^2\\) is the entry-wise square of \\(g_t\\).</p> <p>Adagrad can be implemented by the following code:</p> <pre><code>optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n</code></pre>"},{"location":"chapter_optimization/sgd/#rmsprop","title":"RMSprop","text":"<p>RMSprop modifies Adagrad by introducing a decay factor to control the accumulation of past gradients:</p> \\[ G_{t} = \\gamma G_{t-1} + (1 - \\gamma) g_{t}^2 \\] \\[ x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} g_{t} \\] <p>where \\(\\gamma\\) is the decay rate.</p> <p>RMSprop can be implemented by the following code:</p> <pre><code>optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n</code></pre>"},{"location":"chapter_optimization/sgd/#adam","title":"Adam","text":"<p>Adam (Adaptive Moment Estimation) combines the ideas of momentum and RMSprop, maintaining an exponentially decaying average of past gradients and squared gradients.</p> <p>Adam Algorithm</p> \\[ \\begin{align*}     m_t &amp;= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\\     v_t &amp;= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\\\     \\hat{m}_t &amp;= \\frac{m_t}{1 - \\beta_1^t}\\\\     \\hat{v}_t &amp;= \\frac{v_t}{1 - \\beta_2^t}\\\\     x_{t+1} &amp;= x_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t \\end{align*} \\] <p>Adam is widely used due to its robustness and efficiency in training deep neural networks. It can be implemented by the following code:</p> <pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))\n</code></pre>"},{"location":"chapter_preface/","title":"About this course","text":"<p>This course aims to create a beginner-friendly course on statistical computing.</p>"},{"location":"chapter_preface/#target-audience","title":"Target audience","text":"<p>Whether you're taking your first steps into algorithms and data analysis, or you're looking to strengthen your foundation in statistical computing and establish efficient workflows, this course is designed for you! This course aims to emphasize the width more than the depth. We want to show you the whole landscape of the computing world in one semester to prepare you for zooming into a specific research direction in the future. </p> <p>Prerequisites</p> <p>You should know how to write and read simple code in python and R. You should also have basic knowledge of probability, statistics, and linear algebra. You can refer to the MIT6.100L for the basic Python we expect you to know.</p> <p>To finish the homework, you may need to know some Markdown and LaTeX syntax but it is not required if you are only interested in learning the course materials.</p>"},{"location":"chapter_preface/#content-structure","title":"Content structure","text":"<p>The main content of the book is shown in the figure below.</p> <ul> <li> <p>Workflow: Covers principles of good coding, especially in efficiency, reproducibility, and AI copilot.  We cover Git and GitHub, Makefile, and virtual environments.</p> </li> <li> <p>Data Structures: Focuses on Python data structures like lists, dictionaries, and deque, as well as Numpy arrays and Pandas DataFrames. </p> </li> <li> <p>Algorithms: Computation and memory complexity. We will cover recursion, backtracking, dynamic programming, and greedy algorithms.</p> </li> <li> <p>Numerical Linear Algebra: Introduce the algorithms for linear algebra, including linear equations, eigenvectors, and more.</p> </li> <li> <p>Optimization: Introduces algorithms for solving optimization problems, including convex optimization, stochastic optimization, and non-convex optimization.</p> </li> <li> <p>Deep Learning: introduce the pytorch, deep learning architectures, including neural networks, transformers, and more.</p> </li> </ul>"},{"location":"chapter_preface/#tips-to-learn-this-course","title":"Tips to learn this course","text":"<ul> <li>Practice: To understand the code, you should always implement yourself.</li> <li>Customize: All the workflows, algorithm strategies, and code styles are only suggestions. You should understand the rationale behind these suggestions and customize them for your own work.</li> <li>Seek help: The open-source community is always here to help. You can always seek help from the community. And you should also try to help others.</li> <li>Always explore: We cannot cover all the details in the course. As we mentioned above, the course emphasizes the width more than the depth. You should explore deeper to the directions you are interested in. As this course is a computing-oriented course, we will not cover too much math, especially the theory. You should also explore the theory behind the algorithms and methods you are interested in.</li> <li>Adaptively skip: The course materials are designed to be a beginner-friendly course. You can skip some materials that you are already familiar with (or not interested in).</li> <li>Abstract blackboxes: When we talk about skipping, one of most important philosophy in coding is \"modular programming\". It is more important to know how to decompose your problems into smaller, self-contained modules that encapsulate their implementation details realized yourself or by existing  Application Programming Interface (API). You do not always need to open the blackbox.</li> </ul> <p>Tip: What is the real coding skill?</p> <p>The computing is such a broad field. You will never get prepared for everything. The most important coding skill is to decompose your problems into smaller problems, to be able to identify the potential toolboxes to solve your problems, and to be able to digest them efficiently. This is also the scope of this course to offer you the \"menu\" more than the \"recipe\" and the meta-learning more than the specific learning.</p>"},{"location":"chapter_preface/#acknowledgements","title":"Acknowledgements","text":"<p>This course uses the materials from many fantastic resources. We thank the authors for these materials. Here are the list of resources we have used:</p> <ul> <li>MIT Missing Semester</li> <li>AI4Med</li> <li>INFO550 Data Science Toolkit</li> <li>Harvard Chan Bioinformatics Core Training</li> <li>Berkeley Statistics Tutorials</li> </ul> <p>We also reference many textbooks which are listed in the Reference.</p> <p>If any authors have concerns regarding the use of their materials or copyright issues, please contact us at GitHub Issues. We are committed to respecting intellectual property rights and will promptly address any concerns.</p>"},{"location":"chapter_reference/","title":"References","text":"<p>[1] Knuth, D. E. (1997). The Art of Computer Programming, Volume 1: Fundamental Algorithms. Addison-Wesley Professional.</p> <p>[2] Cormen, T. H., Leiserson, C. E., Rivest, R. L., &amp; Stein, C. (2001). Introduction to Algorithms. MIT Press.</p> <p>[3] Yudong Jin (@krahets). (2024). AI4Med. Posts &amp; Telecom Press.</p> <p>[4] Golub, G. H., &amp; Van Loan, C. F. (2013). Matrix Computations. Johns Hopkins University Press.</p> <p>[5] Boyd, S., &amp; Vandenberghe, L. (2018). Convex Optimization. Cambridge University Press.</p> <p>[6] Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.</p>"}]}